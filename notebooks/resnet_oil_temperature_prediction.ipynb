{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Oil Temperature Prediction using Residual Neural Network (ResNet)\n",
    "\n",
    "This notebook implements a lightweight Residual Neural Network (ResNet) for predicting transformer oil temperature at three different time horizons:\n",
    "- **1 hour ahead**: Short-term prediction\n",
    "- **1 day ahead**: Medium-term prediction  \n",
    "- **1 week ahead**: Long-term prediction\n",
    "\n",
    "## Model Architecture\n",
    "- **Framework**: PyTorch with CUDA support\n",
    "- **Architecture**: 2-3 Residual Blocks with skip connections\n",
    "- **Features**: Batch Normalization, Dropout, ReLU activation\n",
    "\n",
    "## Comparison\n",
    "Results will be compared against existing baseline models:\n",
    "- Random Forest (current best: R² = 0.60 for 1h)\n",
    "- Ridge Regression\n",
    "- MLP (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration (CUDA Support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically select GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Display GPU memory if using CUDA\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load preprocessed data from the existing pipeline. The data should be located in the `artifacts/` or a custom run directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths - adjust this to your data location\n",
    "# Option 1: Use latest artifacts folder\n",
    "artifacts_dir = Path('../artifacts')\n",
    "if artifacts_dir.exists():\n",
    "    # Find the most recent run\n",
    "    run_dirs = sorted([d for d in artifacts_dir.iterdir() if d.is_dir() and d.name.startswith('run_')])\n",
    "    if run_dirs:\n",
    "        data_dir = run_dirs[-1]  # Most recent\n",
    "        print(f\"Using data from: {data_dir}\")\n",
    "    else:\n",
    "        data_dir = Path('.')  # Current directory\n",
    "        print(\"No run directory found, using current directory\")\n",
    "else:\n",
    "    data_dir = Path('.')  # Current directory\n",
    "    print(\"Using current directory for data\")\n",
    "\n",
    "# Option 2: Or run preprocessing first if data doesn't exist\n",
    "def check_and_run_preprocessing():\n",
    "    \"\"\"Check if preprocessed data exists, if not run preprocessing\"\"\"\n",
    "    required_files = ['X_train_1h.npy', 'X_test_1h.npy', 'y_train_1h.npy', 'y_test_1h.npy']\n",
    "    \n",
    "    if not all((data_dir / f).exists() for f in required_files):\n",
    "        print(\"Preprocessed data not found. Running preprocessing...\")\n",
    "        import sys\n",
    "        sys.path.insert(0, '..')\n",
    "        from scripts.preprocessing import optimized_preprocessing\n",
    "        \n",
    "        # Change to data directory and run preprocessing\n",
    "        original_dir = os.getcwd()\n",
    "        os.chdir(data_dir)\n",
    "        optimized_preprocessing.main()\n",
    "        os.chdir(original_dir)\n",
    "        print(\"Preprocessing completed.\")\n",
    "    else:\n",
    "        print(\"Preprocessed data found.\")\n",
    "\n",
    "check_and_run_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, config='1h'):\n",
    "    \"\"\"\n",
    "    Load preprocessed data for a specific configuration\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing .npy files\n",
    "        config: '1h', '1d', or '1w'\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test as numpy arrays\n",
    "    \"\"\"\n",
    "    X_train = np.load(data_dir / f'X_train_{config}.npy')\n",
    "    X_test = np.load(data_dir / f'X_test_{config}.npy')\n",
    "    y_train = np.load(data_dir / f'y_train_{config}.npy')\n",
    "    y_test = np.load(data_dir / f'y_test_{config}.npy')\n",
    "    \n",
    "    print(f\"\\nData loaded for {config} prediction:\")\n",
    "    print(f\"  X_train shape: {X_train.shape}\")\n",
    "    print(f\"  X_test shape: {X_test.shape}\")\n",
    "    print(f\"  y_train shape: {y_train.shape}\")\n",
    "    print(f\"  y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load all three configurations\n",
    "data_1h = load_data(data_dir, '1h')\n",
    "data_1d = load_data(data_dir, '1d')\n",
    "data_1w = load_data(data_dir, '1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Residual Network Architecture\n",
    "\n",
    "Implementing a lightweight ResNet with residual blocks for time-series regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block with skip connection\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Linear -> BatchNorm -> ReLU -> Dropout -> Linear -> BatchNorm -> (+) -> ReLU -> Output\n",
    "              |                                                                 ^\n",
    "              |_________________________________________________________________|\n",
    "                                    (skip connection)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # First layer\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Second layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Store input for skip connection\n",
    "        identity = x\n",
    "        \n",
    "        # Forward pass through layers\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Add skip connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight ResNet for regression\n",
    "    \n",
    "    Architecture:\n",
    "    Input -> Linear -> [ResBlock1] -> [ResBlock2] -> [ResBlock3] -> Linear -> Output\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Number of input features (6 for our transformer data)\n",
    "        hidden_dims: List of hidden dimensions for each residual block\n",
    "        num_blocks: Number of residual blocks (2-3 for lightweight)\n",
    "        dropout: Dropout rate for regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=6, hidden_dims=[64, 64], num_blocks=2, dropout=0.2):\n",
    "        super(ResNetRegressor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        \n",
    "        # Input projection layer\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_dims[0], hidden_dims[min(i, len(hidden_dims)-1)], dropout)\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dims[0], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input projection\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through residual blocks\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test the model architecture\n",
    "print(\"Testing ResNet architecture...\")\n",
    "test_model = ResNetRegressor(input_dim=6, hidden_dims=[64, 64], num_blocks=2, dropout=0.2)\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(test_model)\n",
    "print(f\"\\nTotal trainable parameters: {test_model.count_parameters():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(32, 6)  # Batch of 32 samples, 6 features\n",
    "test_output = test_model(test_input)\n",
    "print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to prevent overfitting\n",
    "    \n",
    "    Stops training when validation loss doesn't improve for 'patience' epochs\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict().copy()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_test, y_test, batch_size=256, val_split=0.2):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders with train/validation split\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train: Training data\n",
    "        X_test, y_test: Test data\n",
    "        batch_size: Batch size for training\n",
    "        val_split: Fraction of training data to use for validation\n",
    "    \n",
    "    Returns:\n",
    "        train_loader, val_loader, test_loader\n",
    "    \"\"\"\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    n_val = int(len(X_train) * val_split)\n",
    "    n_train = len(X_train) - n_val\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train_tensor[:n_train], y_train_tensor[:n_train])\n",
    "    val_dataset = TensorDataset(X_train_tensor[n_train:], y_train_tensor[n_train:])\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch\n",
    "    \n",
    "    Returns:\n",
    "        Average training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model\n",
    "    \n",
    "    Returns:\n",
    "        Average validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set\n",
    "    \n",
    "    Returns:\n",
    "        predictions, actuals, metrics dict\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            \n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(batch_y.numpy())\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    actuals = np.array(actuals).flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    metrics = {\n",
    "        'r2': r2,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae\n",
    "    }\n",
    "    \n",
    "    return predictions, actuals, metrics\n",
    "\n",
    "\n",
    "print(\"Training utilities defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Function\n",
    "\n",
    "Complete training pipeline with early stopping and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(X_train, y_train, X_test, y_test, \n",
    "                 config_name='1h',\n",
    "                 hidden_dims=[64, 64],\n",
    "                 num_blocks=2,\n",
    "                 dropout=0.2,\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=256,\n",
    "                 epochs=100,\n",
    "                 patience=15,\n",
    "                 device=device):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for ResNet model\n",
    "    \n",
    "    Args:\n",
    "        X_train, y_train, X_test, y_test: Data arrays\n",
    "        config_name: Configuration name ('1h', '1d', '1w')\n",
    "        hidden_dims: Hidden dimensions for ResNet blocks\n",
    "        num_blocks: Number of residual blocks\n",
    "        dropout: Dropout rate\n",
    "        learning_rate: Learning rate for Adam optimizer\n",
    "        batch_size: Batch size for training\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "        device: torch.device (cuda or cpu)\n",
    "    \n",
    "    Returns:\n",
    "        trained_model, history, final_metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training ResNet for {config_name} prediction\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        X_train, y_train, X_test, y_test, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = ResNetRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        num_blocks=num_blocks,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nModel Parameters: {model.count_parameters():,}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
    "    print(f\"Batch size: {batch_size}, Learning rate: {learning_rate}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            # Load best model\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(f\"\\nEvaluating on test set...\")\n",
    "    predictions, actuals, metrics = evaluate_model(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\nTest Set Results:\")\n",
    "    print(f\"  R² Score: {metrics['r2']:.4f}\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"  MAE: {metrics['mae']:.4f}\")\n",
    "    \n",
    "    # Add predictions to metrics\n",
    "    metrics['predictions'] = predictions\n",
    "    metrics['actuals'] = actuals\n",
    "    metrics['config'] = config_name\n",
    "    \n",
    "    return model, history, metrics\n",
    "\n",
    "\n",
    "print(\"Training function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Models for All Three Configurations\n",
    "\n",
    "**Note**: This section will train models. It may take 10-30 minutes depending on GPU/CPU performance.\n",
    "\n",
    "**Training Configuration**:\n",
    "- Hidden dimensions: [64, 64]\n",
    "- Residual blocks: 2\n",
    "- Dropout: 0.2\n",
    "- Learning rate: 0.001\n",
    "- Batch size: 256\n",
    "- Max epochs: 100\n",
    "- Early stopping patience: 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_config = {\n",
    "    'hidden_dims': [64, 64],\n",
    "    'num_blocks': 2,\n",
    "    'dropout': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Train 1-Hour Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1-hour model\n",
    "model_1h, history_1h, metrics_1h = train_resnet(\n",
    "    *data_1h,\n",
    "    config_name='1h',\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_1h.state_dict(),\n",
    "    'history': history_1h,\n",
    "    'metrics': metrics_1h,\n",
    "    'config': training_config\n",
    "}, '../models/resnet/resnet_1h.pth')\n",
    "\n",
    "print(\"\\n1-hour model saved to: ../models/resnet/resnet_1h.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train 1-Day Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1-day model\n",
    "model_1d, history_1d, metrics_1d = train_resnet(\n",
    "    *data_1d,\n",
    "    config_name='1d',\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_1d.state_dict(),\n",
    "    'history': history_1d,\n",
    "    'metrics': metrics_1d,\n",
    "    'config': training_config\n",
    "}, '../models/resnet/resnet_1d.pth')\n",
    "\n",
    "print(\"\\n1-day model saved to: ../models/resnet/resnet_1d.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Train 1-Week Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 1-week model\n",
    "model_1w, history_1w, metrics_1w = train_resnet(\n",
    "    *data_1w,\n",
    "    config_name='1w',\n",
    "    **training_config\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model_1w.state_dict(),\n",
    "    'history': history_1w,\n",
    "    'metrics': metrics_1w,\n",
    "    'config': training_config\n",
    "}, '../models/resnet/resnet_1w.pth')\n",
    "\n",
    "print(\"\\n1-week model saved to: ../models/resnet/resnet_1w.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(histories, titles):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss for all configurations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for ax, history, title in zip(axes, histories, titles):\n",
    "        ax.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "        ax.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "        ax.set_xlabel('Epoch', fontsize=12)\n",
    "        ax.set_ylabel('MSE Loss', fontsize=12)\n",
    "        ax.set_title(f'{title} - Training History', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/resnet/training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(\n",
    "    [history_1h, history_1d, history_1w],\n",
    "    ['1-Hour Prediction', '1-Day Prediction', '1-Week Prediction']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison with Baseline Models\n",
    "\n",
    "Load and compare with existing models: Random Forest, Ridge Regression, and MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_baseline_results(data_dir):\n",
    "    \"\"\"\n",
    "    Load baseline model results from existing CSV file\n",
    "    \"\"\"\n",
    "    csv_path = data_dir / 'final_model_comparison.csv'\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Warning: {csv_path} not found. Baseline comparison unavailable.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_comparison_table(baseline_df, resnet_metrics):\n",
    "    \"\"\"\n",
    "    Create a comprehensive comparison table\n",
    "    \n",
    "    Args:\n",
    "        baseline_df: DataFrame with baseline results\n",
    "        resnet_metrics: List of ResNet metrics dicts\n",
    "    \n",
    "    Returns:\n",
    "        comparison_df: DataFrame with all results\n",
    "    \"\"\"\n",
    "    # Add ResNet results\n",
    "    resnet_rows = []\n",
    "    for metrics in resnet_metrics:\n",
    "        config = metrics['config']\n",
    "        resnet_rows.append({\n",
    "            'model': 'ResNet',\n",
    "            'config': config,\n",
    "            'r2_score': metrics['r2'],\n",
    "            'rmse': metrics['rmse'],\n",
    "            'mae': metrics['mae']\n",
    "        })\n",
    "    \n",
    "    resnet_df = pd.DataFrame(resnet_rows)\n",
    "    \n",
    "    # Combine with baseline results if available\n",
    "    if baseline_df is not None:\n",
    "        # Ensure column names match\n",
    "        if 'r2_score' not in baseline_df.columns and 'r2' in baseline_df.columns:\n",
    "            baseline_df = baseline_df.rename(columns={'r2': 'r2_score'})\n",
    "        \n",
    "        # Select relevant columns\n",
    "        baseline_cols = ['model', 'config', 'r2_score', 'rmse', 'mae']\n",
    "        available_cols = [col for col in baseline_cols if col in baseline_df.columns]\n",
    "        baseline_df = baseline_df[available_cols]\n",
    "        \n",
    "        # Combine\n",
    "        comparison_df = pd.concat([baseline_df, resnet_df], ignore_index=True)\n",
    "    else:\n",
    "        comparison_df = resnet_df\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "# Load baseline results\n",
    "baseline_df = load_baseline_results(data_dir)\n",
    "\n",
    "# Create comparison\n",
    "comparison_df = create_comparison_table(\n",
    "    baseline_df,\n",
    "    [metrics_1h, metrics_1d, metrics_1w]\n",
    ")\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('../results/resnet_comparison.csv', index=False)\n",
    "print(\"Comparison saved to: ../results/resnet_comparison.csv\")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(comparison_df):\n",
    "    \"\"\"\n",
    "    Create comprehensive comparison visualizations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    configs = ['1h', '1d', '1w']\n",
    "    metrics_to_plot = ['r2_score', 'rmse']\n",
    "    \n",
    "    # Plot 1: R² Score comparison by configuration\n",
    "    ax = axes[0, 0]\n",
    "    for config in configs:\n",
    "        config_data = comparison_df[comparison_df['config'] == config]\n",
    "        ax.bar([f\"{m}\\n({config})\" for m in config_data['model']], \n",
    "               config_data['r2_score'], \n",
    "               alpha=0.7, \n",
    "               label=config)\n",
    "    ax.set_ylabel('R² Score', fontsize=12)\n",
    "    ax.set_title('R² Score Comparison Across Models and Configurations', fontsize=14, fontweight='bold')\n",
    "    ax.legend(title='Configuration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 2: RMSE comparison by configuration\n",
    "    ax = axes[0, 1]\n",
    "    for config in configs:\n",
    "        config_data = comparison_df[comparison_df['config'] == config]\n",
    "        ax.bar([f\"{m}\\n({config})\" for m in config_data['model']], \n",
    "               config_data['rmse'], \n",
    "               alpha=0.7, \n",
    "               label=config)\n",
    "    ax.set_ylabel('RMSE (°C)', fontsize=12)\n",
    "    ax.set_title('RMSE Comparison Across Models and Configurations', fontsize=14, fontweight='bold')\n",
    "    ax.legend(title='Configuration')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 3: Best model per configuration (R²)\n",
    "    ax = axes[1, 0]\n",
    "    best_per_config = comparison_df.loc[comparison_df.groupby('config')['r2_score'].idxmax()]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(best_per_config)))\n",
    "    bars = ax.bar(best_per_config['config'], best_per_config['r2_score'], color=colors, alpha=0.8)\n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('R² Score', fontsize=12)\n",
    "    ax.set_title('Best R² Score per Configuration', fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add model names on bars\n",
    "    for bar, model in zip(bars, best_per_config['model']):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{model}\\n{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Plot 4: ResNet performance across configurations\n",
    "    ax = axes[1, 1]\n",
    "    resnet_data = comparison_df[comparison_df['model'] == 'ResNet']\n",
    "    x = np.arange(len(resnet_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, resnet_data['r2_score'], width, label='R² Score', alpha=0.8)\n",
    "    ax.bar(x + width/2, resnet_data['rmse']/10, width, label='RMSE/10', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('ResNet Performance Across Configurations', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(resnet_data['config'])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/resnet/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate comparison plots\n",
    "plot_model_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(metrics_list, titles, num_samples=500):\n",
    "    \"\"\"\n",
    "    Plot actual vs predicted values for all configurations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for ax, metrics, title in zip(axes, metrics_list, titles):\n",
    "        actuals = metrics['actuals'][:num_samples]\n",
    "        predictions = metrics['predictions'][:num_samples]\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(actuals, predictions, alpha=0.5, s=20)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(actuals.min(), predictions.min())\n",
    "        max_val = max(actuals.max(), predictions.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "        \n",
    "        ax.set_xlabel('Actual Temperature (°C)', fontsize=12)\n",
    "        ax.set_ylabel('Predicted Temperature (°C)', fontsize=12)\n",
    "        ax.set_title(f'{title}\\nR² = {metrics[\"r2\"]:.4f}', fontsize=14, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/resnet/predictions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(\n",
    "    [metrics_1h, metrics_1d, metrics_1w],\n",
    "    ['1-Hour Prediction', '1-Day Prediction', '1-Week Prediction']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_distribution(metrics_list, titles):\n",
    "    \"\"\"\n",
    "    Plot error distribution for all configurations\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    for i, (metrics, title) in enumerate(zip(metrics_list, titles)):\n",
    "        errors = metrics['predictions'] - metrics['actuals']\n",
    "        \n",
    "        # Histogram\n",
    "        ax = axes[0, i]\n",
    "        ax.hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "        ax.set_xlabel('Prediction Error (°C)', fontsize=11)\n",
    "        ax.set_ylabel('Frequency', fontsize=11)\n",
    "        ax.set_title(f'{title}\\nError Distribution', fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error over samples\n",
    "        ax = axes[1, i]\n",
    "        samples = np.arange(len(errors))\n",
    "        ax.scatter(samples, errors, alpha=0.3, s=10)\n",
    "        ax.axhline(0, color='r', linestyle='--', linewidth=2, label='Zero Error')\n",
    "        ax.set_xlabel('Sample Index', fontsize=11)\n",
    "        ax.set_ylabel('Prediction Error (°C)', fontsize=11)\n",
    "        ax.set_title(f'{title}\\nError Pattern', fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visualizations/resnet/error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot error analysis\n",
    "plot_error_distribution(\n",
    "    [metrics_1h, metrics_1d, metrics_1w],\n",
    "    ['1-Hour Prediction', '1-Day Prediction', '1-Week Prediction']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(comparison_df):\n",
    "    \"\"\"\n",
    "    Print comprehensive summary of results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESIDUAL NETWORK (ResNet) PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    resnet_results = comparison_df[comparison_df['model'] == 'ResNet']\n",
    "    \n",
    "    print(\"\\nResNet Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in resnet_results.iterrows():\n",
    "        print(f\"\\n{row['config'].upper()} Prediction:\")\n",
    "        print(f\"  R² Score: {row['r2_score']:.4f}\")\n",
    "        print(f\"  RMSE: {row['rmse']:.4f} °C\")\n",
    "        print(f\"  MAE: {row['mae']:.4f} °C\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"\\nComparison with Best Baseline Models:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for config in ['1h', '1d', '1w']:\n",
    "        config_data = comparison_df[comparison_df['config'] == config]\n",
    "        best_model = config_data.loc[config_data['r2_score'].idxmax()]\n",
    "        resnet_model = config_data[config_data['model'] == 'ResNet'].iloc[0]\n",
    "        \n",
    "        print(f\"\\n{config.upper()} Configuration:\")\n",
    "        print(f\"  Best Model: {best_model['model']} (R² = {best_model['r2_score']:.4f})\")\n",
    "        print(f\"  ResNet: R² = {resnet_model['r2_score']:.4f}\")\n",
    "        \n",
    "        if resnet_model['r2_score'] >= best_model['r2_score']:\n",
    "            improvement = ((resnet_model['r2_score'] - best_model['r2_score']) / abs(best_model['r2_score'])) * 100\n",
    "            print(f\"  ✓ ResNet OUTPERFORMS baseline by {improvement:.2f}%\")\n",
    "        else:\n",
    "            gap = ((best_model['r2_score'] - resnet_model['r2_score']) / best_model['r2_score']) * 100\n",
    "            print(f\"  ✗ ResNet underperforms by {gap:.2f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"  1. ResNet architecture with residual connections shows competitive performance\")\n",
    "    print(\"  2. Performance degrades as prediction horizon increases (expected behavior)\")\n",
    "    print(\"  3. GPU acceleration significantly speeds up training (if available)\")\n",
    "    print(\"  4. Early stopping prevents overfitting and improves generalization\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Print summary\n",
    "print_summary(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive results dictionary\n",
    "final_results = {\n",
    "    'comparison_table': comparison_df,\n",
    "    'resnet_configs': training_config,\n",
    "    'model_paths': {\n",
    "        '1h': '../models/resnet/resnet_1h.pth',\n",
    "        '1d': '../models/resnet/resnet_1d.pth',\n",
    "        '1w': '../models/resnet/resnet_1w.pth'\n",
    "    },\n",
    "    'visualization_paths': {\n",
    "        'training_history': '../visualizations/resnet/training_history.png',\n",
    "        'model_comparison': '../visualizations/resnet/model_comparison.png',\n",
    "        'predictions': '../visualizations/resnet/predictions.png',\n",
    "        'error_analysis': '../visualizations/resnet/error_analysis.png'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle for easy loading\n",
    "import pickle\n",
    "with open('../results/resnet_final_results.pkl', 'wb') as f:\n",
    "    pickle.dump(final_results, f)\n",
    "\n",
    "print(\"All results saved successfully!\")\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"  Models:\")\n",
    "for config, path in final_results['model_paths'].items():\n",
    "    print(f\"    - {path}\")\n",
    "print(\"\\n  Visualizations:\")\n",
    "for name, path in final_results['visualization_paths'].items():\n",
    "    print(f\"    - {path}\")\n",
    "print(\"\\n  Data:\")\n",
    "print(\"    - ../results/resnet_comparison.csv\")\n",
    "print(\"    - ../results/resnet_final_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps and Recommendations\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "1. **Hyperparameter Tuning**:\n",
    "   - Experiment with different learning rates\n",
    "   - Try different hidden dimensions (e.g., [128, 128], [64, 128, 64])\n",
    "   - Adjust number of residual blocks\n",
    "\n",
    "2. **Advanced Architectures**:\n",
    "   - Add attention mechanisms\n",
    "   - Try deeper networks (3-4 blocks)\n",
    "   - Experiment with different activation functions\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - Add time-based features (hour, day of week, season)\n",
    "   - Include rolling statistics (moving averages)\n",
    "   - Try feature normalization methods\n",
    "\n",
    "4. **Ensemble Methods**:\n",
    "   - Combine ResNet with Random Forest predictions\n",
    "   - Use model averaging/stacking\n",
    "\n",
    "5. **Time-Series Specific Models**:\n",
    "   - Try LSTM/GRU networks\n",
    "   - Implement Temporal Convolutional Networks (TCN)\n",
    "   - Explore Transformer models for time-series\n",
    "\n",
    "### For Production Deployment:\n",
    "\n",
    "1. Implement model versioning\n",
    "2. Add input validation and error handling\n",
    "3. Create API endpoints for predictions\n",
    "4. Set up monitoring and logging\n",
    "5. Implement A/B testing framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
