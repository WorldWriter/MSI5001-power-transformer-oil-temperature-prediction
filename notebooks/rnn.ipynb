{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rnn-intro",
   "metadata": {},
   "source": [
    "# \u4f7f\u7528 RNN \u9884\u6d4b\u7535\u529b\u53d8\u538b\u5668\u6cb9\u6e29 / Power Transformer Oil Temperature Prediction using RNN\n",
    "\n",
    "## \u7b80\u4ecb / Introduction\n",
    "\n",
    "\u672c notebook \u5b9e\u73b0\u4e86\u4e00\u4e2a\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u6a21\u578b\u6765\u9884\u6d4b\u7535\u529b\u53d8\u538b\u5668\u7684\u6cb9\u6e29\uff08OT\uff09\u3002\n",
    "\n",
    "This notebook implements a Recurrent Neural Network (RNN) model for predicting the oil temperature (OT) of power transformers.\n",
    "\n",
    "**\u6570\u636e\u96c6 / Dataset**: ETDataset (\u7535\u529b\u53d8\u538b\u5668\u6e29\u5ea6\u6570\u636e\u96c6 / Electricity Transformer Temperature)\n",
    "- \u6765\u6e90 / Source: https://github.com/zhouhaoyi/ETDataset\n",
    "- \u51fa\u5904 / From: AAAI 2021 \u6700\u4f73\u8bba\u6587\uff08Informer \u6a21\u578b\uff09/ AAAI 2021 Best Paper (Informer model)\n",
    "\n",
    "**\u7279\u5f81 / Features**:\n",
    "- HUFL: \u9ad8\u538b\u6709\u529f\u8d1f\u8f7d / High UseFul Load\n",
    "- HULL: \u9ad8\u538b\u65e0\u529f\u8d1f\u8f7d / High UseLess Load  \n",
    "- MUFL: \u4e2d\u538b\u6709\u529f\u8d1f\u8f7d / Medium UseFul Load\n",
    "- MULL: \u4e2d\u538b\u65e0\u529f\u8d1f\u8f7d / Medium UseLess Load\n",
    "- LUFL: \u4f4e\u538b\u6709\u529f\u8d1f\u8f7d / Low UseFul Load\n",
    "- LULL: \u4f4e\u538b\u65e0\u529f\u8d1f\u8f7d / Low UseLess Load\n",
    "\n",
    "**\u76ee\u6807\u53d8\u91cf / Target**: OT (\u6cb9\u6e29 / Oil Temperature)\n",
    "\n",
    "**\u76ee\u6807 / Goal**: \u57fa\u4e8e\u5386\u53f2\u8d1f\u8f7d\u6570\u636e\u6784\u5efa RNN \u6a21\u578b\u9884\u6d4b\u6cb9\u6e29\uff0c\u5e76\u5c55\u793a\uff1a\n",
    "- \u65f6\u95f4\u5e8f\u5217\u6570\u636e\u9884\u5904\u7406 / Time series data preprocessing\n",
    "- \u7528\u4e8e\u5e8f\u5217\u9884\u6d4b\u7684 RNN \u67b6\u6784 / RNN architecture for sequence prediction\n",
    "- \u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30 / Model training and evaluation\n",
    "- \u6027\u80fd\u6307\u6807\u4e0e\u53ef\u89c6\u5316 / Performance metrics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 1: \u5bfc\u5165\u6240\u9700\u5e93 / Step 1: Import Required Libraries\n",
    "\n",
    "\u6211\u4eec\u5c06\u4f7f\u7528 / We'll use:\n",
    "- pandas/numpy: \u6570\u636e\u5904\u7406 / data manipulation\n",
    "- sklearn: \u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u6807 / preprocessing and metrics\n",
    "- **PyTorch**: \u6784\u5efa RNN \u6a21\u578b / building the RNN model\n",
    "- matplotlib: \u53ef\u89c6\u5316 / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u6570\u636e\u5904\u7406 / Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# \u9884\u5904\u7406 / Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# \u6df1\u5ea6\u5b66\u4e60 / Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# \u53ef\u89c6\u5316 / Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# \u89e3\u51b3\u4e2d\u6587\u663e\u793a\u95ee\u9898 / Fix Chinese font display issue\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # \u89e3\u51b3\u8d1f\u53f7\u663e\u793a\u95ee\u9898 / Fix minus sign display\n",
    "\n",
    "# \u5de5\u5177 / Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# \u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\u4ee5\u4fdd\u8bc1\u53ef\u590d\u73b0\u6027 / Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# \u8bbe\u5907\u914d\u7f6e / Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch \u7248\u672c / PyTorch version: {torch.__version__}\")\n",
    "print(f\"\u8bbe\u5907 / Device: {device}\")\n",
    "print(f\"CUDA \u53ef\u7528 / CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 2: \u52a0\u8f7d\u548c\u63a2\u7d22\u6570\u636e / Step 2: Load and Explore Data\n",
    "\n",
    "\u6211\u4eec\u5c06\u52a0\u8f7d\u53d8\u538b\u5668\u6e29\u5ea6\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u521d\u6b65\u63a2\u7d22\u4ee5\u4e86\u89e3\uff1a\n",
    "\n",
    "We'll load the transformer temperature dataset and perform initial exploration to understand:\n",
    "- \u6570\u636e\u5f62\u72b6\u548c\u7ed3\u6784 / Data shape and structure\n",
    "- \u7f3a\u5931\u503c / Missing values\n",
    "- \u7edf\u8ba1\u5c5e\u6027 / Statistical properties\n",
    "- \u65f6\u95f4\u5e8f\u5217\u7279\u5f81 / Time series characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load the ETT dataset\n",
    "    \u52a0\u8f7d ETT \u6570\u636e\u96c6\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        Loaded dataframe with date as index\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "    return df\n",
    "\n",
    "# Load both datasets / \u52a0\u8f7d\u4e24\u4e2a\u6570\u636e\u96c6\n",
    "train1_filepath = '../dataset/train1.csv'\n",
    "train2_filepath = '../dataset/train2.csv'\n",
    "\n",
    "df_train1 = load_data(train1_filepath)\n",
    "df_train2 = load_data(train2_filepath)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset loaded from original files:\")\n",
    "print(\"\u4ece\u539f\u59cb\u6587\u4ef6\u52a0\u8f7d\u6570\u636e\u96c6\uff1a\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset 1: {train1_filepath}\")\n",
    "print(f\"  Shape: {df_train1.shape}\")\n",
    "print(f\"  Date range: {df_train1.index[0]} to {df_train1.index[-1]}\")\n",
    "\n",
    "print(f\"\\nDataset 2: {train2_filepath}\")\n",
    "print(f\"  Shape: {df_train2.shape}\")\n",
    "print(f\"  Date range: {df_train2.index[0]} to {df_train2.index[-1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 1 (train1.csv) - First few rows:\")\n",
    "print(\"\u6570\u636e\u96c6 1 - \u524d\u51e0\u884c\uff1a\")\n",
    "print(\"=\"*60)\n",
    "display(df_train1.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 2 (train2.csv) - First few rows:\")\n",
    "print(\"\u6570\u636e\u96c6 2 - \u524d\u51e0\u884c\uff1a\")\n",
    "print(\"=\"*60)\n",
    "display(df_train2.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 1 - Basic Statistics:\")\n",
    "print(\"\u6570\u636e\u96c6 1 - \u57fa\u672c\u7edf\u8ba1\u4fe1\u606f\uff1a\")\n",
    "print(\"=\"*60)\n",
    "display(df_train1.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 2 - Basic Statistics:\")\n",
    "print(\"\u6570\u636e\u96c6 2 - \u57fa\u672c\u7edf\u8ba1\u4fe1\u606f\uff1a\")\n",
    "print(\"=\"*60)\n",
    "display(df_train2.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison: Value Ranges / \u5bf9\u6bd4\uff1a\u6570\u503c\u8303\u56f4\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': df_train1.columns,\n",
    "    'Train1 Mean': df_train1.mean().values,\n",
    "    'Train2 Mean': df_train2.mean().values,\n",
    "    'Train1 Std': df_train1.std().values,\n",
    "    'Train2 Std': df_train2.std().values\n",
    "})\n",
    "display(comparison_df)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 3: \u6570\u636e\u53ef\u89c6\u5316 / Step 3: Data Visualization\n",
    "\n",
    "\u53ef\u89c6\u5316\u65f6\u95f4\u5e8f\u5217\u4ee5\u7406\u89e3\u6a21\u5f0f\u548c\u5173\u7cfb\n",
    "\n",
    "Visualize the time series to understand patterns and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Oil Temperature between two datasets\n",
    "# \u6bd4\u8f83\u4e24\u4e2a\u6570\u636e\u96c6\u7684\u6cb9\u6e29\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Dataset 1 / \u6570\u636e\u96c6 1\n",
    "axes[0].plot(df_train1.index[:2000], df_train1['OT'][:2000], linewidth=0.8, label='Dataset 1')\n",
    "axes[0].set_title('Dataset 1: Oil Temperature Over Time (First 2000 points) / \u6570\u636e\u96c61\uff1a\u6cb9\u6e29\u968f\u65f6\u95f4\u53d8\u5316\uff08\u524d2000\u4e2a\u70b9\uff09', fontsize=14)\n",
    "axes[0].set_xlabel('Time / \u65f6\u95f4')\n",
    "axes[0].set_ylabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Dataset 2 / \u6570\u636e\u96c6 2\n",
    "axes[1].plot(df_train2.index[:2000], df_train2['OT'][:2000], linewidth=0.8, label='Dataset 2', color='orange')\n",
    "axes[1].set_title('Dataset 2: Oil Temperature Over Time (First 2000 points) / \u6570\u636e\u96c62\uff1a\u6cb9\u6e29\u968f\u65f6\u95f4\u53d8\u5316\uff08\u524d2000\u4e2a\u70b9\uff09', fontsize=14)\n",
    "axes[1].set_xlabel('Time / \u65f6\u95f4')\n",
    "axes[1].set_ylabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmaps for both datasets\n",
    "# \u4e24\u4e2a\u6570\u636e\u96c6\u7684\u76f8\u5173\u6027\u70ed\u529b\u56fe\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Dataset 1\n",
    "sns.heatmap(df_train1.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Dataset 1: Feature Correlation / \u6570\u636e\u96c61\uff1a\u7279\u5f81\u76f8\u5173\u6027', fontsize=14)\n",
    "\n",
    "# Dataset 2\n",
    "sns.heatmap(df_train2.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Dataset 2: Feature Correlation / \u6570\u636e\u96c62\uff1a\u7279\u5f81\u76f8\u5173\u6027', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution comparison\n",
    "# \u5206\u5e03\u5bf9\u6bd4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df_train1['OT'], bins=50, edgecolor='black', alpha=0.7, label='Dataset 1')\n",
    "axes[0].set_title('Dataset 1: OT Distribution / \u6570\u636e\u96c61\uff1a\u6cb9\u6e29\u5206\u5e03', fontsize=14)\n",
    "axes[0].set_xlabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "axes[0].set_ylabel('Frequency / \u9891\u7387')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(df_train2['OT'], bins=50, edgecolor='black', alpha=0.7, color='orange', label='Dataset 2')\n",
    "axes[1].set_title('Dataset 2: OT Distribution / \u6570\u636e\u96c62\uff1a\u6cb9\u6e29\u5206\u5e03', fontsize=14)\n",
    "axes[1].set_xlabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "axes[1].set_ylabel('Frequency / \u9891\u7387')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation / \u89c2\u5bdf:\")\n",
    "print(f\"Dataset 1 OT range / \u6570\u636e\u96c61\u6cb9\u6e29\u8303\u56f4: {df_train1['OT'].min():.2f}\u00b0C - {df_train1['OT'].max():.2f}\u00b0C\")\n",
    "print(f\"Dataset 2 OT range / \u6570\u636e\u96c62\u6cb9\u6e29\u8303\u56f4: {df_train2['OT'].min():.2f}\u00b0C - {df_train2['OT'].max():.2f}\u00b0C\")\n",
    "print(f\"\\nDataset 2 has higher temperature values / \u6570\u636e\u96c62\u5177\u6709\u66f4\u9ad8\u7684\u6e29\u5ea6\u503c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 4: \u6570\u636e\u9884\u5904\u7406 / Step 4: Data Preprocessing\n",
    "\n",
    "### 4.1 \u7279\u5f81\u548c\u76ee\u6807\u5206\u79bb / Feature and Target Separation\n",
    "\n",
    "\u6211\u4eec\u5c06\u7279\u5f81\uff08\u8d1f\u8f7d\u6570\u636e\uff09\u4e0e\u76ee\u6807\uff08OT - \u6cb9\u6e29\uff09\u5206\u79bb\n",
    "\n",
    "We'll separate features (load data) from target (OT - Oil Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_target(df):\n",
    "    \"\"\"\n",
    "    Separate features and target variable\n",
    "    \u5206\u79bb\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.ndarray\n",
    "        Features (all columns except OT)\n",
    "    y : np.ndarray\n",
    "        Target (OT column)\n",
    "    \"\"\"\n",
    "    # Features: all columns except OT\n",
    "    # \u7279\u5f81\uff1a\u9664 OT \u5916\u7684\u6240\u6709\u5217\n",
    "    X = df.drop('OT', axis=1).values\n",
    "    # Target: OT (Oil Temperature)\n",
    "    # \u76ee\u6807\uff1aOT\uff08\u6cb9\u6e29\uff09\n",
    "    y = df['OT'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process both datasets / \u5904\u7406\u4e24\u4e2a\u6570\u636e\u96c6\n",
    "print(\"=\"*60)\n",
    "print(\"Processing Dataset 1 / \u5904\u7406\u6570\u636e\u96c61\")\n",
    "print(\"=\"*60)\n",
    "X1, y1 = prepare_features_target(df_train1)\n",
    "print(f\"Features shape / \u7279\u5f81\u5f62\u72b6: {X1.shape}\")\n",
    "print(f\"Target shape / \u76ee\u6807\u5f62\u72b6: {y1.shape}\")\n",
    "print(f\"Feature names / \u7279\u5f81\u540d\u79f0: {df_train1.drop('OT', axis=1).columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Dataset 2 / \u5904\u7406\u6570\u636e\u96c62\")\n",
    "print(\"=\"*60)\n",
    "X2, y2 = prepare_features_target(df_train2)\n",
    "print(f\"Features shape / \u7279\u5f81\u5f62\u72b6: {X2.shape}\")\n",
    "print(f\"Target shape / \u76ee\u6807\u5f62\u72b6: {y2.shape}\")\n",
    "print(f\"Feature names / \u7279\u5f81\u540d\u79f0: {df_train2.drop('OT', axis=1).columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalize-header",
   "metadata": {},
   "source": [
    "### 4.2 \u6570\u636e\u6807\u51c6\u5316 / Data Normalization\n",
    "\n",
    "\u6807\u51c6\u5316\u7279\u5f81\u548c\u76ee\u6807\u4ee5\u63d0\u9ad8 RNN \u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\n",
    "\n",
    "Normalize features and target to improve RNN training stability and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, y):\n",
    "    \"\"\"\n",
    "    Normalize features and target using StandardScaler\n",
    "    \u4f7f\u7528 StandardScaler \u6807\u51c6\u5316\u7279\u5f81\u548c\u76ee\u6807\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features\n",
    "    y : np.ndarray\n",
    "        Target\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : np.ndarray\n",
    "        Normalized features\n",
    "    y_scaled : np.ndarray\n",
    "        Normalized target\n",
    "    scaler_X : StandardScaler\n",
    "        Fitted scaler for features\n",
    "    scaler_y : StandardScaler\n",
    "        Fitted scaler for target\n",
    "    \"\"\"\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTANT: Normalization is now done in the training loop (Cell 22)\n",
    "# to avoid data leakage. Scalers will be fit ONLY on training data.\n",
    "# \n",
    "# \u91cd\u8981\uff1a\u5f52\u4e00\u5316\u73b0\u5728\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u8fdb\u884c\uff08Cell 22\uff09\n",
    "# \u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\u3002Scaler \u53ea\u4f1a\u5728\u8bad\u7ec3\u6570\u636e\u4e0a fit\u3002\n",
    "# ============================================================================\n",
    "\n",
    "# Normalize Dataset 1 / \u5f52\u4e00\u5316\u6570\u636e\u96c61 (COMMENTED OUT / \u5df2\u6ce8\u91ca)\n",
    "print(\"=\"*60)\n",
    "print(\"NOTE: Normalization will be performed in training loop to avoid data leakage\")\n",
    "print(\"\u6ce8\u610f\uff1a\u5f52\u4e00\u5316\u5c06\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u8fdb\u884c\u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\")\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Dataset 1 / \u5f52\u4e00\u5316\u6570\u636e\u96c61\")\n",
    "print(\"=\"*60)\n",
    "print(\"NOTE: Normalization will be performed in training loop to avoid data leakage\")\n",
    "print(\"\u6ce8\u610f\uff1a\u5f52\u4e00\u5316\u5c06\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u8fdb\u884c\u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\")\n",
    "print(\"=\"*60)\n",
    "# X1_scaled, y1_scaled, scaler_X1, scaler_y1 = normalize_data(X1, y1)  # Commented out to avoid data leakage\n",
    "print(\"Data normalized successfully! / \u6570\u636e\u6807\u51c6\u5316\u6210\u529f\uff01\")\n",
    "print(f\"Features - Mean: {X1_scaled.mean():.4f}, Std: {X1_scaled.std():.4f}\")\n",
    "print(f\"Target - Mean: {y1_scaled.mean():.4f}, Std: {y1_scaled.std():.4f}\")\n",
    "\n",
    "# Normalize Dataset 2 / \u5f52\u4e00\u5316\u6570\u636e\u96c62\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Normalizing Dataset 2 / \u5f52\u4e00\u5316\u6570\u636e\u96c62\")\n",
    "print(\"=\"*60)\n",
    "print(\"NOTE: Normalization will be performed in training loop to avoid data leakage\")\n",
    "print(\"\u6ce8\u610f\uff1a\u5f52\u4e00\u5316\u5c06\u5728\u8bad\u7ec3\u5faa\u73af\u4e2d\u8fdb\u884c\u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\")\n",
    "print(\"=\"*60)\n",
    "# X2_scaled, y2_scaled, scaler_X2, scaler_y2 = normalize_data(X2, y2)  # Commented out to avoid data leakage\n",
    "print(\"Data normalized successfully! / \u6570\u636e\u6807\u51c6\u5316\u6210\u529f\uff01\")\n",
    "print(f\"Features - Mean: {X2_scaled.mean():.4f}, Std: {X2_scaled.std():.4f}\")\n",
    "print(f\"Target - Mean: {y2_scaled.mean():.4f}, Std: {y2_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequence-header",
   "metadata": {},
   "source": [
    "### 4.3 \u521b\u5efa\u65f6\u95f4\u5e8f\u5217\u5e8f\u5217 / Create Time Series Sequences\n",
    "\n",
    "\u5c06\u6570\u636e\u8f6c\u6362\u4e3a RNN \u8f93\u5165\u5e8f\u5217\u3002\u6211\u4eec\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\uff1a\n",
    "\n",
    "Transform data into sequences for RNN input. We use a sliding window approach:\n",
    "- \u8f93\u5165\uff1a\u8fc7\u53bb `seq_length` \u4e2a\u65f6\u95f4\u6b65 / Input: Past `seq_length` time steps\n",
    "- \u8f93\u51fa\uff1a\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u6cb9\u6e29 / Output: Next time step's oil temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q6k6l347mmg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction configurations / \u5b9a\u4e49\u9884\u6d4b\u914d\u7f6e\n",
    "# Based on dataset.md requirements / \u57fa\u4e8e dataset.md \u7684\u8981\u6c42\n",
    "\n",
    "PREDICTION_CONFIGS = {\n",
    "    'hour': {\n",
    "        'name': 'Hour / \u5c0f\u65f6',\n",
    "        'offset': 4,      # Start from 4 time points before target / \u4ece\u76ee\u6807\u524d4\u4e2a\u65f6\u95f4\u70b9\u5f00\u59cb\n",
    "        'seq_length': 24, # Use 24 time points (1.5 hours, tunable) / \u4f7f\u752824\u4e2a\u65f6\u95f4\u70b9\uff081.5\u5c0f\u65f6\uff0c\u53ef\u8c03\uff09\n",
    "        'description': 'Predict 1 hour ahead / \u9884\u6d4b1\u5c0f\u65f6\u540e'\n",
    "    },\n",
    "    'day': {\n",
    "        'name': 'Day / \u5929',\n",
    "        'offset': 96,     # Start from 96 time points before target (1 day) / \u4ece\u76ee\u6807\u524d96\u4e2a\u65f6\u95f4\u70b9\u5f00\u59cb\uff081\u5929\uff09\n",
    "        'seq_length': 96, # Use 96 time points (1 day, tunable) / \u4f7f\u752896\u4e2a\u65f6\u95f4\u70b9\uff081\u5929\uff0c\u53ef\u8c03\uff09\n",
    "        'description': 'Predict 1 day ahead / \u9884\u6d4b1\u5929\u540e'\n",
    "    },\n",
    "    'week': {\n",
    "        'name': 'Week / \u5468',\n",
    "        'offset': 672,    # Start from 672 time points before target (1 week) / \u4ece\u76ee\u6807\u524d672\u4e2a\u65f6\u95f4\u70b9\u5f00\u59cb\uff081\u5468\uff09\n",
    "        'seq_length': 168, # Use 168 time points (1 week, tunable) / \u4f7f\u7528168\u4e2a\u65f6\u95f4\u70b9\uff081\u5468\uff0c\u53ef\u8c03\uff09\n",
    "        'description': 'Predict 1 week ahead / \u9884\u6d4b1\u5468\u540e'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Prediction Configurations / \u9884\u6d4b\u914d\u7f6e\")\n",
    "print(\"=\"*60)\n",
    "for key, config in PREDICTION_CONFIGS.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  {config['description']}\")\n",
    "    print(f\"  Offset / \u504f\u79fb: {config['offset']} time points\")\n",
    "    print(f\"  Sequence Length / \u5e8f\u5217\u957f\u5ea6: {config['seq_length']} time points\")\n",
    "    print(f\"  Total lookback / \u603b\u56de\u6eaf: {config['offset'] + config['seq_length']} time points\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote / \u6ce8\u610f:\")\n",
    "print(\"- Sequence length (N) is tunable for optimization\")\n",
    "print(\"  \u5e8f\u5217\u957f\u5ea6 N \u53ef\u8c03\u6574\u4ee5\u4f18\u5316\u6027\u80fd\")\n",
    "print(\"- We will train 6 models total: 2 datasets \u00d7 3 prediction horizons\")\n",
    "print(\"  \u6211\u4eec\u5c06\u8bad\u7ec36\u4e2a\u6a21\u578b\uff1a2\u4e2a\u6570\u636e\u96c6 \u00d7 3\u79cd\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_offset(X, y, seq_length=24, offset=4):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction with offset\n",
    "    \u521b\u5efa\u5e26\u504f\u79fb\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5e8f\u5217\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features array\n",
    "    y : np.ndarray\n",
    "        Target array\n",
    "    seq_length : int\n",
    "        Number of time steps to look back (default: 24)\n",
    "        \u56de\u6eaf\u7684\u65f6\u95f4\u6b65\u6570\uff08\u9ed8\u8ba4\uff1a24\uff09\n",
    "    offset : int\n",
    "        Prediction time offset from current point\n",
    "        \u4ece\u5f53\u524d\u70b9\u9884\u6d4b\u7684\u65f6\u95f4\u504f\u79fb\n",
    "        - offset=4: predict 1 hour ahead (4 \u00d7 15min = 1 hour)\n",
    "        - offset=96: predict 1 day ahead (96 \u00d7 15min = 24 hours)\n",
    "        - offset=672: predict 1 week ahead (672 \u00d7 15min = 7 days)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_seq : np.ndarray\n",
    "        Sequences of features (samples, seq_length, n_features)\n",
    "        From time t-offset-seq_length to t-offset-1\n",
    "    y_seq : np.ndarray\n",
    "        Target values at time t (future target)\n",
    "        \n",
    "    Example / \u793a\u4f8b:\n",
    "    ---------------\n",
    "    For predicting 1 hour ahead (offset=4, seq_length=24):\n",
    "    - To predict at time t=100:\n",
    "      - Input: X[72:96] (from t-28 to t-5, excluding t-4 to t-1)\n",
    "      - Target: y[100] (the value at t)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    # Total lookback required / \u9700\u8981\u7684\u603b\u56de\u6eaf\u957f\u5ea6\n",
    "    total_lookback = offset + seq_length\n",
    "    \n",
    "    # Start from where we have enough history / \u4ece\u6709\u8db3\u591f\u5386\u53f2\u6570\u636e\u7684\u5730\u65b9\u5f00\u59cb\n",
    "    for i in range(total_lookback, len(X)):\n",
    "        # Input sequence: from t-offset-seq_length to t-offset-1\n",
    "        # \u8f93\u5165\u5e8f\u5217\uff1a\u4ece t-offset-seq_length \u5230 t-offset-1\n",
    "        seq_start = i - offset - seq_length\n",
    "        seq_end = i - offset\n",
    "        X_seq.append(X[seq_start:seq_end])\n",
    "        \n",
    "        # Target: value at time t (future point)\n",
    "        # \u76ee\u6807\uff1a\u65f6\u95f4\u70b9 t \u7684\u503c\uff08\u672a\u6765\u70b9\uff09\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Demonstrate with default configuration (hour prediction)\n",
    "# \u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u6f14\u793a\uff08\u5c0f\u65f6\u9884\u6d4b\uff09\n",
    "seq_length_demo = 24\n",
    "offset_demo = 4\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Sequence Creation Demo / \u5e8f\u5217\u521b\u5efa\u6f14\u793a\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Demo configuration: Hour prediction / \u6f14\u793a\u914d\u7f6e\uff1a\u5c0f\u65f6\u9884\u6d4b\")\n",
    "print(f\"  Sequence length: {seq_length_demo}\")\n",
    "print(f\"  Offset: {offset_demo}\")\n",
    "print(f\"  Total lookback: {seq_length_demo + offset_demo} time points\")\n",
    "\n",
    "print(f\"\\nExample / \u793a\u4f8b:\")\n",
    "print(f\"  To predict at time t=100:\")\n",
    "print(f\"    - Input: time points {100-offset_demo-seq_length_demo} to {100-offset_demo-1}\")\n",
    "print(f\"             (t-{offset_demo+seq_length_demo} to t-{offset_demo+1})\")\n",
    "print(f\"    - Target: time point {100} (t)\")\n",
    "print(f\"\\n  \u4e3a\u4e86\u9884\u6d4b\u65f6\u95f4\u70b9 t=100:\")\n",
    "print(f\"    - \u8f93\u5165\uff1a\u65f6\u95f4\u70b9 {100-offset_demo-seq_length_demo} \u5230 {100-offset_demo-1}\")\n",
    "print(f\"    - \u76ee\u6807\uff1a\u65f6\u95f4\u70b9 {100}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: We will create sequences for each prediction configuration later\")\n",
    "print(\"\u6ce8\u610f\uff1a\u6211\u4eec\u7a0d\u540e\u4f1a\u4e3a\u6bcf\u79cd\u9884\u6d4b\u914d\u7f6e\u521b\u5efa\u5e8f\u5217\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "### 4.4 \u8bad\u7ec3\u96c6/\u6d4b\u8bd5\u96c6\u5206\u5272 / Train/Test Split\n",
    "\n",
    "\u5c06\u6570\u636e\u5206\u5272\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u3002\u5bf9\u4e8e\u65f6\u95f4\u5e8f\u5217\uff0c\u6211\u4eec\u4f7f\u7528\u65f6\u95f4\u5206\u5272\uff08\u975e\u968f\u673a\uff09\u3002\n",
    "\n",
    "Split data into training and testing sets. For time series, we use temporal split (not random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement group-based train/test split as recommended in dataset.md\n",
    "# \u6309\u7167 dataset.md \u63a8\u8350\u7684\u65b9\u5f0f\u5b9e\u73b0\u57fa\u4e8e\u5206\u7ec4\u7684\u8bad\u7ec3/\u6d4b\u8bd5\u62c6\u5206\n",
    "\n",
    "def split_into_groups(data_length, n_groups=20):\n",
    "    \"\"\"\n",
    "    Divide data into n_groups of continuous time point groups\n",
    "    \u5c06\u6570\u636e\u5206\u6210 n_groups \u4e2a\u8fde\u7eed\u7684\u65f6\u95f4\u70b9\u7ec4\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_length : int\n",
    "        Total length of data\n",
    "    n_groups : int\n",
    "        Number of groups to split into (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    groups : list of tuples\n",
    "        List of (start_idx, end_idx) for each group\n",
    "    \"\"\"\n",
    "    group_size = data_length // n_groups\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        start_idx = i * group_size\n",
    "        # Last group takes all remaining data\n",
    "        end_idx = start_idx + group_size if i < n_groups - 1 else data_length\n",
    "        groups.append((start_idx, end_idx))\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def split_train_test_by_groups(X_seq, y_seq, train_ratio=0.8, n_groups=20, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split data by groups as recommended in dataset.md:\n",
    "    \u6309\u7167 dataset.md \u63a8\u8350\u7684\u65b9\u5f0f\u6309\u7ec4\u62c6\u5206\u6570\u636e\uff1a\n",
    "    \n",
    "    1. Divide data into n_groups continuous blocks\n",
    "       \u5c06\u6570\u636e\u5206\u6210 n_groups \u4e2a\u8fde\u7eed\u5757\n",
    "    2. Randomly select train_ratio of groups for training\n",
    "       \u968f\u673a\u9009\u62e9 train_ratio \u7684\u7ec4\u4f5c\u4e3a\u8bad\u7ec3\u96c6\n",
    "    3. Remaining groups for testing\n",
    "       \u5269\u4f59\u7684\u7ec4\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\n",
    "    \n",
    "    This ensures training and test data are completely disjoint\n",
    "    \u8fd9\u786e\u4fdd\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5b8c\u5168\u4e0d\u91cd\u53e0\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_seq, y_seq : np.ndarray\n",
    "        Sequence data\n",
    "    train_ratio : float\n",
    "        Proportion of groups for training (default: 0.8 = 16 out of 20 groups)\n",
    "    n_groups : int\n",
    "        Number of groups to split into (default: 20)\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get group indices\n",
    "    groups = split_into_groups(len(X_seq), n_groups)\n",
    "    n_train_groups = int(n_groups * train_ratio)\n",
    "    \n",
    "    # Randomly select training groups\n",
    "    # \u968f\u673a\u9009\u62e9\u8bad\u7ec3\u7ec4\n",
    "    group_indices = np.arange(n_groups)\n",
    "    np.random.shuffle(group_indices)\n",
    "    train_group_indices = sorted(group_indices[:n_train_groups])\n",
    "    test_group_indices = sorted(group_indices[n_train_groups:])\n",
    "    \n",
    "    # Extract data from selected groups\n",
    "    # \u4ece\u9009\u5b9a\u7684\u7ec4\u4e2d\u63d0\u53d6\u6570\u636e\n",
    "    train_data_X = []\n",
    "    train_data_y = []\n",
    "    test_data_X = []\n",
    "    test_data_y = []\n",
    "    \n",
    "    for idx in train_group_indices:\n",
    "        start, end = groups[idx]\n",
    "        train_data_X.append(X_seq[start:end])\n",
    "        train_data_y.append(y_seq[start:end])\n",
    "    \n",
    "    for idx in test_group_indices:\n",
    "        start, end = groups[idx]\n",
    "        test_data_X.append(X_seq[start:end])\n",
    "        test_data_y.append(y_seq[start:end])\n",
    "    \n",
    "    X_train = np.concatenate(train_data_X, axis=0)\n",
    "    y_train = np.concatenate(train_data_y, axis=0)\n",
    "    X_test = np.concatenate(test_data_X, axis=0)\n",
    "    y_test = np.concatenate(test_data_y, axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, train_group_indices, test_group_indices\n",
    "\n",
    "# Demo: Show the splitting strategy\n",
    "# \u6f14\u793a\uff1a\u5c55\u793a\u62c6\u5206\u7b56\u7565\n",
    "print(\"=\"*60)\n",
    "print(\"Group-Based Data Splitting Strategy / \u57fa\u4e8e\u5206\u7ec4\u7684\u6570\u636e\u62c6\u5206\u7b56\u7565\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAs recommended in dataset.md:\")\n",
    "print(\"\u6309\u7167 dataset.md \u7684\u5efa\u8bae\uff1a\")\n",
    "print(\"\\n1. Divide data into 20 continuous time point groups\")\n",
    "print(\"   \u5c06\u6570\u636e\u5206\u621020\u4e2a\u8fde\u7eed\u7684\u65f6\u95f4\u70b9\u7ec4\")\n",
    "print(\"2. Randomly select 80% (16 groups) for training\")\n",
    "print(\"   \u968f\u673a\u9009\u62e980%\uff0816\u7ec4\uff09\u7528\u4e8e\u8bad\u7ec3\")\n",
    "print(\"3. Remaining 20% (4 groups) for testing\")\n",
    "print(\"   \u5269\u4f5920%\uff084\u7ec4\uff09\u7528\u4e8e\u6d4b\u8bd5\")\n",
    "print(\"\\nAdvantages / \u4f18\u52bf:\")\n",
    "print(\"\u2713 Training and test data are completely disjoint\")\n",
    "print(\"  \u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5b8c\u5168\u4e0d\u91cd\u53e0\")\n",
    "print(\"\u2713 Maintains temporal continuity within each group\")\n",
    "print(\"  \u4fdd\u6301\u6bcf\u7ec4\u5185\u7684\u65f6\u95f4\u8fde\u7eed\u6027\")\n",
    "print(\"\u2713 Random selection reduces bias\")\n",
    "print(\"  \u968f\u673a\u9009\u62e9\u51cf\u5c11\u504f\u5dee\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: Actual splitting will be performed during model training\")\n",
    "print(\"\u6ce8\u610f\uff1a\u5b9e\u9645\u62c6\u5206\u5c06\u5728\u6a21\u578b\u8bad\u7ec3\u671f\u95f4\u6267\u884c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 5: \u6784\u5efa RNN \u6a21\u578b / Step 5: Build RNN Model\n",
    "\n",
    "\u6211\u4eec\u5c06\u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684 RNN \u6a21\u578b\uff0c\u5305\u542b\uff1a\n",
    "\n",
    "We'll build a Simple RNN model with:\n",
    "- \u8f93\u5165\u5c42\uff1a\u7279\u5f81\u5e8f\u5217 / Input layer: sequences of features\n",
    "- RNN \u5c42\uff1a\u6355\u83b7\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb / RNN layers: to capture temporal dependencies\n",
    "- Dropout \u5c42\uff1a\u7528\u4e8e\u6b63\u5219\u5316 / Dropout layers: for regularization\n",
    "- \u5168\u8fde\u63a5\u8f93\u51fa\u5c42\uff1a\u7528\u4e8e\u56de\u5f52 / Dense output layer: for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbixy6jwfm7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create DataLoaders / \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\u7684\u8f85\u52a9\u51fd\u6570\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders from numpy arrays\n",
    "    \u4ecenumpy\u6570\u7ec4\u521b\u5efaPyTorch DataLoader\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : np.ndarray\n",
    "        Training features and labels / \u8bad\u7ec3\u7279\u5f81\u548c\u6807\u7b7e\n",
    "    X_test, y_test : np.ndarray\n",
    "        Test features and labels / \u6d4b\u8bd5\u7279\u5f81\u548c\u6807\u7b7e\n",
    "    batch_size : int\n",
    "        Batch size for training / \u8bad\u7ec3\u6279\u6b21\u5927\u5c0f\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Training and test data loaders / \u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u52a0\u8f7d\u5668\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to PyTorch tensors / \u5c06numpy\u6570\u7ec4\u8f6c\u6362\u4e3aPyTorch\u5f20\u91cf\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)  # Add dimension for labels\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    \n",
    "    # Create TensorDatasets / \u521b\u5efaTensorDataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Create DataLoaders / \u521b\u5efaDataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"create_dataloaders function defined successfully!\")\n",
    "print(\"create_dataloaders \u51fd\u6570\u5b9a\u4e49\u6210\u529f\uff01\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Simple RNN Model class / \u5b9a\u4e49\u7b80\u5355RNN\u6a21\u578b\u7c7b\n",
    "\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN model for time series prediction\n",
    "    \u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7b80\u5355RNN\u6a21\u578b\n",
    "    \n",
    "    Architecture / \u67b6\u6784:\n",
    "    - Multiple RNN layers with dropout / \u591a\u4e2a\u5e26dropout\u7684RNN\u5c42\n",
    "    - Fully connected output layers / \u5168\u8fde\u63a5\u8f93\u51fa\u5c42\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features / \u8f93\u5165\u7279\u5f81\u6570\n",
    "        hidden_sizes : list of int\n",
    "            List of hidden layer sizes for RNN / RNN\u9690\u85cf\u5c42\u5927\u5c0f\u5217\u8868\n",
    "        dropout : float\n",
    "            Dropout rate / Dropout\u7387\n",
    "        \"\"\"\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        \n",
    "        # Create RNN layers / \u521b\u5efaRNN\u5c42\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        \n",
    "        # First RNN layer / \u7b2c\u4e00\u4e2aRNN\u5c42\n",
    "        self.rnn_layers.append(\n",
    "            nn.RNN(input_size, hidden_sizes[0], batch_first=True)\n",
    "        )\n",
    "        \n",
    "        # Additional RNN layers / \u989d\u5916\u7684RNN\u5c42\n",
    "        for i in range(1, self.num_layers):\n",
    "            self.rnn_layers.append(\n",
    "                nn.RNN(hidden_sizes[i-1], hidden_sizes[i], batch_first=True)\n",
    "            )\n",
    "        \n",
    "        # Dropout layers / Dropout\u5c42\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layers / \u5168\u8fde\u63a5\u5c42\n",
    "        self.fc1 = nn.Linear(hidden_sizes[-1], 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass / \u524d\u5411\u4f20\u64ad\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input sequences (batch_size, seq_length, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor\n",
    "            Predictions (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Pass through RNN layers / \u901a\u8fc7RNN\u5c42\n",
    "        for i, rnn_layer in enumerate(self.rnn_layers):\n",
    "            x, _ = rnn_layer(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Take the last time step output / \u53d6\u6700\u540e\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u8f93\u51fa\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers / \u901a\u8fc7\u5168\u8fde\u63a5\u5c42\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SimpleRNNModel class defined successfully!\")\n",
    "print(\"SimpleRNNModel \u7c7b\u5b9a\u4e49\u6210\u529f\uff01\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture remains the same from previous cell (Simple RNNModel)\n",
    "# \u6a21\u578b\u67b6\u6784\u4e0e\u4e4b\u524d\u7684cell\u76f8\u540c\uff08SimpleRNNModel\uff09\n",
    "\n",
    "# Model parameters / \u6a21\u578b\u53c2\u6570\n",
    "input_size = 6  # Number of features: HUFL, HULL, MUFL, MULL, LUFL, LULL / \u7279\u5f81\u6570\u91cf\n",
    "hidden_sizes = [64, 32]  # RNN hidden units / RNN \u9690\u85cf\u5355\u5143\n",
    "dropout = 0.2  # Dropout rate / Dropout \u7387\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Model Architecture / \u6a21\u578b\u67b6\u6784\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSimpleRNNModel:\")\n",
    "print(f\"  Input size / \u8f93\u5165\u5927\u5c0f: {input_size} features\")\n",
    "print(f\"  Hidden layers / \u9690\u85cf\u5c42: {hidden_sizes}\")\n",
    "print(f\"  Dropout rate / Dropout\u7387: {dropout}\")\n",
    "print(f\"\\nArchitecture flow / \u67b6\u6784\u6d41\u7a0b:\")\n",
    "print(f\"  {input_size} (features) \u2192 RNN({hidden_sizes[0]}) \u2192 Dropout({dropout})\")\n",
    "print(f\"  \u2192 RNN({hidden_sizes[1]}) \u2192 Dropout({dropout})\")\n",
    "print(f\"  \u2192 FC(16) \u2192 ReLU \u2192 Dropout \u2192 FC(1)\")\n",
    "print(f\"\\nOutput / \u8f93\u51fa: Single value (oil temperature prediction)\")\n",
    "print(f\"        \u5355\u4e2a\u503c\uff08\u6cb9\u6e29\u9884\u6d4b\uff09\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a sample model to count parameters / \u521b\u5efa\u793a\u4f8b\u6a21\u578b\u4ee5\u7edf\u8ba1\u53c2\u6570\n",
    "sample_model = SimpleRNNModel(input_size, hidden_sizes, dropout)\n",
    "total_params = sum(p.numel() for p in sample_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in sample_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Parameters / \u6a21\u578b\u53c2\u6570:\")\n",
    "print(f\"  Total / \u603b\u6570: {total_params:,}\")\n",
    "print(f\"  Trainable / \u53ef\u8bad\u7ec3: {trainable_params:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: We will create 6 instances of this model architecture\")\n",
    "print(\"\u6ce8\u610f\uff1a\u6211\u4eec\u5c06\u521b\u5efa\u6b64\u6a21\u578b\u67b6\u6784\u76846\u4e2a\u5b9e\u4f8b\")\n",
    "print(\"  - 2 datasets \u00d7 3 prediction horizons = 6 models\")\n",
    "print(\"  - 2\u4e2a\u6570\u636e\u96c6 \u00d7 3\u79cd\u9884\u6d4b\u65f6\u95f4\u8303\u56f4 = 6\u4e2a\u6a21\u578b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qafk9ulqdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function / \u5b9a\u4e49\u8bad\u7ec3\u51fd\u6570\n",
    "\n",
    "def train_rnn_model(train_loader, test_loader, input_size, hidden_sizes, dropout=0.2,\n",
    "                    num_epochs=100, lr=0.001, patience=10, dataset_name='Model', device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the RNN model with early stopping\n",
    "    \u4f7f\u7528\u65e9\u505c\u8bad\u7ec3RNN\u6a21\u578b\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Training and test data loaders\n",
    "    input_size : int\n",
    "        Number of input features\n",
    "    hidden_sizes : list of int\n",
    "        Hidden layer sizes\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    num_epochs : int\n",
    "        Maximum number of epochs\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    patience : int\n",
    "        Early stopping patience\n",
    "    dataset_name : str\n",
    "        Name for logging\n",
    "    device : str or torch.device\n",
    "        Device to train on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : SimpleRNNModel\n",
    "        Trained model\n",
    "    history : dict\n",
    "        Training history with train/test loss and MAE\n",
    "    \"\"\"\n",
    "    # Initialize model / \u521d\u59cb\u5316\u6a21\u578b\n",
    "    model = SimpleRNNModel(input_size, hidden_sizes, dropout).to(device)\n",
    "    \n",
    "    # Loss function and optimizer / \u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler / \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training history / \u8bad\u7ec3\u5386\u53f2\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_mae': [],\n",
    "        'test_mae': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping variables / \u65e9\u505c\u53d8\u91cf\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training {dataset_name}...\")\n",
    "    print(f\"\u8bad\u7ec3 {dataset_name}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase / \u8bad\u7ec3\u9636\u6bb5\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device).squeeze()  # Fix shape: (batch, 1) \u2192 (batch)\n",
    "            \n",
    "            # Forward pass / \u524d\u5411\u4f20\u64ad\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass / \u53cd\u5411\u4f20\u64ad\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_mae += torch.abs(outputs - y_batch).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mae /= len(train_loader.dataset)\n",
    "        \n",
    "        # Test phase / \u6d4b\u8bd5\u9636\u6bb5\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_mae = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device).squeeze()  # Fix shape: (batch, 1) \u2192 (batch)\n",
    "                \n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "                test_mae += torch.abs(outputs - y_batch).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_mae /= len(test_loader.dataset)\n",
    "        \n",
    "        # Update history / \u66f4\u65b0\u5386\u53f2\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['test_mae'].append(test_mae)\n",
    "        \n",
    "        # Learning rate scheduling / \u5b66\u4e60\u7387\u8c03\u5ea6\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Debug: Gradient monitoring (optional) / \u8c03\u8bd5\uff1a\u68af\u5ea6\u76d1\u63a7\uff08\u53ef\u9009\uff09\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            total_grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)\n",
    "            # Uncomment to print gradient norm / \u53d6\u6d88\u6ce8\u91ca\u4ee5\u6253\u5370\u68af\u5ea6\u8303\u6570\n",
    "            # print(f'  Gradient norm / \u68af\u5ea6\u8303\u6570: {total_grad_norm:.4f}')\n",
    "        \n",
    "        # Print progress every 10 epochs / \u6bcf10\u4e2a\u8f6e\u6b21\u6253\u5370\u8fdb\u5ea6\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "                  f'Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f} | '\n",
    "                  f'Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}')\n",
    "        \n",
    "        # Early stopping check / \u65e9\u505c\u68c0\u67e5\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n",
    "            print(f'\u5728\u7b2c {epoch+1} \u8f6e\u89e6\u53d1\u65e9\u505c')\n",
    "            break\n",
    "    \n",
    "    # Load best model / \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'\\nTraining completed! Best test loss: {best_test_loss:.6f}')\n",
    "    print(f'\u8bad\u7ec3\u5b8c\u6210\uff01\u6700\u4f73\u6d4b\u8bd5\u635f\u5931: {best_test_loss:.6f}\\n')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"=\"*60)print(\"train_rnn_model function defined successfully!\")\n",
    "print(\"train_rnn_model \u51fd\u6570\u5b9a\u4e49\u6210\u529f\uff01\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 6: \u8bad\u7ec3\u6a21\u578b / Step 6: Train the Model\n",
    "\n",
    "\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u8bad\u7ec3 RNN\uff1a\n",
    "\n",
    "Train the RNN with:\n",
    "- Adam \u4f18\u5316\u5668 / Adam optimizer\n",
    "- MSE \u635f\u5931\u51fd\u6570 / MSE loss function\n",
    "- \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff1a\u63d0\u9ad8\u6536\u655b\u6027 / Learning rate scheduler: improve convergence\n",
    "- \u65e9\u505c\uff1a\u9632\u6b62\u8fc7\u62df\u5408 / Early stopping: prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training all 6 models: 2 datasets \u00d7 3 prediction horizons\n",
    "# \u8bad\u7ec3\u6240\u67096\u4e2a\u6a21\u578b\uff1a2\u4e2a\u6570\u636e\u96c6 \u00d7 3\u79cd\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\n",
    "\n",
    "# Storage for all models and results / \u5b58\u50a8\u6240\u6709\u6a21\u578b\u548c\u7ed3\u679c\n",
    "all_models = {}\n",
    "all_histories = {}\n",
    "all_predictions = {}\n",
    "all_metrics = {}\n",
    "all_scalers = {}\n",
    "all_data_info = {}\n",
    "\n",
    "# Datasets configuration / \u6570\u636e\u96c6\u914d\u7f6e\n",
    "datasets_config = {\n",
    "    'Dataset1': {'df': df_train1, 'X': X1, 'y': y1},\n",
    "    'Dataset2': {'df': df_train2, 'X': X2, 'y': y2}\n",
    "}\n",
    "\n",
    "# Training hyperparameters / \u8bad\u7ec3\u8d85\u53c2\u6570\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "batch_size = 32\n",
    "n_groups = 20\n",
    "train_ratio = 0.8\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 6 RNN MODELS / \u8bad\u7ec36\u4e2aRNN\u6a21\u578b\")\n",
    "print(\"=\"*80)print(f\"Configuration / \u914d\u7f6e:\")\n",
    "print(f\"  Datasets / \u6570\u636e\u96c6: 2 (Dataset1, Dataset2)\")\n",
    "print(f\"  Prediction horizons / \u9884\u6d4b\u65f6\u95f4\u8303\u56f4: 3 (hour, day, week)\")\n",
    "print(f\"  Total models / \u603b\u6a21\u578b\u6570: 6\")\n",
    "print(f\"  Epochs / \u8f6e\u6570: {num_epochs}\")\n",
    "print(f\"  Batch size / \u6279\u6b21\u5927\u5c0f: {batch_size}\")\n",
    "print(f\"  Data split / \u6570\u636e\u62c6\u5206: {n_groups} groups, {int(train_ratio*100)}% train\")\n",
    "print(\"=\"*80)",
    "# Train each model / \u8bad\u7ec3\u6bcf\u4e2a\u6a21\u578b\n",
    "model_counter = 0\n",
    "\n",
    "for dataset_name, dataset_config in datasets_config.items():\n",
    "    for pred_key, pred_config in PREDICTION_CONFIGS.items():\n",
    "        model_counter += 1\n",
    "        model_name = f\"{dataset_name}_{pred_key}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{model_counter}/6] Training Model: {model_name}\")\n",
    "        print(f\"[{model_counter}/6] \u8bad\u7ec3\u6a21\u578b\uff1a{model_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Configuration / \u914d\u7f6e:\")\n",
    "        print(f\"  Dataset / \u6570\u636e\u96c6: {dataset_name}\")\n",
    "        print(f\"  Prediction type / \u9884\u6d4b\u7c7b\u578b: {pred_config['description']}\")\n",
    "        print(f\"  Offset / \u504f\u79fb: {pred_config['offset']} time points\")\n",
    "        print(f\"  Sequence length / \u5e8f\u5217\u957f\u5ea6: {pred_config['seq_length']} time points\")\n",
    "        print(f\"  Total lookback / \u603b\u56de\u6eaf: {pred_config['offset'] + pred_config['seq_length']} time points\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # 1. Get raw data (not normalized yet) / \u83b7\u53d6\u539f\u59cb\u6570\u636e\uff08\u5c1a\u672a\u5f52\u4e00\u5316\uff09\n",
    "        X_raw = dataset_config['X']\n",
    "        y_raw = dataset_config['y']\n",
    "        \n",
    "        # 2. Create sequences with offset (on raw data) / \u4f7f\u7528\u504f\u79fb\u521b\u5efa\u5e8f\u5217\uff08\u5728\u539f\u59cb\u6570\u636e\u4e0a\uff09\n",
    "        X_seq, y_seq = create_sequences_with_offset(\n",
    "            X_raw, y_raw,\n",
    "            seq_length=pred_config['seq_length'],\n",
    "            offset=pred_config['offset']\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(X_seq):,} sequences / \u521b\u5efa\u4e86{len(X_seq):,}\u4e2a\u5e8f\u5217\")\n",
    "        \n",
    "        # 3. Split data by groups / \u6309\u7ec4\u62c6\u5206\u6570\u636e\n",
    "        X_train, X_test, y_train, y_test, train_groups, test_groups = split_train_test_by_groups(\n",
    "            X_seq, y_seq, train_ratio=train_ratio, n_groups=n_groups\n",
    "        )\n",
    "        \n",
    "        print(f\"Split into groups / \u62c6\u5206\u4e3a\u7ec4:\")\n",
    "        print(f\"  Train groups / \u8bad\u7ec3\u7ec4: {train_groups}\")\n",
    "        print(f\"  Test groups / \u6d4b\u8bd5\u7ec4: {test_groups}\")\n",
    "        print(f\"  Train samples / \u8bad\u7ec3\u6837\u672c: {len(X_train):,}\")\n",
    "        print(f\"  Test samples / \u6d4b\u8bd5\u6837\u672c: {len(X_test):,}\\n\")\n",
    "        \n",
    "        # 4. Normalize data (fit on train only) / \u5f52\u4e00\u5316\u6570\u636e\uff08\u53ea\u5728\u8bad\u7ec3\u96c6\u4e0a fit\uff09\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        \n",
    "        # Normalize X (3D array: samples x timesteps x features)\n",
    "        # \u5f52\u4e00\u5316 X\uff083D\u6570\u7ec4\uff1a\u6837\u672c\u6570 x \u65f6\u95f4\u6b65 x \u7279\u5f81\u6570\uff09\n",
    "        n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "        X_train_2d = X_train.reshape(-1, n_features)\n",
    "        scaler_X.fit(X_train_2d)  # Fit only on train data / \u53ea\u5728\u8bad\u7ec3\u6570\u636e\u4e0a fit\n",
    "        X_train_scaled = scaler_X.transform(X_train_2d).reshape(n_samples_train, n_timesteps, n_features)\n",
    "        \n",
    "        # Transform test data with the same scaler / \u4f7f\u7528\u76f8\u540c\u7684 scaler \u8f6c\u6362\u6d4b\u8bd5\u6570\u636e\n",
    "        X_test_2d = X_test.reshape(-1, n_features)\n",
    "        X_test_scaled = scaler_X.transform(X_test_2d).reshape(X_test.shape)\n",
    "        \n",
    "        # Normalize y / \u5f52\u4e00\u5316 y\n",
    "        scaler_y.fit(y_train.reshape(-1, 1))  # Fit only on train data / \u53ea\u5728\u8bad\u7ec3\u6570\u636e\u4e0a fit\n",
    "        y_train_scaled = scaler_y.transform(y_train.reshape(-1, 1)).flatten()\n",
    "        y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        print(f\"Data normalized (scaler fit on train only) / \u6570\u636e\u5df2\u5f52\u4e00\u5316\uff08scaler \u53ea\u5728\u8bad\u7ec3\u96c6\u4e0a fit\uff09\\n\")\n",
    "        \n",
    "        # 5. Create DataLoaders / \u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668\n",
    "        train_loader, test_loader = create_dataloaders(\n",
    "            X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # 6. Train model / \u8bad\u7ec3\u6a21\u578b\n",
    "        print(f\"Starting training... / \u5f00\u59cb\u8bad\u7ec3...\\n\")\n",
    "        model, history = train_rnn_model(\n",
    "            train_loader, test_loader,\n",
    "            input_size=X_train.shape[2],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=learning_rate,\n",
    "            patience=patience,\n",
    "            dataset_name=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 7. Store results / \u5b58\u50a8\u7ed3\u679c\n",
    "        all_models[model_name] = model\n",
    "        all_histories[model_name] = history\n",
    "        all_scalers[model_name] = scaler_y  # Use the scaler created in step 4 / \u4f7f\u7528\u6b65\u9aa44\u4e2d\u521b\u5efa\u7684scaler\n",
    "        all_data_info[model_name] = {\n",
    "            'train_groups': train_groups,\n",
    "            'test_groups': test_groups,\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'train_loader': train_loader,\n",
    "            'test_loader': test_loader\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n\u2713 Model {model_name} training completed!\")\n",
    "        print(f\"\u2713 \u6a21\u578b {model_name} \u8bad\u7ec3\u5b8c\u6210\uff01\")\n",
    "        print(f\"  Best test loss: {min(history['test_loss']):.6f}\")\n",
    "        print(f\"  \u6700\u4f73\u6d4b\u8bd5\u635f\u5931: {min(history['test_loss']):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL 6 MODELS TRAINING COMPLETED! / \u6240\u67096\u4e2a\u6a21\u578b\u8bad\u7ec3\u5b8c\u6210\uff01\")\n",
    "print(\"=\"*80)print(f\"\\nTrained models / \u5df2\u8bad\u7ec3\u6a21\u578b:\")\n",
    "for i, model_name in enumerate(all_models.keys(), 1):\n",
    "    best_loss = min(all_histories[model_name]['test_loss'])\n",
    "    print(f\"  {i}. {model_name}: Best Test Loss = {best_loss:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "### 6.1 \u8bad\u7ec3\u5386\u53f2\u53ef\u89c6\u5316 / Training History Visualization\n",
    "\n",
    "\u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u66f2\u7ebf\u4ee5\u68c0\u67e5\u8fc7\u62df\u5408\n",
    "\n",
    "Plot training and validation loss curves to check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history for all 6 models / \u53ef\u89c6\u5316\u6240\u67096\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u5386\u53f2\n",
    "\n",
    "# 1. Individual model training histories / \u5404\u6a21\u578b\u8bad\u7ec3\u5386\u53f2\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_names = list(all_histories.keys())\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    history = all_histories[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.plot(history['train_loss'], label='Train Loss / \u8bad\u7ec3\u635f\u5931', linewidth=2, alpha=0.7)\n",
    "    ax.plot(history['test_loss'], label='Test Loss / \u6d4b\u8bd5\u635f\u5931', linewidth=2, alpha=0.7)\n",
    "    ax.set_title(f'{model_name} - Training History / \u8bad\u7ec3\u5386\u53f2', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / \u8f6e\u6b21')\n",
    "    ax.set_ylabel('Loss (MSE) / \u635f\u5931')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Training History for All 6 Models / \u6240\u67096\u4e2a\u6a21\u578b\u7684\u8bad\u7ec3\u5386\u53f2', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 2. Comparison by prediction type / \u6309\u9884\u6d4b\u7c7b\u578b\u5bf9\u6bd4\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "for idx, pred_type in enumerate(['hour', 'day', 'week']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for dataset in ['Dataset1', 'Dataset2']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        history = all_histories[model_name]\n",
    "        ax.plot(history['test_loss'], label=dataset, linewidth=2.5)\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    ax.set_title(f'{pred_config[\"name\"]} Prediction Comparison\\n{pred_config[\"description\"]}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / \u8f6e\u6b21')\n",
    "    ax.set_ylabel('Test Loss (MSE) / \u6d4b\u8bd5\u635f\u5931')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison by Prediction Type / \u6309\u9884\u6d4b\u7c7b\u578b\u5bf9\u6bd4', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparison by dataset / \u6309\u6570\u636e\u96c6\u5bf9\u6bd4\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for idx, dataset in enumerate(['Dataset1', 'Dataset2']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for pred_type in ['hour', 'day', 'week']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        history = all_histories[model_name]\n",
    "        pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "        ax.plot(history['test_loss'], label=pred_config['name'], linewidth=2.5)\n",
    "    \n",
    "    ax.set_title(f'{dataset} - All Prediction Types\\n{dataset} - \u6240\u6709\u9884\u6d4b\u7c7b\u578b', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / \u8f6e\u6b21')\n",
    "    ax.set_ylabel('Test Loss (MSE) / \u6d4b\u8bd5\u635f\u5931')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison by Dataset / \u6309\u6570\u636e\u96c6\u5bf9\u6bd4', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics summary / \u6253\u5370\u6700\u7ec8\u6307\u6807\u603b\u7ed3\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL TRAINING METRICS SUMMARY / \u6700\u7ec8\u8bad\u7ec3\u6307\u6807\u603b\u7ed3\")\n",
    "print(\"=\"*80)\n",
    "for model_name in model_names:\n",
    "    history = all_histories[model_name]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Final Test Loss / \u6700\u7ec8\u6d4b\u8bd5\u635f\u5931: {history['test_loss'][-1]:.6f}\")\n",
    "    print(f\"  Best Test Loss / \u6700\u4f73\u6d4b\u8bd5\u635f\u5931: {min(history['test_loss']):.6f}\")\n",
    "    print(f\"  Final Test MAE / \u6700\u7ec8\u6d4b\u8bd5MAE: {history['test_mae'][-1]:.6f}\")\n",
    "    print(f\"  Best Test MAE / \u6700\u4f73\u6d4b\u8bd5MAE: {min(history['test_mae']):.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 7: \u6a21\u578b\u8bc4\u4f30 / Step 7: Model Evaluation\n",
    "\n",
    "### 7.1 \u751f\u6210\u9884\u6d4b / Make Predictions\n",
    "\n",
    "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u751f\u6210\u9884\u6d4b\u5e76\u53cd\u5411\u8f6c\u6362\u5230\u539f\u59cb\u5c3a\u5ea6\n",
    "\n",
    "Generate predictions on test set and inverse transform to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch6fv9outch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function / \u5b9a\u4e49\u9884\u6d4b\u51fd\u6570\n",
    "\n",
    "def make_predictions(model, train_loader, test_loader, scaler_y, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate predictions on train and test data\n",
    "    \u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u4e0a\u751f\u6210\u9884\u6d4b\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SimpleRNNModel\n",
    "        Trained model\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Data loaders\n",
    "    scaler_y : StandardScaler\n",
    "        Scaler for inverse transformation / \u7528\u4e8e\u53cd\u6807\u51c6\u5316\u7684scaler\n",
    "    device : str or torch.device\n",
    "        Device for inference\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_train_pred : np.ndarray\n",
    "        Training predictions (original scale / \u539f\u59cb\u5c3a\u5ea6)\n",
    "    y_test_pred : np.ndarray\n",
    "        Test predictions (original scale / \u539f\u59cb\u5c3a\u5ea6)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Training predictions / \u8bad\u7ec3\u9884\u6d4b\n",
    "    train_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            train_predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_train_pred = np.concatenate(train_predictions)\n",
    "    \n",
    "    # Test predictions / \u6d4b\u8bd5\u9884\u6d4b\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            test_predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_test_pred = np.concatenate(test_predictions)\n",
    "    \n",
    "    # \u2705 Denormalize predictions to original scale / \u5c06\u9884\u6d4b\u503c\u53cd\u6807\u51c6\u5316\u5230\u539f\u59cb\u5c3a\u5ea6\n",
    "    y_train_pred = scaler_y.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_pred = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "print(\"=\"*60)",
    "print(\"make_predictions function defined successfully!\")\n",
    "print(\"make_predictions \u51fd\u6570\u5b9a\u4e49\u6210\u529f\uff01\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all 6 models / \u4e3a\u6240\u67096\u4e2a\u6a21\u578b\u751f\u6210\u9884\u6d4b\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS FOR ALL 6 MODELS\")\n",
    "print(\"\u4e3a\u6240\u67096\u4e2a\u6a21\u578b\u751f\u6210\u9884\u6d4b\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in all_models.keys():\n",
    "    print(f\"\\nGenerating predictions for {model_name}...\")\n",
    "    print(f\"\u4e3a {model_name} \u751f\u6210\u9884\u6d4b...\")\n",
    "    \n",
    "    model = all_models[model_name]\n",
    "    scaler_y = all_scalers[model_name]\n",
    "    data_info = all_data_info[model_name]\n",
    "    \n",
    "    # Generate predictions / \u751f\u6210\u9884\u6d4b\n",
    "    y_train_pred, y_test_pred = make_predictions(\n",
    "        model, data_info['train_loader'], data_info['test_loader'], scaler_y, device\n",
    "    )\n",
    "    \n",
    "    # Inverse transform to original scale / \u53cd\u5411\u8f6c\u6362\u5230\u539f\u59cb\u5c3a\u5ea6\n",
    "    y_train_actual = scaler_y.inverse_transform(data_info['y_train'].reshape(-1, 1)).flatten()\n",
    "    y_train_pred_inv = scaler_y.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = scaler_y.inverse_transform(data_info['y_test'].reshape(-1, 1)).flatten()\n",
    "    y_test_pred_inv = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Store predictions / \u5b58\u50a8\u9884\u6d4b\n",
    "    all_predictions[model_name] = {\n",
    "        'y_train_actual': y_train_actual,\n",
    "        'y_train_pred': y_train_pred_inv,\n",
    "        'y_test_actual': y_test_actual,\n",
    "        'y_test_pred': y_test_pred_inv\n",
    "    }\n",
    "    \n",
    "    print(f\"  \u2713 Predictions generated / \u9884\u6d4b\u5df2\u751f\u6210\")\n",
    "    print(f\"    Train samples / \u8bad\u7ec3\u6837\u672c: {len(y_train_actual):,}\")\n",
    "    print(f\"    Test samples / \u6d4b\u8bd5\u6837\u672c: {len(y_test_actual):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL PREDICTIONS GENERATED! / \u6240\u6709\u9884\u6d4b\u5df2\u751f\u6210\uff01\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "### 7.2 \u8ba1\u7b97\u8bc4\u4f30\u6307\u6807 / Calculate Evaluation Metrics\n",
    "\n",
    "\u8ba1\u7b97\u6807\u51c6\u56de\u5f52\u6307\u6807\u4ee5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\n",
    "\n",
    "Compute standard regression metrics to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcx4gj1sddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics calculation function / \u5b9a\u4e49\u6307\u6807\u8ba1\u7b97\u51fd\u6570\n",
    "\n",
    "def calculate_metrics(y_actual, y_pred, dataset_name='Dataset'):\n",
    "    \"\"\"\n",
    "    Calculate regression metrics\n",
    "    \u8ba1\u7b97\u56de\u5f52\u6307\u6807\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_actual : np.ndarray\n",
    "        Actual values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "    dataset_name : str\n",
    "        Name for display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing R2, RMSE, MAE, MAPE\n",
    "    \"\"\"\n",
    "    # R\u00b2 Score / R\u00b2\u5206\u6570\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    \n",
    "    # RMSE (Root Mean Squared Error) / \u5747\u65b9\u6839\u8bef\u5dee\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    \n",
    "    # MAE (Mean Absolute Error) / \u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error) / \u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\n",
    "    # Avoid division by zero / \u907f\u514d\u9664\u96f6\u9519\u8bef\n",
    "    mask = y_actual != 0\n",
    "    mape = np.mean(np.abs((y_actual[mask] - y_pred[mask]) / y_actual[mask])) * 100\n",
    "    \n",
    "    # Store metrics / \u5b58\u50a8\u6307\u6807\n",
    "    metrics = {\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    # Print metrics / \u6253\u5370\u6307\u6807\n",
    "    print(f\"\\n{dataset_name} Performance Metrics / {dataset_name} \u6027\u80fd\u6307\u6807:\")\n",
    "    print(f\"  R\u00b2 Score / R\u00b2\u5206\u6570: {r2:.4f}\")\n",
    "    print(f\"  RMSE / \u5747\u65b9\u6839\u8bef\u5dee: {rmse:.4f}\u00b0C\")\n",
    "    print(f\"  MAE / \u5e73\u5747\u7edd\u5bf9\u8bef\u5dee: {mae:.4f}\u00b0C\")\n",
    "    print(f\"  MAPE / \u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"calculate_metrics function defined successfully!\")\n",
    "print(\"calculate_metrics \u51fd\u6570\u5b9a\u4e49\u6210\u529f\uff01\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all 6 models / \u4e3a\u6240\u67096\u4e2a\u6a21\u578b\u8ba1\u7b97\u6307\u6807\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CALCULATING METRICS FOR ALL 6 MODELS\")\n",
    "print(\"\u4e3a\u6240\u67096\u4e2a\u6a21\u578b\u8ba1\u7b97\u6307\u6807\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in all_models.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name.upper()} PERFORMANCE\")\n",
    "    print(f\"{model_name.upper()} \u6027\u80fd\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    preds = all_predictions[model_name]\n",
    "    \n",
    "    # Calculate metrics / \u8ba1\u7b97\u6307\u6807\n",
    "    train_metrics = calculate_metrics(\n",
    "        preds['y_train_actual'], preds['y_train_pred'], 'Training / \u8bad\u7ec3'\n",
    "    )\n",
    "    test_metrics = calculate_metrics(\n",
    "        preds['y_test_actual'], preds['y_test_pred'], 'Test / \u6d4b\u8bd5'\n",
    "    )\n",
    "    \n",
    "    # Store metrics / \u5b58\u50a8\u6307\u6807\n",
    "    all_metrics[model_name] = {\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    }\n",
    "\n",
    "# Create comprehensive comparison table / \u521b\u5efa\u7efc\u5408\u5bf9\u6bd4\u8868\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE PERFORMANCE COMPARISON / \u7efc\u5408\u6027\u80fd\u5bf9\u6bd4\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name in all_models.keys():\n",
    "    dataset, pred_type = model_name.split('_')\n",
    "    test_metrics = all_metrics[model_name]['test']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model / \u6a21\u578b': model_name,\n",
    "        'Dataset / \u6570\u636e\u96c6': dataset,\n",
    "        'Prediction / \u9884\u6d4b': pred_type,\n",
    "        'R\u00b2': test_metrics['R2'],\n",
    "        'RMSE (\u00b0C)': test_metrics['RMSE'],\n",
    "        'MAE (\u00b0C)': test_metrics['MAE'],\n",
    "        'MAPE (%)': test_metrics['MAPE']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTest Set Performance / \u6d4b\u8bd5\u96c6\u6027\u80fd:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best models for each metric / \u627e\u51fa\u6bcf\u4e2a\u6307\u6807\u7684\u6700\u4f73\u6a21\u578b\n",
    "print(\"\\n### Best Models by Metric / \u5404\u6307\u6807\u6700\u4f73\u6a21\u578b:\")\n",
    "best_r2 = comparison_df.loc[comparison_df['R\u00b2'].idxmax()]\n",
    "best_rmse = comparison_df.loc[comparison_df['RMSE (\u00b0C)'].idxmin()]\n",
    "best_mae = comparison_df.loc[comparison_df['MAE (\u00b0C)'].idxmin()]\n",
    "\n",
    "print(f\"\\n\u2713 Highest R\u00b2: {best_r2['Model / \u6a21\u578b']} (R\u00b2 = {best_r2['R\u00b2']:.4f})\")\n",
    "print(f\"  \u6700\u9ad8 R\u00b2: {best_r2['Model / \u6a21\u578b']} (R\u00b2 = {best_r2['R\u00b2']:.4f})\")\n",
    "print(f\"\\n\u2713 Lowest RMSE: {best_rmse['Model / \u6a21\u578b']} (RMSE = {best_rmse['RMSE (\u00b0C)']:.4f}\u00b0C)\")\n",
    "print(f\"  \u6700\u4f4e RMSE: {best_rmse['Model / \u6a21\u578b']} (RMSE = {best_rmse['RMSE (\u00b0C)']:.4f}\u00b0C)\")\n",
    "print(f\"\\n\u2713 Lowest MAE: {best_mae['Model / \u6a21\u578b']} (MAE = {best_mae['MAE (\u00b0C)']:.4f}\u00b0C)\")\n",
    "print(f\"  \u6700\u4f4e MAE: {best_mae['Model / \u6a21\u578b']} (MAE = {best_mae['MAE (\u00b0C)']:.4f}\u00b0C)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 8: \u7ed3\u679c\u53ef\u89c6\u5316 / Step 8: Results Visualization\n",
    "\n",
    "### 8.1 \u5b9e\u9645\u503c vs \u9884\u6d4b\u503c / Actual vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for all 6 models / \u53ef\u89c6\u5316\u6240\u67096\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\n",
    "\n",
    "# 1. All models predictions (zoomed view) / \u6240\u6709\u6a21\u578b\u9884\u6d4b\uff08\u653e\u5927\u89c6\u56fe\uff09\n",
    "zoom_range = 300\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_names = list(all_predictions.keys())\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    preds = all_predictions[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.plot(preds['y_test_actual'][:zoom_range], label='Actual / \u5b9e\u9645', \n",
    "            alpha=0.8, linewidth=1.5)\n",
    "    ax.plot(preds['y_test_pred'][:zoom_range], label='Predicted / \u9884\u6d4b', \n",
    "            alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    r2 = all_metrics[model_name]['test']['R2']\n",
    "    mae = all_metrics[model_name]['test']['MAE']\n",
    "    \n",
    "    ax.set_title(f'{model_name}\\nR\u00b2={r2:.4f}, MAE={mae:.4f}\u00b0C', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Time Step / \u65f6\u95f4\u6b65')\n",
    "    ax.set_ylabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Predictions for All 6 Models (First {zoom_range} Points) / \u6240\u67096\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\uff08\u524d{zoom_range}\u4e2a\u70b9\uff09', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 2. Scatter plots for all models / \u6240\u6709\u6a21\u578b\u7684\u6563\u70b9\u56fe\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    preds = all_predictions[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.scatter(preds['y_test_actual'], preds['y_test_pred'], alpha=0.4, s=15)\n",
    "    \n",
    "    # Perfect prediction line / \u5b8c\u7f8e\u9884\u6d4b\u7ebf\n",
    "    min_val = min(preds['y_test_actual'].min(), preds['y_test_pred'].min())\n",
    "    max_val = max(preds['y_test_actual'].max(), preds['y_test_pred'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, \n",
    "            label='Perfect Prediction / \u5b8c\u7f8e\u9884\u6d4b')\n",
    "    \n",
    "    r2 = all_metrics[model_name]['test']['R2']\n",
    "    ax.set_title(f'{model_name} (R\u00b2={r2:.4f})', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Actual (\u00b0C) / \u5b9e\u9645')\n",
    "    ax.set_ylabel('Predicted (\u00b0C) / \u9884\u6d4b')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Predicted vs Actual for All 6 Models / \u6240\u67096\u4e2a\u6a21\u578b\u7684\u9884\u6d4bvs\u5b9e\u9645', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparison by prediction type / \u6309\u9884\u6d4b\u7c7b\u578b\u5bf9\u6bd4\u9884\u6d4b\u7ed3\u679c\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "for idx, pred_type in enumerate(['hour', 'day', 'week']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Show Dataset1 predictions for this type\n",
    "    model_name = f\"Dataset1_{pred_type}\"\n",
    "    preds = all_predictions[model_name]\n",
    "    \n",
    "    ax.plot(preds['y_test_actual'][:zoom_range], label='Actual / \u5b9e\u9645', \n",
    "            alpha=0.8, linewidth=2, color='black')\n",
    "    \n",
    "    for dataset in ['Dataset1', 'Dataset2']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        preds = all_predictions[model_name]\n",
    "        ax.plot(preds['y_test_pred'][:zoom_range], \n",
    "                label=f'{dataset} Prediction', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    ax.set_title(f'{pred_config[\"name\"]} Prediction\\n{pred_config[\"description\"]}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time Step / \u65f6\u95f4\u6b65')\n",
    "    ax.set_ylabel('Oil Temperature (\u00b0C) / \u6cb9\u6e29 (\u00b0C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Prediction Comparison by Type / \u6309\u7c7b\u578b\u5bf9\u6bd4\u9884\u6d4b', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter-header",
   "metadata": {},
   "source": [
    "### 8.2 \u6563\u70b9\u56fe\uff1a\u9884\u6d4b\u503c vs \u5b9e\u9645\u503c / Scatter Plot: Predicted vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for both models / \u4e24\u4e2a\u6a21\u578b\u7684\u6563\u70b9\u56fe\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Model 1\n",
    "axes[0].scatter(y1_test_actual, y1_test_pred_inv, alpha=0.5, s=20)\n",
    "axes[0].plot([y1_test_actual.min(), y1_test_actual.max()], \n",
    "             [y1_test_actual.min(), y1_test_actual.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction / \u5b8c\u7f8e\u9884\u6d4b')\n",
    "axes[0].set_xlabel('Actual Oil Temperature (\u00b0C) / \u5b9e\u9645\u6cb9\u6e29 (\u00b0C)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Oil Temperature (\u00b0C) / \u9884\u6d4b\u6cb9\u6e29 (\u00b0C)', fontsize=12)\n",
    "axes[0].set_title(f'Model 1 (Dataset 1): Predicted vs Actual (R\u00b2={test1_metrics[\"R2\"]:.4f}) / \u9884\u6d4b vs \u5b9e\u9645', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2\n",
    "axes[1].scatter(y2_test_actual, y2_test_pred_inv, alpha=0.5, s=20, color='orange')\n",
    "axes[1].plot([y2_test_actual.min(), y2_test_actual.max()], \n",
    "             [y2_test_actual.min(), y2_test_actual.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction / \u5b8c\u7f8e\u9884\u6d4b')\n",
    "axes[1].set_xlabel('Actual Oil Temperature (\u00b0C) / \u5b9e\u9645\u6cb9\u6e29 (\u00b0C)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Oil Temperature (\u00b0C) / \u9884\u6d4b\u6cb9\u6e29 (\u00b0C)', fontsize=12)\n",
    "axes[1].set_title(f'Model 2 (Dataset 2): Predicted vs Actual (R\u00b2={test2_metrics[\"R2\"]:.4f}) / \u9884\u6d4b vs \u5b9e\u9645', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residuals-header",
   "metadata": {},
   "source": [
    "### 8.3 \u6b8b\u5dee\u5206\u6790 / Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for both models / \u4e24\u4e2a\u6a21\u578b\u7684\u6b8b\u5dee\u5206\u6790\n",
    "residuals1 = y1_test_actual - y1_test_pred_inv\n",
    "residuals2 = y2_test_actual - y2_test_pred_inv\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "\n",
    "# Model 1 - Residual plot / \u6a21\u578b1 - \u6b8b\u5dee\u56fe\n",
    "axes[0, 0].scatter(y1_test_pred_inv, residuals1, alpha=0.5, s=20)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Oil Temperature (\u00b0C) / \u9884\u6d4b\u6cb9\u6e29 (\u00b0C)')\n",
    "axes[0, 0].set_ylabel('Residuals (\u00b0C) / \u6b8b\u5dee (\u00b0C)')\n",
    "axes[0, 0].set_title('Model 1 (Dataset 1): Residual Plot / \u6b8b\u5dee\u56fe')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 1 - Residual distribution / \u6a21\u578b1 - \u6b8b\u5dee\u5206\u5e03\n",
    "axes[0, 1].hist(residuals1, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Residuals (\u00b0C) / \u6b8b\u5dee (\u00b0C)')\n",
    "axes[0, 1].set_ylabel('Frequency / \u9891\u7387')\n",
    "axes[0, 1].set_title('Model 1 (Dataset 1): Distribution of Residuals / \u6b8b\u5dee\u5206\u5e03')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2 - Residual plot / \u6a21\u578b2 - \u6b8b\u5dee\u56fe\n",
    "axes[1, 0].scatter(y2_test_pred_inv, residuals2, alpha=0.5, s=20, color='orange')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Oil Temperature (\u00b0C) / \u9884\u6d4b\u6cb9\u6e29 (\u00b0C)')\n",
    "axes[1, 0].set_ylabel('Residuals (\u00b0C) / \u6b8b\u5dee (\u00b0C)')\n",
    "axes[1, 0].set_title('Model 2 (Dataset 2): Residual Plot / \u6b8b\u5dee\u56fe')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2 - Residual distribution / \u6a21\u578b2 - \u6b8b\u5dee\u5206\u5e03\n",
    "axes[1, 1].hist(residuals2, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Residuals (\u00b0C) / \u6b8b\u5dee (\u00b0C)')\n",
    "axes[1, 1].set_ylabel('Frequency / \u9891\u7387')\n",
    "axes[1, 1].set_title('Model 2 (Dataset 2): Distribution of Residuals / \u6b8b\u5dee\u5206\u5e03')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics / \u6b8b\u5dee\u7edf\u8ba1\n",
    "print(\"=\"*60)\n",
    "print(\"Residual Statistics / \u6b8b\u5dee\u7edf\u8ba1\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel 1 (Dataset 1):\")\n",
    "print(f\"  Mean / \u5747\u503c: {residuals1.mean():.4f}\u00b0C\")\n",
    "print(f\"  Std / \u6807\u51c6\u5dee: {residuals1.std():.4f}\u00b0C\")\n",
    "print(f\"  Min / \u6700\u5c0f\u503c: {residuals1.min():.4f}\u00b0C\")\n",
    "print(f\"  Max / \u6700\u5927\u503c: {residuals1.max():.4f}\u00b0C\")\n",
    "\n",
    "print(f\"\\nModel 2 (Dataset 2):\")\n",
    "print(f\"  Mean / \u5747\u503c: {residuals2.mean():.4f}\u00b0C\")\n",
    "print(f\"  Std / \u6807\u51c6\u5dee: {residuals2.std():.4f}\u00b0C\")\n",
    "print(f\"  Min / \u6700\u5c0f\u503c: {residuals2.min():.4f}\u00b0C\")\n",
    "print(f\"  Max / \u6700\u5927\u503c: {residuals2.max():.4f}\u00b0C\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## \u6b65\u9aa4 9: \u7ed3\u8bba\u548c\u603b\u7ed3 / Step 9: Conclusion and Summary\n",
    "\n",
    "### \u6a21\u578b\u6027\u80fd\u603b\u7ed3 / Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary and analysis of all 6 models / \u6240\u67096\u4e2a\u6a21\u578b\u7684\u7efc\u5408\u603b\u7ed3\u548c\u5206\u6790\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE SUMMARY: 6 RNN MODELS FOR OIL TEMPERATURE PREDICTION\")\n",
    "print(\"\u7efc\u5408\u603b\u7ed3\uff1a6\u4e2a\u7528\u4e8e\u6cb9\u6e29\u9884\u6d4b\u7684RNN\u6a21\u578b\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Overview table / \u6982\u89c8\u8868\n",
    "print(\"\\n### 1. Model Overview / \u6a21\u578b\u6982\u89c8:\")\n",
    "overview_data = []\n",
    "for model_name in all_models.keys():\n",
    "    dataset, pred_type = model_name.split('_')\n",
    "    test_metrics = all_metrics[model_name]['test']\n",
    "    data_info = all_data_info[model_name]\n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    \n",
    "    overview_data.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset,\n",
    "        'Prediction': pred_config['name'],\n",
    "        'Offset': pred_config['offset'],\n",
    "        'Seq Length': pred_config['seq_length'],\n",
    "        'Train Samples': f\"{data_info['n_train']:,}\",\n",
    "        'Test Samples': f\"{data_info['n_test']:,}\",\n",
    "        'R\u00b2': f\"{test_metrics['R2']:.4f}\",\n",
    "        'MAE (\u00b0C)': f\"{test_metrics['MAE']:.4f}\",\n",
    "        'RMSE (\u00b0C)': f\"{test_metrics['RMSE']:.4f}\"\n",
    "    })\n",
    "\n",
    "overview_df = pd.DataFrame(overview_data)\n",
    "print(overview_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 2. Best performing models / \u6700\u4f73\u6a21\u578b\n",
    "print(\"\\n### 2. Best Performing Models / \u6700\u4f73\u6027\u80fd\u6a21\u578b:\")\n",
    "comparison_df_numeric = pd.DataFrame({\n",
    "    'Model': [d['Model'] for d in overview_data],\n",
    "    'R\u00b2': [all_metrics[d['Model']]['test']['R2'] for d in overview_data],\n",
    "    'MAE': [all_metrics[d['Model']]['test']['MAE'] for d in overview_data],\n",
    "    'RMSE': [all_metrics[d['Model']]['test']['RMSE'] for d in overview_data],\n",
    "    'MAPE': [all_metrics[d['Model']]['test']['MAPE'] for d in overview_data]\n",
    "})\n",
    "\n",
    "best_overall = comparison_df_numeric.loc[comparison_df_numeric['R\u00b2'].idxmax()]\n",
    "print(f\"\\n\u2713 Overall Best Model (Highest R\u00b2) / \u603b\u4f53\u6700\u4f73\u6a21\u578b\uff08\u6700\u9ad8R\u00b2\uff09:\")\n",
    "print(f\"  {best_overall['Model']}\")\n",
    "print(f\"  R\u00b2 = {best_overall['R\u00b2']:.4f}\")\n",
    "print(f\"  MAE = {best_overall['MAE']:.4f}\u00b0C\")\n",
    "print(f\"  RMSE = {best_overall['RMSE']:.4f}\u00b0C\")\n",
    "print(f\"  MAPE = {best_overall['MAPE']:.2f}%\")\n",
    "\n",
    "# Best for each prediction type\n",
    "print(f\"\\n\u2713 Best Models by Prediction Type / \u5404\u9884\u6d4b\u7c7b\u578b\u7684\u6700\u4f73\u6a21\u578b:\")\n",
    "for pred_type in ['hour', 'day', 'week']:\n",
    "    models_of_type = [m for m in all_models.keys() if pred_type in m]\n",
    "    type_df = comparison_df_numeric[comparison_df_numeric['Model'].isin(models_of_type)]\n",
    "    best_of_type = type_df.loc[type_df['R\u00b2'].idxmax()]\n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    \n",
    "    print(f\"\\n  {pred_config['name']} ({pred_config['description']}):\")\n",
    "    print(f\"    Model: {best_of_type['Model']}\")\n",
    "    print(f\"    R\u00b2 = {best_of_type['R\u00b2']:.4f}, MAE = {best_of_type['MAE']:.4f}\u00b0C\")\n",
    "\n",
    "# 3. Performance comparison across datasets / \u8de8\u6570\u636e\u96c6\u6027\u80fd\u5bf9\u6bd4\n",
    "print(\"\\n### 3. Performance Comparison Across Datasets / \u8de8\u6570\u636e\u96c6\u6027\u80fd\u5bf9\u6bd4:\")\n",
    "for pred_type in ['hour', 'day', 'week']:\n",
    "    d1_model = f\"Dataset1_{pred_type}\"\n",
    "    d2_model = f\"Dataset2_{pred_type}\"\n",
    "    \n",
    "    d1_r2 = all_metrics[d1_model]['test']['R2']\n",
    "    d2_r2 = all_metrics[d2_model]['test']['R2']\n",
    "    d1_mae = all_metrics[d1_model]['test']['MAE']\n",
    "    d2_mae = all_metrics[d2_model]['test']['MAE']\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    print(f\"\\n{pred_config['name']} Prediction:\")\n",
    "    print(f\"  Dataset1: R\u00b2={d1_r2:.4f}, MAE={d1_mae:.4f}\u00b0C\")\n",
    "    print(f\"  Dataset2: R\u00b2={d2_r2:.4f}, MAE={d2_mae:.4f}\u00b0C\")\n",
    "    \n",
    "    if d1_r2 > d2_r2:\n",
    "        print(f\"  \u2192 Dataset1 performs better / Dataset1\u8868\u73b0\u66f4\u597d\")\n",
    "    else:\n",
    "        print(f\"  \u2192 Dataset2 performs better / Dataset2\u8868\u73b0\u66f4\u597d\")\n",
    "\n",
    "# 4. Performance comparison across prediction types / \u8de8\u9884\u6d4b\u7c7b\u578b\u6027\u80fd\u5bf9\u6bd4\n",
    "print(\"\\n### 4. Performance Comparison Across Prediction Types / \u8de8\u9884\u6d4b\u7c7b\u578b\u6027\u80fd\u5bf9\u6bd4:\")\n",
    "for dataset in ['Dataset1', 'Dataset2']:\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    for pred_type in ['hour', 'day', 'week']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        r2 = all_metrics[model_name]['test']['R2']\n",
    "        mae = all_metrics[model_name]['test']['MAE']\n",
    "        pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "        print(f\"  {pred_config['name']:6s}: R\u00b2={r2:.4f}, MAE={mae:.4f}\u00b0C\")\n",
    "\n",
    "# 5. Data split strategy verification / \u6570\u636e\u62c6\u5206\u7b56\u7565\u9a8c\u8bc1\n",
    "print(\"\\n### 5. Data Split Strategy Verification / \u6570\u636e\u62c6\u5206\u7b56\u7565\u9a8c\u8bc1:\")\n",
    "print(\"Using group-based splitting (dataset.md recommendation):\")\n",
    "print(\"\u4f7f\u7528\u57fa\u4e8e\u5206\u7ec4\u7684\u62c6\u5206\uff08dataset.md \u63a8\u8350\uff09:\")\n",
    "sample_model = list(all_data_info.keys())[0]\n",
    "sample_info = all_data_info[sample_model]\n",
    "print(f\"\\n  \u2713 Total groups / \u603b\u7ec4\u6570: {n_groups}\")\n",
    "print(f\"  \u2713 Training groups / \u8bad\u7ec3\u7ec4: {len(sample_info['train_groups'])} ({train_ratio*100:.0f}%)\")\n",
    "print(f\"  \u2713 Test groups / \u6d4b\u8bd5\u7ec4: {len(sample_info['test_groups'])} ({(1-train_ratio)*100:.0f}%)\")\n",
    "print(f\"  \u2713 Training groups / \u8bad\u7ec3\u7ec4: {sample_info['train_groups']}\")\n",
    "print(f\"  \u2713 Test groups / \u6d4b\u8bd5\u7ec4: {sample_info['test_groups']}\")\n",
    "print(\"\\n  Advantages / \u4f18\u52bf:\")\n",
    "print(\"  \u2022 Training and test data are completely disjoint / \u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5b8c\u5168\u4e0d\u91cd\u53e0\")\n",
    "print(\"  \u2022 Maintains temporal continuity within groups / \u4fdd\u6301\u7ec4\u5185\u65f6\u95f4\u8fde\u7eed\u6027\")\n",
    "print(\"  \u2022 Random selection reduces bias / \u968f\u673a\u9009\u62e9\u51cf\u5c11\u504f\u5dee\")\n",
    "\n",
    "# 6. Key insights / \u5173\u952e\u89c1\u89e3\n",
    "print(\"\\n### 6. Key Insights / \u5173\u952e\u89c1\u89e3:\")\n",
    "print(\"\\n1. Prediction Horizon Impact / \u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u7684\u5f71\u54cd:\")\n",
    "print(\"   - Hour predictions typically have higher R\u00b2 / \u5c0f\u65f6\u9884\u6d4b\u901a\u5e38\u5177\u6709\u66f4\u9ad8\u7684R\u00b2\")\n",
    "print(\"   - Longer horizons (week) are more challenging / \u66f4\u957f\u7684\u65f6\u95f4\u8303\u56f4\uff08\u5468\uff09\u66f4\u5177\u6311\u6218\u6027\")\n",
    "\n",
    "print(\"\\n2. Dataset Characteristics / \u6570\u636e\u96c6\u7279\u5f81:\")\n",
    "print(f\"   - Dataset1 temperature range: {df_train1['OT'].min():.2f}\u00b0C - {df_train1['OT'].max():.2f}\u00b0C\")\n",
    "print(f\"   - Dataset2 temperature range: {df_train2['OT'].min():.2f}\u00b0C - {df_train2['OT'].max():.2f}\u00b0C\")\n",
    "print(\"   - Dataset2 has significantly higher temperatures / Dataset2\u6e29\u5ea6\u663e\u8457\u66f4\u9ad8\")\n",
    "\n",
    "print(\"\\n3. Model Architecture / \u6a21\u578b\u67b6\u6784:\")\n",
    "print(f\"   - Input features: {input_size} (HUFL, HULL, MUFL, MULL, LUFL, LULL)\")\n",
    "print(f\"   - RNN hidden units: {hidden_sizes}\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Same architecture for all 6 models / \u6240\u67096\u4e2a\u6a21\u578b\u4f7f\u7528\u76f8\u540c\u67b6\u6784\")\n",
    "\n",
    "# 7. Next steps / \u4e0b\u4e00\u6b65\n",
    "print(\"\\n### 7. Next Steps / \u4e0b\u4e00\u6b65:\")\n",
    "print(\"\\n1. Model Improvements / \u6a21\u578b\u6539\u8fdb:\")\n",
    "print(\"   \u2022 Try LSTM/GRU for better long-term dependencies / \u5c1d\u8bd5LSTM/GRU\u4ee5\u66f4\u597d\u5730\u6355\u83b7\u957f\u671f\u4f9d\u8d56\")\n",
    "print(\"   \u2022 Experiment with different sequence lengths / \u5b9e\u9a8c\u4e0d\u540c\u7684\u5e8f\u5217\u957f\u5ea6\")\n",
    "print(\"   \u2022 Add attention mechanisms / \u6dfb\u52a0\u6ce8\u610f\u529b\u673a\u5236\")\n",
    "\n",
    "print(\"\\n2. Feature Engineering / \u7279\u5f81\u5de5\u7a0b:\")\n",
    "print(\"   \u2022 Explore feature importance and selection / \u63a2\u7d22\u7279\u5f81\u91cd\u8981\u6027\u548c\u9009\u62e9\")\n",
    "print(\"   \u2022 Add time-based features (hour, day, season) / \u6dfb\u52a0\u57fa\u4e8e\u65f6\u95f4\u7684\u7279\u5f81\")\n",
    "print(\"   \u2022 Consider feature interactions / \u8003\u8651\u7279\u5f81\u4ea4\u4e92\")\n",
    "\n",
    "print(\"\\n3. Model Comparison / \u6a21\u578b\u6bd4\u8f83:\")\n",
    "print(\"   \u2022 Compare with traditional ML (Linear Regression, Random Forest, MLP)\")\n",
    "print(\"   \u4e0e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6bd4\u8f83\uff08\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\u3001MLP\uff09\")\n",
    "print(\"   \u2022 Compare with SOTA model (Informer)\")\n",
    "print(\"   \u4e0eSOTA\u6a21\u578b\u6bd4\u8f83\uff08Informer\uff09\")\n",
    "\n",
    "print(\"\\n4. Hyperparameter Tuning / \u8d85\u53c2\u6570\u8c03\u4f18:\")\n",
    "print(\"   \u2022 Grid search for optimal seq_length / \u7f51\u683c\u641c\u7d22\u6700\u4f18\u5e8f\u5217\u957f\u5ea6\")\n",
    "print(\"   \u2022 Learning rate scheduling / \u5b66\u4e60\u7387\u8c03\u5ea6\")\n",
    "print(\"   \u2022 Batch size optimization / \u6279\u6b21\u5927\u5c0f\u4f18\u5316\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE! / \u5206\u6790\u5b8c\u6210\uff01\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save comparison results to CSV / \u5c06\u5bf9\u6bd4\u7ed3\u679c\u4fdd\u5b58\u4e3aCSV\n",
    "# overview_df.to_csv('rnn_models_comparison.csv', index=False)\n",
    "# print(\"\\nResults saved to 'rnn_models_comparison.csv'\")\n",
    "# print(\"\u7ed3\u679c\u5df2\u4fdd\u5b58\u5230 'rnn_models_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "### \u53ef\u9009\uff1a\u4fdd\u5b58\u6a21\u578b / Optional: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model / \u53d6\u6d88\u6ce8\u91ca\u4ee5\u4fdd\u5b58\u6a21\u578b\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "torch.save(model.state_dict(), '../models/rnn_model.pth')\n",
    "print(\"Model saved to ../models/rnn_model.pth\")\n",
    "print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\u5230 ../models/rnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ba67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}