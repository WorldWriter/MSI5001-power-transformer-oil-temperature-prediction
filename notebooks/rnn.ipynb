{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rnn-intro",
   "metadata": {},
   "source": [
    "# 使用 RNN 预测电力变压器油温 / Power Transformer Oil Temperature Prediction using RNN\n",
    "\n",
    "## 简介 / Introduction\n",
    "\n",
    "本 notebook 实现了一个循环神经网络（RNN）模型来预测电力变压器的油温（OT）。\n",
    "\n",
    "This notebook implements a Recurrent Neural Network (RNN) model for predicting the oil temperature (OT) of power transformers.\n",
    "\n",
    "**数据集 / Dataset**: ETDataset (电力变压器温度数据集 / Electricity Transformer Temperature)\n",
    "- 来源 / Source: https://github.com/zhouhaoyi/ETDataset\n",
    "- 出处 / From: AAAI 2021 最佳论文（Informer 模型）/ AAAI 2021 Best Paper (Informer model)\n",
    "\n",
    "**特征 / Features**:\n",
    "- HUFL: 高压有功负载 / High UseFul Load\n",
    "- HULL: 高压无功负载 / High UseLess Load  \n",
    "- MUFL: 中压有功负载 / Medium UseFul Load\n",
    "- MULL: 中压无功负载 / Medium UseLess Load\n",
    "- LUFL: 低压有功负载 / Low UseFul Load\n",
    "- LULL: 低压无功负载 / Low UseLess Load\n",
    "\n",
    "**目标变量 / Target**: OT (油温 / Oil Temperature)\n",
    "\n",
    "**目标 / Goal**: 基于历史负载数据构建 RNN 模型预测油温，并展示：\n",
    "- 时间序列数据预处理 / Time series data preprocessing\n",
    "- 用于序列预测的 RNN 架构 / RNN architecture for sequence prediction\n",
    "- 模型训练与评估 / Model training and evaluation\n",
    "- 性能指标与可视化 / Performance metrics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs",
   "metadata": {},
   "source": [
    "## 步骤 1: 导入所需库 / Step 1: Import Required Libraries\n",
    "\n",
    "我们将使用 / We'll use:\n",
    "- pandas/numpy: 数据处理 / data manipulation\n",
    "- sklearn: 预处理和评估指标 / preprocessing and metrics\n",
    "- **PyTorch**: 构建 RNN 模型 / building the RNN model\n",
    "- matplotlib: 可视化 / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理 / Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 预处理 / Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 深度学习 / Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 可视化 / Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 解决中文显示问题 / Fix Chinese font display issue\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题 / Fix minus sign display\n",
    "\n",
    "# 工具 / Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子以保证可复现性 / Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# 设备配置 / Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch 版本 / PyTorch version: {torch.__version__}\")\n",
    "print(f\"设备 / Device: {device}\")\n",
    "print(f\"CUDA 可用 / CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## 步骤 2: 加载和探索数据 / Step 2: Load and Explore Data\n",
    "\n",
    "我们将加载变压器温度数据集并进行初步探索以了解：\n",
    "\n",
    "We'll load the transformer temperature dataset and perform initial exploration to understand:\n",
    "- 数据形状和结构 / Data shape and structure\n",
    "- 缺失值 / Missing values\n",
    "- 统计属性 / Statistical properties\n",
    "- 时间序列特征 / Time series characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load the ETT dataset\n",
    "    加载 ETT 数据集\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame\n",
    "        Loaded dataframe with date as index\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.set_index('date')\n",
    "    return df\n",
    "\n",
    "# Load both datasets / 加载两个数据集\n",
    "train1_filepath = '../dataset/train1.csv'\n",
    "train2_filepath = '../dataset/train2.csv'\n",
    "\n",
    "df_train1 = load_data(train1_filepath)\n",
    "df_train2 = load_data(train2_filepath)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset loaded from original files:\")\n",
    "print(\"从原始文件加载数据集：\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset 1: {train1_filepath}\")\n",
    "print(f\"  Shape: {df_train1.shape}\")\n",
    "print(f\"  Date range: {df_train1.index[0]} to {df_train1.index[-1]}\")\n",
    "\n",
    "print(f\"\\nDataset 2: {train2_filepath}\")\n",
    "print(f\"  Shape: {df_train2.shape}\")\n",
    "print(f\"  Date range: {df_train2.index[0]} to {df_train2.index[-1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 1 (train1.csv) - First few rows:\")\n",
    "print(\"数据集 1 - 前几行：\")\n",
    "print(\"=\"*60)\n",
    "display(df_train1.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 2 (train2.csv) - First few rows:\")\n",
    "print(\"数据集 2 - 前几行：\")\n",
    "print(\"=\"*60)\n",
    "display(df_train2.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 1 - Basic Statistics:\")\n",
    "print(\"数据集 1 - 基本统计信息：\")\n",
    "print(\"=\"*60)\n",
    "display(df_train1.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Dataset 2 - Basic Statistics:\")\n",
    "print(\"数据集 2 - 基本统计信息：\")\n",
    "print(\"=\"*60)\n",
    "display(df_train2.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Comparison: Value Ranges / 对比：数值范围\")\n",
    "print(\"=\"*60)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': df_train1.columns,\n",
    "    'Train1 Mean': df_train1.mean().values,\n",
    "    'Train2 Mean': df_train2.mean().values,\n",
    "    'Train1 Std': df_train1.std().values,\n",
    "    'Train2 Std': df_train2.std().values\n",
    "})\n",
    "display(comparison_df)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## 步骤 3: 数据可视化 / Step 3: Data Visualization\n",
    "\n",
    "可视化时间序列以理解模式和关系\n",
    "\n",
    "Visualize the time series to understand patterns and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Oil Temperature between two datasets\n",
    "# 比较两个数据集的油温\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Dataset 1 / 数据集 1\n",
    "axes[0].plot(df_train1.index[:2000], df_train1['OT'][:2000], linewidth=0.8, label='Dataset 1')\n",
    "axes[0].set_title('Dataset 1: Oil Temperature Over Time (First 2000 points) / 数据集1：油温随时间变化（前2000个点）', fontsize=14)\n",
    "axes[0].set_xlabel('Time / 时间')\n",
    "axes[0].set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Dataset 2 / 数据集 2\n",
    "axes[1].plot(df_train2.index[:2000], df_train2['OT'][:2000], linewidth=0.8, label='Dataset 2', color='orange')\n",
    "axes[1].set_title('Dataset 2: Oil Temperature Over Time (First 2000 points) / 数据集2：油温随时间变化（前2000个点）', fontsize=14)\n",
    "axes[1].set_xlabel('Time / 时间')\n",
    "axes[1].set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmaps for both datasets\n",
    "# 两个数据集的相关性热力图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Dataset 1\n",
    "sns.heatmap(df_train1.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Dataset 1: Feature Correlation / 数据集1：特征相关性', fontsize=14)\n",
    "\n",
    "# Dataset 2\n",
    "sns.heatmap(df_train2.corr(), annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f', ax=axes[1])\n",
    "axes[1].set_title('Dataset 2: Feature Correlation / 数据集2：特征相关性', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution comparison\n",
    "# 分布对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df_train1['OT'], bins=50, edgecolor='black', alpha=0.7, label='Dataset 1')\n",
    "axes[0].set_title('Dataset 1: OT Distribution / 数据集1：油温分布', fontsize=14)\n",
    "axes[0].set_xlabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[0].set_ylabel('Frequency / 频率')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].hist(df_train2['OT'], bins=50, edgecolor='black', alpha=0.7, color='orange', label='Dataset 2')\n",
    "axes[1].set_title('Dataset 2: OT Distribution / 数据集2：油温分布', fontsize=14)\n",
    "axes[1].set_xlabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[1].set_ylabel('Frequency / 频率')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation / 观察:\")\n",
    "print(f\"Dataset 1 OT range / 数据集1油温范围: {df_train1['OT'].min():.2f}°C - {df_train1['OT'].max():.2f}°C\")\n",
    "print(f\"Dataset 2 OT range / 数据集2油温范围: {df_train2['OT'].min():.2f}°C - {df_train2['OT'].max():.2f}°C\")\n",
    "print(f\"\\nDataset 2 has higher temperature values / 数据集2具有更高的温度值\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## 步骤 4: 数据预处理 / Step 4: Data Preprocessing\n",
    "\n",
    "### 4.1 特征和目标分离 / Feature and Target Separation\n",
    "\n",
    "我们将特征（负载数据）与目标（OT - 油温）分离\n",
    "\n",
    "We'll separate features (load data) from target (OT - Oil Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features_target(df):\n",
    "    \"\"\"\n",
    "    Separate features and target variable\n",
    "    分离特征和目标变量\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : np.ndarray\n",
    "        Features (all columns except OT)\n",
    "    y : np.ndarray\n",
    "        Target (OT column)\n",
    "    \"\"\"\n",
    "    # Features: all columns except OT\n",
    "    # 特征：除 OT 外的所有列\n",
    "    X = df.drop('OT', axis=1).values\n",
    "    # Target: OT (Oil Temperature)\n",
    "    # 目标：OT（油温）\n",
    "    y = df['OT'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process both datasets / 处理两个数据集\n",
    "print(\"=\"*60)\n",
    "print(\"Processing Dataset 1 / 处理数据集1\")\n",
    "print(\"=\"*60)\n",
    "X1, y1 = prepare_features_target(df_train1)\n",
    "print(f\"Features shape / 特征形状: {X1.shape}\")\n",
    "print(f\"Target shape / 目标形状: {y1.shape}\")\n",
    "print(f\"Feature names / 特征名称: {df_train1.drop('OT', axis=1).columns.tolist()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing Dataset 2 / 处理数据集2\")\n",
    "print(\"=\"*60)\n",
    "X2, y2 = prepare_features_target(df_train2)\n",
    "print(f\"Features shape / 特征形状: {X2.shape}\")\n",
    "print(f\"Target shape / 目标形状: {y2.shape}\")\n",
    "print(f\"Feature names / 特征名称: {df_train2.drop('OT', axis=1).columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normalize-header",
   "metadata": {},
   "source": [
    "### 4.2 数据标准化 / Data Normalization\n",
    "\n",
    "标准化特征和目标以提高 RNN 训练稳定性和收敛性\n",
    "\n",
    "Normalize features and target to improve RNN training stability and convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, y):\n",
    "    \"\"\"\n",
    "    Normalize features and target using StandardScaler\n",
    "    使用 StandardScaler 标准化特征和目标\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features\n",
    "    y : np.ndarray\n",
    "        Target\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_scaled : np.ndarray\n",
    "        Normalized features\n",
    "    y_scaled : np.ndarray\n",
    "        Normalized target\n",
    "    scaler_X : StandardScaler\n",
    "        Fitted scaler for features\n",
    "    scaler_y : StandardScaler\n",
    "        Fitted scaler for target\n",
    "    \"\"\"\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    \n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    return X_scaled, y_scaled, scaler_X, scaler_y\n",
    "\n",
    "# Normalize Dataset 1 / 归一化数据集1\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing Dataset 1 / 归一化数据集1\")\n",
    "print(\"=\"*60)\n",
    "X1_scaled, y1_scaled, scaler_X1, scaler_y1 = normalize_data(X1, y1)\n",
    "print(\"Data normalized successfully! / 数据标准化成功！\")\n",
    "print(f\"Features - Mean: {X1_scaled.mean():.4f}, Std: {X1_scaled.std():.4f}\")\n",
    "print(f\"Target - Mean: {y1_scaled.mean():.4f}, Std: {y1_scaled.std():.4f}\")\n",
    "\n",
    "# Normalize Dataset 2 / 归一化数据集2\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Normalizing Dataset 2 / 归一化数据集2\")\n",
    "print(\"=\"*60)\n",
    "X2_scaled, y2_scaled, scaler_X2, scaler_y2 = normalize_data(X2, y2)\n",
    "print(\"Data normalized successfully! / 数据标准化成功！\")\n",
    "print(f\"Features - Mean: {X2_scaled.mean():.4f}, Std: {X2_scaled.std():.4f}\")\n",
    "print(f\"Target - Mean: {y2_scaled.mean():.4f}, Std: {y2_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequence-header",
   "metadata": {},
   "source": [
    "### 4.3 创建时间序列序列 / Create Time Series Sequences\n",
    "\n",
    "将数据转换为 RNN 输入序列。我们使用滑动窗口方法：\n",
    "\n",
    "Transform data into sequences for RNN input. We use a sliding window approach:\n",
    "- 输入：过去 `seq_length` 个时间步 / Input: Past `seq_length` time steps\n",
    "- 输出：下一个时间步的油温 / Output: Next time step's oil temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q6k6l347mmg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction configurations / 定义预测配置\n",
    "# Based on dataset.md requirements / 基于 dataset.md 的要求\n",
    "\n",
    "PREDICTION_CONFIGS = {\n",
    "    'hour': {\n",
    "        'name': 'Hour / 小时',\n",
    "        'offset': 4,      # Start from 4 time points before target / 从目标前4个时间点开始\n",
    "        'seq_length': 24, # Use 24 time points (1.5 hours, tunable) / 使用24个时间点（1.5小时，可调）\n",
    "        'description': 'Predict 1 hour ahead / 预测1小时后'\n",
    "    },\n",
    "    'day': {\n",
    "        'name': 'Day / 天',\n",
    "        'offset': 96,     # Start from 96 time points before target (1 day) / 从目标前96个时间点开始（1天）\n",
    "        'seq_length': 96, # Use 96 time points (1 day, tunable) / 使用96个时间点（1天，可调）\n",
    "        'description': 'Predict 1 day ahead / 预测1天后'\n",
    "    },\n",
    "    'week': {\n",
    "        'name': 'Week / 周',\n",
    "        'offset': 672,    # Start from 672 time points before target (1 week) / 从目标前672个时间点开始（1周）\n",
    "        'seq_length': 168, # Use 168 time points (1 week, tunable) / 使用168个时间点（1周，可调）\n",
    "        'description': 'Predict 1 week ahead / 预测1周后'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Prediction Configurations / 预测配置\")\n",
    "print(\"=\"*60)\n",
    "for key, config in PREDICTION_CONFIGS.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  {config['description']}\")\n",
    "    print(f\"  Offset / 偏移: {config['offset']} time points\")\n",
    "    print(f\"  Sequence Length / 序列长度: {config['seq_length']} time points\")\n",
    "    print(f\"  Total lookback / 总回溯: {config['offset'] + config['seq_length']} time points\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote / 注意:\")\n",
    "print(\"- Sequence length (N) is tunable for optimization\")\n",
    "print(\"  序列长度 N 可调整以优化性能\")\n",
    "print(\"- We will train 6 models total: 2 datasets × 3 prediction horizons\")\n",
    "print(\"  我们将训练6个模型：2个数据集 × 3种预测时间范围\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_with_offset(X, y, seq_length=24, offset=4):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction with offset\n",
    "    创建带偏移的时间序列预测序列\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features array\n",
    "    y : np.ndarray\n",
    "        Target array\n",
    "    seq_length : int\n",
    "        Number of time steps to look back (default: 24)\n",
    "        回溯的时间步数（默认：24）\n",
    "    offset : int\n",
    "        Prediction time offset from current point\n",
    "        从当前点预测的时间偏移\n",
    "        - offset=4: predict 1 hour ahead (4 × 15min = 1 hour)\n",
    "        - offset=96: predict 1 day ahead (96 × 15min = 24 hours)\n",
    "        - offset=672: predict 1 week ahead (672 × 15min = 7 days)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_seq : np.ndarray\n",
    "        Sequences of features (samples, seq_length, n_features)\n",
    "        From time t-offset-seq_length to t-offset-1\n",
    "    y_seq : np.ndarray\n",
    "        Target values at time t (future target)\n",
    "        \n",
    "    Example / 示例:\n",
    "    ---------------\n",
    "    For predicting 1 hour ahead (offset=4, seq_length=24):\n",
    "    - To predict at time t=100:\n",
    "      - Input: X[72:96] (from t-28 to t-5, excluding t-4 to t-1)\n",
    "      - Target: y[100] (the value at t)\n",
    "    \"\"\"\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    # Total lookback required / 需要的总回溯长度\n",
    "    total_lookback = offset + seq_length\n",
    "    \n",
    "    # Start from where we have enough history / 从有足够历史数据的地方开始\n",
    "    for i in range(total_lookback, len(X)):\n",
    "        # Input sequence: from t-offset-seq_length to t-offset-1\n",
    "        # 输入序列：从 t-offset-seq_length 到 t-offset-1\n",
    "        seq_start = i - offset - seq_length\n",
    "        seq_end = i - offset\n",
    "        X_seq.append(X[seq_start:seq_end])\n",
    "        \n",
    "        # Target: value at time t (future point)\n",
    "        # 目标：时间点 t 的值（未来点）\n",
    "        y_seq.append(y[i])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Demonstrate with default configuration (hour prediction)\n",
    "# 使用默认配置演示（小时预测）\n",
    "seq_length_demo = 24\n",
    "offset_demo = 4\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Sequence Creation Demo / 序列创建演示\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Demo configuration: Hour prediction / 演示配置：小时预测\")\n",
    "print(f\"  Sequence length: {seq_length_demo}\")\n",
    "print(f\"  Offset: {offset_demo}\")\n",
    "print(f\"  Total lookback: {seq_length_demo + offset_demo} time points\")\n",
    "\n",
    "print(f\"\\nExample / 示例:\")\n",
    "print(f\"  To predict at time t=100:\")\n",
    "print(f\"    - Input: time points {100-offset_demo-seq_length_demo} to {100-offset_demo-1}\")\n",
    "print(f\"             (t-{offset_demo+seq_length_demo} to t-{offset_demo+1})\")\n",
    "print(f\"    - Target: time point {100} (t)\")\n",
    "print(f\"\\n  为了预测时间点 t=100:\")\n",
    "print(f\"    - 输入：时间点 {100-offset_demo-seq_length_demo} 到 {100-offset_demo-1}\")\n",
    "print(f\"    - 目标：时间点 {100}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: We will create sequences for each prediction configuration later\")\n",
    "print(\"注意：我们稍后会为每种预测配置创建序列\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-header",
   "metadata": {},
   "source": [
    "### 4.4 训练集/测试集分割 / Train/Test Split\n",
    "\n",
    "将数据分割为训练集和测试集。对于时间序列，我们使用时间分割（非随机）。\n",
    "\n",
    "Split data into training and testing sets. For time series, we use temporal split (not random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement group-based train/test split as recommended in dataset.md\n",
    "# 按照 dataset.md 推荐的方式实现基于分组的训练/测试拆分\n",
    "\n",
    "def split_into_groups(data_length, n_groups=20):\n",
    "    \"\"\"\n",
    "    Divide data into n_groups of continuous time point groups\n",
    "    将数据分成 n_groups 个连续的时间点组\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_length : int\n",
    "        Total length of data\n",
    "    n_groups : int\n",
    "        Number of groups to split into (default: 20)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    groups : list of tuples\n",
    "        List of (start_idx, end_idx) for each group\n",
    "    \"\"\"\n",
    "    group_size = data_length // n_groups\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(n_groups):\n",
    "        start_idx = i * group_size\n",
    "        # Last group takes all remaining data\n",
    "        end_idx = start_idx + group_size if i < n_groups - 1 else data_length\n",
    "        groups.append((start_idx, end_idx))\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def split_train_test_by_groups(X_seq, y_seq, train_ratio=0.8, n_groups=20, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split data by groups as recommended in dataset.md:\n",
    "    按照 dataset.md 推荐的方式按组拆分数据：\n",
    "    \n",
    "    1. Divide data into n_groups continuous blocks\n",
    "       将数据分成 n_groups 个连续块\n",
    "    2. Randomly select train_ratio of groups for training\n",
    "       随机选择 train_ratio 的组作为训练集\n",
    "    3. Remaining groups for testing\n",
    "       剩余的组作为测试集\n",
    "    \n",
    "    This ensures training and test data are completely disjoint\n",
    "    这确保训练和测试数据完全不重叠\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_seq, y_seq : np.ndarray\n",
    "        Sequence data\n",
    "    train_ratio : float\n",
    "        Proportion of groups for training (default: 0.8 = 16 out of 20 groups)\n",
    "    n_groups : int\n",
    "        Number of groups to split into (default: 20)\n",
    "    random_seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Get group indices\n",
    "    groups = split_into_groups(len(X_seq), n_groups)\n",
    "    n_train_groups = int(n_groups * train_ratio)\n",
    "    \n",
    "    # Randomly select training groups\n",
    "    # 随机选择训练组\n",
    "    group_indices = np.arange(n_groups)\n",
    "    np.random.shuffle(group_indices)\n",
    "    train_group_indices = sorted(group_indices[:n_train_groups])\n",
    "    test_group_indices = sorted(group_indices[n_train_groups:])\n",
    "    \n",
    "    # Extract data from selected groups\n",
    "    # 从选定的组中提取数据\n",
    "    train_data_X = []\n",
    "    train_data_y = []\n",
    "    test_data_X = []\n",
    "    test_data_y = []\n",
    "    \n",
    "    for idx in train_group_indices:\n",
    "        start, end = groups[idx]\n",
    "        train_data_X.append(X_seq[start:end])\n",
    "        train_data_y.append(y_seq[start:end])\n",
    "    \n",
    "    for idx in test_group_indices:\n",
    "        start, end = groups[idx]\n",
    "        test_data_X.append(X_seq[start:end])\n",
    "        test_data_y.append(y_seq[start:end])\n",
    "    \n",
    "    X_train = np.concatenate(train_data_X, axis=0)\n",
    "    y_train = np.concatenate(train_data_y, axis=0)\n",
    "    X_test = np.concatenate(test_data_X, axis=0)\n",
    "    y_test = np.concatenate(test_data_y, axis=0)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, train_group_indices, test_group_indices\n",
    "\n",
    "# Demo: Show the splitting strategy\n",
    "# 演示：展示拆分策略\n",
    "print(\"=\"*60)\n",
    "print(\"Group-Based Data Splitting Strategy / 基于分组的数据拆分策略\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAs recommended in dataset.md:\")\n",
    "print(\"按照 dataset.md 的建议：\")\n",
    "print(\"\\n1. Divide data into 20 continuous time point groups\")\n",
    "print(\"   将数据分成20个连续的时间点组\")\n",
    "print(\"2. Randomly select 80% (16 groups) for training\")\n",
    "print(\"   随机选择80%（16组）用于训练\")\n",
    "print(\"3. Remaining 20% (4 groups) for testing\")\n",
    "print(\"   剩余20%（4组）用于测试\")\n",
    "print(\"\\nAdvantages / 优势:\")\n",
    "print(\"✓ Training and test data are completely disjoint\")\n",
    "print(\"  训练和测试数据完全不重叠\")\n",
    "print(\"✓ Maintains temporal continuity within each group\")\n",
    "print(\"  保持每组内的时间连续性\")\n",
    "print(\"✓ Random selection reduces bias\")\n",
    "print(\"  随机选择减少偏差\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: Actual splitting will be performed during model training\")\n",
    "print(\"注意：实际拆分将在模型训练期间执行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 步骤 5: 构建 RNN 模型 / Step 5: Build RNN Model\n",
    "\n",
    "我们将构建一个简单的 RNN 模型，包含：\n",
    "\n",
    "We'll build a Simple RNN model with:\n",
    "- 输入层：特征序列 / Input layer: sequences of features\n",
    "- RNN 层：捕获时间依赖关系 / RNN layers: to capture temporal dependencies\n",
    "- Dropout 层：用于正则化 / Dropout layers: for regularization\n",
    "- 全连接输出层：用于回归 / Dense output layer: for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbixy6jwfm7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create DataLoaders / 创建数据加载器的辅助函数\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create PyTorch DataLoaders from numpy arrays\n",
    "    从numpy数组创建PyTorch DataLoader\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : np.ndarray\n",
    "        Training features and labels / 训练特征和标签\n",
    "    X_test, y_test : np.ndarray\n",
    "        Test features and labels / 测试特征和标签\n",
    "    batch_size : int\n",
    "        Batch size for training / 训练批次大小\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Training and test data loaders / 训练和测试数据加载器\n",
    "    \"\"\"\n",
    "    # Convert numpy arrays to PyTorch tensors / 将numpy数组转换为PyTorch张量\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)  # Add dimension for labels\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
    "    \n",
    "    # Create TensorDatasets / 创建TensorDataset\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # Create DataLoaders / 创建DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"create_dataloaders function defined successfully!\")\n",
    "print(\"create_dataloaders 函数定义成功！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9c3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Simple RNN Model class / 定义简单RNN模型类\n",
    "\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN model for time series prediction\n",
    "    用于时间序列预测的简单RNN模型\n",
    "    \n",
    "    Architecture / 架构:\n",
    "    - Multiple RNN layers with dropout / 多个带dropout的RNN层\n",
    "    - Fully connected output layers / 全连接输出层\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features / 输入特征数\n",
    "        hidden_sizes : list of int\n",
    "            List of hidden layer sizes for RNN / RNN隐藏层大小列表\n",
    "        dropout : float\n",
    "            Dropout rate / Dropout率\n",
    "        \"\"\"\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "        \n",
    "        # Create RNN layers / 创建RNN层\n",
    "        self.rnn_layers = nn.ModuleList()\n",
    "        \n",
    "        # First RNN layer / 第一个RNN层\n",
    "        self.rnn_layers.append(\n",
    "            nn.RNN(input_size, hidden_sizes[0], batch_first=True)\n",
    "        )\n",
    "        \n",
    "        # Additional RNN layers / 额外的RNN层\n",
    "        for i in range(1, self.num_layers):\n",
    "            self.rnn_layers.append(\n",
    "                nn.RNN(hidden_sizes[i-1], hidden_sizes[i], batch_first=True)\n",
    "            )\n",
    "        \n",
    "        # Dropout layers / Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layers / 全连接层\n",
    "        self.fc1 = nn.Linear(hidden_sizes[-1], 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass / 前向传播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input sequences (batch_size, seq_length, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor\n",
    "            Predictions (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Pass through RNN layers / 通过RNN层\n",
    "        for i, rnn_layer in enumerate(self.rnn_layers):\n",
    "            x, _ = rnn_layer(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Take the last time step output / 取最后一个时间步的输出\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        # Pass through fully connected layers / 通过全连接层\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SimpleRNNModel class defined successfully!\")\n",
    "print(\"SimpleRNNModel 类定义成功！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture remains the same from previous cell (Simple RNNModel)\n",
    "# 模型架构与之前的cell相同（SimpleRNNModel）\n",
    "\n",
    "# Model parameters / 模型参数\n",
    "input_size = 6  # Number of features: HUFL, HULL, MUFL, MULL, LUFL, LULL / 特征数量\n",
    "hidden_sizes = [64, 32]  # RNN hidden units / RNN 隐藏单元\n",
    "dropout = 0.2  # Dropout rate / Dropout 率\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Model Architecture / 模型架构\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSimpleRNNModel:\")\n",
    "print(f\"  Input size / 输入大小: {input_size} features\")\n",
    "print(f\"  Hidden layers / 隐藏层: {hidden_sizes}\")\n",
    "print(f\"  Dropout rate / Dropout率: {dropout}\")\n",
    "print(f\"\\nArchitecture flow / 架构流程:\")\n",
    "print(f\"  {input_size} (features) → RNN({hidden_sizes[0]}) → Dropout({dropout})\")\n",
    "print(f\"  → RNN({hidden_sizes[1]}) → Dropout({dropout})\")\n",
    "print(f\"  → FC(16) → ReLU → Dropout → FC(1)\")\n",
    "print(f\"\\nOutput / 输出: Single value (oil temperature prediction)\")\n",
    "print(f\"        单个值（油温预测）\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a sample model to count parameters / 创建示例模型以统计参数\n",
    "sample_model = SimpleRNNModel(input_size, hidden_sizes, dropout)\n",
    "total_params = sum(p.numel() for p in sample_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in sample_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Parameters / 模型参数:\")\n",
    "print(f\"  Total / 总数: {total_params:,}\")\n",
    "print(f\"  Trainable / 可训练: {trainable_params:,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNote: We will create 6 instances of this model architecture\")\n",
    "print(\"注意：我们将创建此模型架构的6个实例\")\n",
    "print(\"  - 2 datasets × 3 prediction horizons = 6 models\")\n",
    "print(\"  - 2个数据集 × 3种预测时间范围 = 6个模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3qafk9ulqdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function / 定义训练函数\n",
    "\n",
    "def train_rnn_model(train_loader, test_loader, input_size, hidden_sizes, dropout=0.2,\n",
    "                    num_epochs=100, lr=0.001, patience=10, dataset_name='Model', device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the RNN model with early stopping\n",
    "    使用早停训练RNN模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Training and test data loaders\n",
    "    input_size : int\n",
    "        Number of input features\n",
    "    hidden_sizes : list of int\n",
    "        Hidden layer sizes\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    num_epochs : int\n",
    "        Maximum number of epochs\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    patience : int\n",
    "        Early stopping patience\n",
    "    dataset_name : str\n",
    "        Name for logging\n",
    "    device : str or torch.device\n",
    "        Device to train on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : SimpleRNNModel\n",
    "        Trained model\n",
    "    history : dict\n",
    "        Training history with train/test loss and MAE\n",
    "    \"\"\"\n",
    "    # Initialize model / 初始化模型\n",
    "    model = SimpleRNNModel(input_size, hidden_sizes, dropout).to(device)\n",
    "    \n",
    "    # Loss function and optimizer / 损失函数和优化器\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Learning rate scheduler / 学习率调度器\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training history / 训练历史\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_mae': [],\n",
    "        'test_mae': []\n",
    "    }\n",
    "    \n",
    "    # Early stopping variables / 早停变量\n",
    "    best_test_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training {dataset_name}...\")\n",
    "    print(f\"训练 {dataset_name}...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase / 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Forward pass / 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass / 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_mae += torch.abs(outputs - y_batch).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mae /= len(train_loader.dataset)\n",
    "        \n",
    "        # Test phase / 测试阶段\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_mae = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                outputs = model(X_batch).squeeze()\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "                test_mae += torch.abs(outputs - y_batch).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_mae /= len(test_loader.dataset)\n",
    "        \n",
    "        # Update history / 更新历史\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['test_mae'].append(test_mae)\n",
    "        \n",
    "        # Learning rate scheduling / 学习率调度\n",
    "        scheduler.step(test_loss)\n",
    "        \n",
    "        # Print progress every 10 epochs / 每10个轮次打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "                  f'Train Loss: {train_loss:.6f}, Train MAE: {train_mae:.6f} | '\n",
    "                  f'Test Loss: {test_loss:.6f}, Test MAE: {test_mae:.6f}')\n",
    "        \n",
    "        # Early stopping check / 早停检查\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping triggered at epoch {epoch+1}')\n",
    "            print(f'在第 {epoch+1} 轮触发早停')\n",
    "            break\n",
    "    \n",
    "    # Load best model / 加载最佳模型\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f'\\nTraining completed! Best test loss: {best_test_loss:.6f}')\n",
    "    print(f'训练完成！最佳测试损失: {best_test_loss:.6f}\\n')\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"train_rnn_model function defined successfully!\")\n",
    "print(\"train_rnn_model 函数定义成功！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 步骤 6: 训练模型 / Step 6: Train the Model\n",
    "\n",
    "使用以下方法训练 RNN：\n",
    "\n",
    "Train the RNN with:\n",
    "- Adam 优化器 / Adam optimizer\n",
    "- MSE 损失函数 / MSE loss function\n",
    "- 学习率调度器：提高收敛性 / Learning rate scheduler: improve convergence\n",
    "- 早停：防止过拟合 / Early stopping: prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training all 6 models: 2 datasets × 3 prediction horizons\n",
    "# 训练所有6个模型：2个数据集 × 3种预测时间范围\n",
    "\n",
    "# Storage for all models and results / 存储所有模型和结果\n",
    "all_models = {}\n",
    "all_histories = {}\n",
    "all_predictions = {}\n",
    "all_metrics = {}\n",
    "all_scalers = {}\n",
    "all_data_info = {}\n",
    "\n",
    "# Datasets configuration / 数据集配置\n",
    "datasets_config = {\n",
    "    'Dataset1': {'df': df_train1, 'X': X1, 'y': y1, 'scaler_X': scaler_X1, 'scaler_y': scaler_y1},\n",
    "    'Dataset2': {'df': df_train2, 'X': X2, 'y': y2, 'scaler_X': scaler_X2, 'scaler_y': scaler_y2}\n",
    "}\n",
    "\n",
    "# Training hyperparameters / 训练超参数\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "patience = 10\n",
    "batch_size = 32\n",
    "n_groups = 20\n",
    "train_ratio = 0.8\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING 6 RNN MODELS / 训练6个RNN模型\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Configuration / 配置:\")\n",
    "print(f\"  Datasets / 数据集: 2 (Dataset1, Dataset2)\")\n",
    "print(f\"  Prediction horizons / 预测时间范围: 3 (hour, day, week)\")\n",
    "print(f\"  Total models / 总模型数: 6\")\n",
    "print(f\"  Epochs / 轮数: {num_epochs}\")\n",
    "print(f\"  Batch size / 批次大小: {batch_size}\")\n",
    "print(f\"  Data split / 数据拆分: {n_groups} groups, {int(train_ratio*100)}% train\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train each model / 训练每个模型\n",
    "model_counter = 0\n",
    "\n",
    "for dataset_name, dataset_config in datasets_config.items():\n",
    "    for pred_key, pred_config in PREDICTION_CONFIGS.items():\n",
    "        model_counter += 1\n",
    "        model_name = f\"{dataset_name}_{pred_key}\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"[{model_counter}/6] Training Model: {model_name}\")\n",
    "        print(f\"[{model_counter}/6] 训练模型：{model_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Configuration / 配置:\")\n",
    "        print(f\"  Dataset / 数据集: {dataset_name}\")\n",
    "        print(f\"  Prediction type / 预测类型: {pred_config['description']}\")\n",
    "        print(f\"  Offset / 偏移: {pred_config['offset']} time points\")\n",
    "        print(f\"  Sequence length / 序列长度: {pred_config['seq_length']} time points\")\n",
    "        print(f\"  Total lookback / 总回溯: {pred_config['offset'] + pred_config['seq_length']} time points\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # 1. Get data / 获取数据\n",
    "        X_scaled = dataset_config['scaler_X'].transform(dataset_config['X'])\n",
    "        y_scaled = dataset_config['scaler_y'].transform(dataset_config['y'].reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # 2. Create sequences with offset / 使用偏移创建序列\n",
    "        X_seq, y_seq = create_sequences_with_offset(\n",
    "            X_scaled, y_scaled,\n",
    "            seq_length=pred_config['seq_length'],\n",
    "            offset=pred_config['offset']\n",
    "        )\n",
    "        \n",
    "        print(f\"Created {len(X_seq):,} sequences / 创建了{len(X_seq):,}个序列\")\n",
    "        \n",
    "        # 3. Split data by groups / 按组拆分数据\n",
    "        X_train, X_test, y_train, y_test, train_groups, test_groups = split_train_test_by_groups(\n",
    "            X_seq, y_seq, train_ratio=train_ratio, n_groups=n_groups\n",
    "        )\n",
    "        \n",
    "        print(f\"Split into groups / 拆分为组:\")\n",
    "        print(f\"  Train groups / 训练组: {train_groups}\")\n",
    "        print(f\"  Test groups / 测试组: {test_groups}\")\n",
    "        print(f\"  Train samples / 训练样本: {len(X_train):,}\")\n",
    "        print(f\"  Test samples / 测试样本: {len(X_test):,}\\n\")\n",
    "        \n",
    "        # 4. Create DataLoaders / 创建数据加载器\n",
    "        train_loader, test_loader = create_dataloaders(\n",
    "            X_train, y_train, X_test, y_test, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # 5. Train model / 训练模型\n",
    "        print(f\"Starting training... / 开始训练...\\n\")\n",
    "        model, history = train_rnn_model(\n",
    "            train_loader, test_loader,\n",
    "            input_size=X_train.shape[2],\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            dropout=dropout,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=learning_rate,\n",
    "            patience=patience,\n",
    "            dataset_name=model_name,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 6. Store results / 存储结果\n",
    "        all_models[model_name] = model\n",
    "        all_histories[model_name] = history\n",
    "        all_scalers[model_name] = dataset_config['scaler_y']\n",
    "        all_data_info[model_name] = {\n",
    "            'train_groups': train_groups,\n",
    "            'test_groups': test_groups,\n",
    "            'n_train': len(X_train),\n",
    "            'n_test': len(X_test),\n",
    "            'X_train': X_train,\n",
    "            'X_test': X_test,\n",
    "            'y_train': y_train,\n",
    "            'y_test': y_test,\n",
    "            'train_loader': train_loader,\n",
    "            'test_loader': test_loader\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n✓ Model {model_name} training completed!\")\n",
    "        print(f\"✓ 模型 {model_name} 训练完成！\")\n",
    "        print(f\"  Best test loss: {min(history['test_loss']):.6f}\")\n",
    "        print(f\"  最佳测试损失: {min(history['test_loss']):.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL 6 MODELS TRAINING COMPLETED! / 所有6个模型训练完成！\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTrained models / 已训练模型:\")\n",
    "for i, model_name in enumerate(all_models.keys(), 1):\n",
    "    best_loss = min(all_histories[model_name]['test_loss'])\n",
    "    print(f\"  {i}. {model_name}: Best Test Loss = {best_loss:.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "### 6.1 训练历史可视化 / Training History Visualization\n",
    "\n",
    "绘制训练和验证损失曲线以检查过拟合\n",
    "\n",
    "Plot training and validation loss curves to check for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history for all 6 models / 可视化所有6个模型的训练历史\n",
    "\n",
    "# 1. Individual model training histories / 各模型训练历史\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_names = list(all_histories.keys())\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    history = all_histories[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.plot(history['train_loss'], label='Train Loss / 训练损失', linewidth=2, alpha=0.7)\n",
    "    ax.plot(history['test_loss'], label='Test Loss / 测试损失', linewidth=2, alpha=0.7)\n",
    "    ax.set_title(f'{model_name} - Training History / 训练历史', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / 轮次')\n",
    "    ax.set_ylabel('Loss (MSE) / 损失')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Training History for All 6 Models / 所有6个模型的训练历史', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 2. Comparison by prediction type / 按预测类型对比\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "for idx, pred_type in enumerate(['hour', 'day', 'week']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for dataset in ['Dataset1', 'Dataset2']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        history = all_histories[model_name]\n",
    "        ax.plot(history['test_loss'], label=dataset, linewidth=2.5)\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    ax.set_title(f'{pred_config[\"name\"]} Prediction Comparison\\n{pred_config[\"description\"]}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / 轮次')\n",
    "    ax.set_ylabel('Test Loss (MSE) / 测试损失')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison by Prediction Type / 按预测类型对比', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparison by dataset / 按数据集对比\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for idx, dataset in enumerate(['Dataset1', 'Dataset2']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for pred_type in ['hour', 'day', 'week']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        history = all_histories[model_name]\n",
    "        pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "        ax.plot(history['test_loss'], label=pred_config['name'], linewidth=2.5)\n",
    "    \n",
    "    ax.set_title(f'{dataset} - All Prediction Types\\n{dataset} - 所有预测类型', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch / 轮次')\n",
    "    ax.set_ylabel('Test Loss (MSE) / 测试损失')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparison by Dataset / 按数据集对比', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics summary / 打印最终指标总结\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL TRAINING METRICS SUMMARY / 最终训练指标总结\")\n",
    "print(\"=\"*80)\n",
    "for model_name in model_names:\n",
    "    history = all_histories[model_name]\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Final Test Loss / 最终测试损失: {history['test_loss'][-1]:.6f}\")\n",
    "    print(f\"  Best Test Loss / 最佳测试损失: {min(history['test_loss']):.6f}\")\n",
    "    print(f\"  Final Test MAE / 最终测试MAE: {history['test_mae'][-1]:.6f}\")\n",
    "    print(f\"  Best Test MAE / 最佳测试MAE: {min(history['test_mae']):.6f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## 步骤 7: 模型评估 / Step 7: Model Evaluation\n",
    "\n",
    "### 7.1 生成预测 / Make Predictions\n",
    "\n",
    "在测试集上生成预测并反向转换到原始尺度\n",
    "\n",
    "Generate predictions on test set and inverse transform to original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch6fv9outch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function / 定义预测函数\n",
    "\n",
    "def make_predictions(model, train_loader, test_loader, scaler_y, device='cpu'):\n",
    "    \"\"\"\n",
    "    Generate predictions on train and test data\n",
    "    在训练和测试数据上生成预测\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SimpleRNNModel\n",
    "        Trained model\n",
    "    train_loader, test_loader : DataLoader\n",
    "        Data loaders\n",
    "    scaler_y : StandardScaler\n",
    "        Scaler for inverse transformation (not used here, but kept for compatibility)\n",
    "    device : str or torch.device\n",
    "        Device for inference\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y_train_pred : np.ndarray\n",
    "        Training predictions (normalized scale)\n",
    "    y_test_pred : np.ndarray\n",
    "        Test predictions (normalized scale)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Training predictions / 训练预测\n",
    "    train_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            train_predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_train_pred = np.concatenate(train_predictions)\n",
    "    \n",
    "    # Test predictions / 测试预测\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            test_predictions.append(outputs.cpu().numpy())\n",
    "    \n",
    "    y_test_pred = np.concatenate(test_predictions)\n",
    "    \n",
    "    return y_train_pred, y_test_pred\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"make_predictions function defined successfully!\")\n",
    "print(\"make_predictions 函数定义成功！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all 6 models / 为所有6个模型生成预测\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING PREDICTIONS FOR ALL 6 MODELS\")\n",
    "print(\"为所有6个模型生成预测\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in all_models.keys():\n",
    "    print(f\"\\nGenerating predictions for {model_name}...\")\n",
    "    print(f\"为 {model_name} 生成预测...\")\n",
    "    \n",
    "    model = all_models[model_name]\n",
    "    scaler_y = all_scalers[model_name]\n",
    "    data_info = all_data_info[model_name]\n",
    "    \n",
    "    # Generate predictions / 生成预测\n",
    "    y_train_pred, y_test_pred = make_predictions(\n",
    "        model, data_info['train_loader'], data_info['test_loader'], scaler_y, device\n",
    "    )\n",
    "    \n",
    "    # Inverse transform to original scale / 反向转换到原始尺度\n",
    "    y_train_actual = scaler_y.inverse_transform(data_info['y_train'].reshape(-1, 1)).flatten()\n",
    "    y_train_pred_inv = scaler_y.inverse_transform(y_train_pred.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = scaler_y.inverse_transform(data_info['y_test'].reshape(-1, 1)).flatten()\n",
    "    y_test_pred_inv = scaler_y.inverse_transform(y_test_pred.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Store predictions / 存储预测\n",
    "    all_predictions[model_name] = {\n",
    "        'y_train_actual': y_train_actual,\n",
    "        'y_train_pred': y_train_pred_inv,\n",
    "        'y_test_actual': y_test_actual,\n",
    "        'y_test_pred': y_test_pred_inv\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Predictions generated / 预测已生成\")\n",
    "    print(f\"    Train samples / 训练样本: {len(y_train_actual):,}\")\n",
    "    print(f\"    Test samples / 测试样本: {len(y_test_actual):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL PREDICTIONS GENERATED! / 所有预测已生成！\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "### 7.2 计算评估指标 / Calculate Evaluation Metrics\n",
    "\n",
    "计算标准回归指标以评估模型性能\n",
    "\n",
    "Compute standard regression metrics to assess model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcx4gj1sddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics calculation function / 定义指标计算函数\n",
    "\n",
    "def calculate_metrics(y_actual, y_pred, dataset_name='Dataset'):\n",
    "    \"\"\"\n",
    "    Calculate regression metrics\n",
    "    计算回归指标\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_actual : np.ndarray\n",
    "        Actual values\n",
    "    y_pred : np.ndarray\n",
    "        Predicted values\n",
    "    dataset_name : str\n",
    "        Name for display\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary containing R2, RMSE, MAE, MAPE\n",
    "    \"\"\"\n",
    "    # R² Score / R²分数\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    \n",
    "    # RMSE (Root Mean Squared Error) / 均方根误差\n",
    "    rmse = np.sqrt(mean_squared_error(y_actual, y_pred))\n",
    "    \n",
    "    # MAE (Mean Absolute Error) / 平均绝对误差\n",
    "    mae = mean_absolute_error(y_actual, y_pred)\n",
    "    \n",
    "    # MAPE (Mean Absolute Percentage Error) / 平均绝对百分比误差\n",
    "    # Avoid division by zero / 避免除零错误\n",
    "    mask = y_actual != 0\n",
    "    mape = np.mean(np.abs((y_actual[mask] - y_pred[mask]) / y_actual[mask])) * 100\n",
    "    \n",
    "    # Store metrics / 存储指标\n",
    "    metrics = {\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    # Print metrics / 打印指标\n",
    "    print(f\"\\n{dataset_name} Performance Metrics / {dataset_name} 性能指标:\")\n",
    "    print(f\"  R² Score / R²分数: {r2:.4f}\")\n",
    "    print(f\"  RMSE / 均方根误差: {rmse:.4f}°C\")\n",
    "    print(f\"  MAE / 平均绝对误差: {mae:.4f}°C\")\n",
    "    print(f\"  MAPE / 平均绝对百分比误差: {mape:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"calculate_metrics function defined successfully!\")\n",
    "print(\"calculate_metrics 函数定义成功！\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all 6 models / 为所有6个模型计算指标\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CALCULATING METRICS FOR ALL 6 MODELS\")\n",
    "print(\"为所有6个模型计算指标\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for model_name in all_models.keys():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name.upper()} PERFORMANCE\")\n",
    "    print(f\"{model_name.upper()} 性能\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    preds = all_predictions[model_name]\n",
    "    \n",
    "    # Calculate metrics / 计算指标\n",
    "    train_metrics = calculate_metrics(\n",
    "        preds['y_train_actual'], preds['y_train_pred'], 'Training / 训练'\n",
    "    )\n",
    "    test_metrics = calculate_metrics(\n",
    "        preds['y_test_actual'], preds['y_test_pred'], 'Test / 测试'\n",
    "    )\n",
    "    \n",
    "    # Store metrics / 存储指标\n",
    "    all_metrics[model_name] = {\n",
    "        'train': train_metrics,\n",
    "        'test': test_metrics\n",
    "    }\n",
    "\n",
    "# Create comprehensive comparison table / 创建综合对比表\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE PERFORMANCE COMPARISON / 综合性能对比\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "for model_name in all_models.keys():\n",
    "    dataset, pred_type = model_name.split('_')\n",
    "    test_metrics = all_metrics[model_name]['test']\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model / 模型': model_name,\n",
    "        'Dataset / 数据集': dataset,\n",
    "        'Prediction / 预测': pred_type,\n",
    "        'R²': test_metrics['R2'],\n",
    "        'RMSE (°C)': test_metrics['RMSE'],\n",
    "        'MAE (°C)': test_metrics['MAE'],\n",
    "        'MAPE (%)': test_metrics['MAPE']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nTest Set Performance / 测试集性能:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best models for each metric / 找出每个指标的最佳模型\n",
    "print(\"\\n### Best Models by Metric / 各指标最佳模型:\")\n",
    "best_r2 = comparison_df.loc[comparison_df['R²'].idxmax()]\n",
    "best_rmse = comparison_df.loc[comparison_df['RMSE (°C)'].idxmin()]\n",
    "best_mae = comparison_df.loc[comparison_df['MAE (°C)'].idxmin()]\n",
    "\n",
    "print(f\"\\n✓ Highest R²: {best_r2['Model / 模型']} (R² = {best_r2['R²']:.4f})\")\n",
    "print(f\"  最高 R²: {best_r2['Model / 模型']} (R² = {best_r2['R²']:.4f})\")\n",
    "print(f\"\\n✓ Lowest RMSE: {best_rmse['Model / 模型']} (RMSE = {best_rmse['RMSE (°C)']:.4f}°C)\")\n",
    "print(f\"  最低 RMSE: {best_rmse['Model / 模型']} (RMSE = {best_rmse['RMSE (°C)']:.4f}°C)\")\n",
    "print(f\"\\n✓ Lowest MAE: {best_mae['Model / 模型']} (MAE = {best_mae['MAE (°C)']:.4f}°C)\")\n",
    "print(f\"  最低 MAE: {best_mae['Model / 模型']} (MAE = {best_mae['MAE (°C)']:.4f}°C)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results-header",
   "metadata": {},
   "source": [
    "## 步骤 8: 结果可视化 / Step 8: Results Visualization\n",
    "\n",
    "### 8.1 实际值 vs 预测值 / Actual vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for all 6 models / 可视化所有6个模型的预测结果\n",
    "\n",
    "# 1. All models predictions (zoomed view) / 所有模型预测（放大视图）\n",
    "zoom_range = 300\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "model_names = list(all_predictions.keys())\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    preds = all_predictions[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.plot(preds['y_test_actual'][:zoom_range], label='Actual / 实际', \n",
    "            alpha=0.8, linewidth=1.5)\n",
    "    ax.plot(preds['y_test_pred'][:zoom_range], label='Predicted / 预测', \n",
    "            alpha=0.8, linewidth=1.5)\n",
    "    \n",
    "    r2 = all_metrics[model_name]['test']['R2']\n",
    "    mae = all_metrics[model_name]['test']['MAE']\n",
    "    \n",
    "    ax.set_title(f'{model_name}\\nR²={r2:.4f}, MAE={mae:.4f}°C', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Time Step / 时间步')\n",
    "    ax.set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f'Predictions for All 6 Models (First {zoom_range} Points) / 所有6个模型的预测（前{zoom_range}个点）', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 2. Scatter plots for all models / 所有模型的散点图\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(model_names):\n",
    "    preds = all_predictions[model_name]\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.scatter(preds['y_test_actual'], preds['y_test_pred'], alpha=0.4, s=15)\n",
    "    \n",
    "    # Perfect prediction line / 完美预测线\n",
    "    min_val = min(preds['y_test_actual'].min(), preds['y_test_pred'].min())\n",
    "    max_val = max(preds['y_test_actual'].max(), preds['y_test_pred'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, \n",
    "            label='Perfect Prediction / 完美预测')\n",
    "    \n",
    "    r2 = all_metrics[model_name]['test']['R2']\n",
    "    ax.set_title(f'{model_name} (R²={r2:.4f})', fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('Actual (°C) / 实际')\n",
    "    ax.set_ylabel('Predicted (°C) / 预测')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Predicted vs Actual for All 6 Models / 所有6个模型的预测vs实际', \n",
    "             fontsize=16, fontweight='bold', y=1.002)\n",
    "plt.show()\n",
    "\n",
    "# 3. Comparison by prediction type / 按预测类型对比预测结果\n",
    "fig, axes = plt.subplots(1, 3, figsize=(22, 6))\n",
    "\n",
    "for idx, pred_type in enumerate(['hour', 'day', 'week']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Show Dataset1 predictions for this type\n",
    "    model_name = f\"Dataset1_{pred_type}\"\n",
    "    preds = all_predictions[model_name]\n",
    "    \n",
    "    ax.plot(preds['y_test_actual'][:zoom_range], label='Actual / 实际', \n",
    "            alpha=0.8, linewidth=2, color='black')\n",
    "    \n",
    "    for dataset in ['Dataset1', 'Dataset2']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        preds = all_predictions[model_name]\n",
    "        ax.plot(preds['y_test_pred'][:zoom_range], \n",
    "                label=f'{dataset} Prediction', alpha=0.7, linewidth=1.5)\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    ax.set_title(f'{pred_config[\"name\"]} Prediction\\n{pred_config[\"description\"]}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time Step / 时间步')\n",
    "    ax.set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Prediction Comparison by Type / 按类型对比预测', \n",
    "             fontsize=14, fontweight='bold', y=1.002)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter-header",
   "metadata": {},
   "source": [
    "### 8.2 散点图：预测值 vs 实际值 / Scatter Plot: Predicted vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for both models / 两个模型的散点图\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Model 1\n",
    "axes[0].scatter(y1_test_actual, y1_test_pred_inv, alpha=0.5, s=20)\n",
    "axes[0].plot([y1_test_actual.min(), y1_test_actual.max()], \n",
    "             [y1_test_actual.min(), y1_test_actual.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction / 完美预测')\n",
    "axes[0].set_xlabel('Actual Oil Temperature (°C) / 实际油温 (°C)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Oil Temperature (°C) / 预测油温 (°C)', fontsize=12)\n",
    "axes[0].set_title(f'Model 1 (Dataset 1): Predicted vs Actual (R²={test1_metrics[\"R2\"]:.4f}) / 预测 vs 实际', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2\n",
    "axes[1].scatter(y2_test_actual, y2_test_pred_inv, alpha=0.5, s=20, color='orange')\n",
    "axes[1].plot([y2_test_actual.min(), y2_test_actual.max()], \n",
    "             [y2_test_actual.min(), y2_test_actual.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction / 完美预测')\n",
    "axes[1].set_xlabel('Actual Oil Temperature (°C) / 实际油温 (°C)', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Oil Temperature (°C) / 预测油温 (°C)', fontsize=12)\n",
    "axes[1].set_title(f'Model 2 (Dataset 2): Predicted vs Actual (R²={test2_metrics[\"R2\"]:.4f}) / 预测 vs 实际', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residuals-header",
   "metadata": {},
   "source": [
    "### 8.3 残差分析 / Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis for both models / 两个模型的残差分析\n",
    "residuals1 = y1_test_actual - y1_test_pred_inv\n",
    "residuals2 = y2_test_actual - y2_test_pred_inv\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "\n",
    "# Model 1 - Residual plot / 模型1 - 残差图\n",
    "axes[0, 0].scatter(y1_test_pred_inv, residuals1, alpha=0.5, s=20)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Predicted Oil Temperature (°C) / 预测油温 (°C)')\n",
    "axes[0, 0].set_ylabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[0, 0].set_title('Model 1 (Dataset 1): Residual Plot / 残差图')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 1 - Residual distribution / 模型1 - 残差分布\n",
    "axes[0, 1].hist(residuals1, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[0, 1].set_ylabel('Frequency / 频率')\n",
    "axes[0, 1].set_title('Model 1 (Dataset 1): Distribution of Residuals / 残差分布')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2 - Residual plot / 模型2 - 残差图\n",
    "axes[1, 0].scatter(y2_test_pred_inv, residuals2, alpha=0.5, s=20, color='orange')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Oil Temperature (°C) / 预测油温 (°C)')\n",
    "axes[1, 0].set_ylabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[1, 0].set_title('Model 2 (Dataset 2): Residual Plot / 残差图')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Model 2 - Residual distribution / 模型2 - 残差分布\n",
    "axes[1, 1].hist(residuals2, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[1, 1].set_ylabel('Frequency / 频率')\n",
    "axes[1, 1].set_title('Model 2 (Dataset 2): Distribution of Residuals / 残差分布')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics / 残差统计\n",
    "print(\"=\"*60)\n",
    "print(\"Residual Statistics / 残差统计\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel 1 (Dataset 1):\")\n",
    "print(f\"  Mean / 均值: {residuals1.mean():.4f}°C\")\n",
    "print(f\"  Std / 标准差: {residuals1.std():.4f}°C\")\n",
    "print(f\"  Min / 最小值: {residuals1.min():.4f}°C\")\n",
    "print(f\"  Max / 最大值: {residuals1.max():.4f}°C\")\n",
    "\n",
    "print(f\"\\nModel 2 (Dataset 2):\")\n",
    "print(f\"  Mean / 均值: {residuals2.mean():.4f}°C\")\n",
    "print(f\"  Std / 标准差: {residuals2.std():.4f}°C\")\n",
    "print(f\"  Min / 最小值: {residuals2.min():.4f}°C\")\n",
    "print(f\"  Max / 最大值: {residuals2.max():.4f}°C\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 步骤 9: 结论和总结 / Step 9: Conclusion and Summary\n",
    "\n",
    "### 模型性能总结 / Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary and analysis of all 6 models / 所有6个模型的综合总结和分析\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE SUMMARY: 6 RNN MODELS FOR OIL TEMPERATURE PREDICTION\")\n",
    "print(\"综合总结：6个用于油温预测的RNN模型\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 1. Overview table / 概览表\n",
    "print(\"\\n### 1. Model Overview / 模型概览:\")\n",
    "overview_data = []\n",
    "for model_name in all_models.keys():\n",
    "    dataset, pred_type = model_name.split('_')\n",
    "    test_metrics = all_metrics[model_name]['test']\n",
    "    data_info = all_data_info[model_name]\n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    \n",
    "    overview_data.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset,\n",
    "        'Prediction': pred_config['name'],\n",
    "        'Offset': pred_config['offset'],\n",
    "        'Seq Length': pred_config['seq_length'],\n",
    "        'Train Samples': f\"{data_info['n_train']:,}\",\n",
    "        'Test Samples': f\"{data_info['n_test']:,}\",\n",
    "        'R²': f\"{test_metrics['R2']:.4f}\",\n",
    "        'MAE (°C)': f\"{test_metrics['MAE']:.4f}\",\n",
    "        'RMSE (°C)': f\"{test_metrics['RMSE']:.4f}\"\n",
    "    })\n",
    "\n",
    "overview_df = pd.DataFrame(overview_data)\n",
    "print(overview_df.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 2. Best performing models / 最佳模型\n",
    "print(\"\\n### 2. Best Performing Models / 最佳性能模型:\")\n",
    "comparison_df_numeric = pd.DataFrame({\n",
    "    'Model': [d['Model'] for d in overview_data],\n",
    "    'R²': [all_metrics[d['Model']]['test']['R2'] for d in overview_data],\n",
    "    'MAE': [all_metrics[d['Model']]['test']['MAE'] for d in overview_data],\n",
    "    'RMSE': [all_metrics[d['Model']]['test']['RMSE'] for d in overview_data],\n",
    "    'MAPE': [all_metrics[d['Model']]['test']['MAPE'] for d in overview_data]\n",
    "})\n",
    "\n",
    "best_overall = comparison_df_numeric.loc[comparison_df_numeric['R²'].idxmax()]\n",
    "print(f\"\\n✓ Overall Best Model (Highest R²) / 总体最佳模型（最高R²）:\")\n",
    "print(f\"  {best_overall['Model']}\")\n",
    "print(f\"  R² = {best_overall['R²']:.4f}\")\n",
    "print(f\"  MAE = {best_overall['MAE']:.4f}°C\")\n",
    "print(f\"  RMSE = {best_overall['RMSE']:.4f}°C\")\n",
    "print(f\"  MAPE = {best_overall['MAPE']:.2f}%\")\n",
    "\n",
    "# Best for each prediction type\n",
    "print(f\"\\n✓ Best Models by Prediction Type / 各预测类型的最佳模型:\")\n",
    "for pred_type in ['hour', 'day', 'week']:\n",
    "    models_of_type = [m for m in all_models.keys() if pred_type in m]\n",
    "    type_df = comparison_df_numeric[comparison_df_numeric['Model'].isin(models_of_type)]\n",
    "    best_of_type = type_df.loc[type_df['R²'].idxmax()]\n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    \n",
    "    print(f\"\\n  {pred_config['name']} ({pred_config['description']}):\")\n",
    "    print(f\"    Model: {best_of_type['Model']}\")\n",
    "    print(f\"    R² = {best_of_type['R²']:.4f}, MAE = {best_of_type['MAE']:.4f}°C\")\n",
    "\n",
    "# 3. Performance comparison across datasets / 跨数据集性能对比\n",
    "print(\"\\n### 3. Performance Comparison Across Datasets / 跨数据集性能对比:\")\n",
    "for pred_type in ['hour', 'day', 'week']:\n",
    "    d1_model = f\"Dataset1_{pred_type}\"\n",
    "    d2_model = f\"Dataset2_{pred_type}\"\n",
    "    \n",
    "    d1_r2 = all_metrics[d1_model]['test']['R2']\n",
    "    d2_r2 = all_metrics[d2_model]['test']['R2']\n",
    "    d1_mae = all_metrics[d1_model]['test']['MAE']\n",
    "    d2_mae = all_metrics[d2_model]['test']['MAE']\n",
    "    \n",
    "    pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "    print(f\"\\n{pred_config['name']} Prediction:\")\n",
    "    print(f\"  Dataset1: R²={d1_r2:.4f}, MAE={d1_mae:.4f}°C\")\n",
    "    print(f\"  Dataset2: R²={d2_r2:.4f}, MAE={d2_mae:.4f}°C\")\n",
    "    \n",
    "    if d1_r2 > d2_r2:\n",
    "        print(f\"  → Dataset1 performs better / Dataset1表现更好\")\n",
    "    else:\n",
    "        print(f\"  → Dataset2 performs better / Dataset2表现更好\")\n",
    "\n",
    "# 4. Performance comparison across prediction types / 跨预测类型性能对比\n",
    "print(\"\\n### 4. Performance Comparison Across Prediction Types / 跨预测类型性能对比:\")\n",
    "for dataset in ['Dataset1', 'Dataset2']:\n",
    "    print(f\"\\n{dataset}:\")\n",
    "    for pred_type in ['hour', 'day', 'week']:\n",
    "        model_name = f\"{dataset}_{pred_type}\"\n",
    "        r2 = all_metrics[model_name]['test']['R2']\n",
    "        mae = all_metrics[model_name]['test']['MAE']\n",
    "        pred_config = PREDICTION_CONFIGS[pred_type]\n",
    "        print(f\"  {pred_config['name']:6s}: R²={r2:.4f}, MAE={mae:.4f}°C\")\n",
    "\n",
    "# 5. Data split strategy verification / 数据拆分策略验证\n",
    "print(\"\\n### 5. Data Split Strategy Verification / 数据拆分策略验证:\")\n",
    "print(\"Using group-based splitting (dataset.md recommendation):\")\n",
    "print(\"使用基于分组的拆分（dataset.md 推荐）:\")\n",
    "sample_model = list(all_data_info.keys())[0]\n",
    "sample_info = all_data_info[sample_model]\n",
    "print(f\"\\n  ✓ Total groups / 总组数: {n_groups}\")\n",
    "print(f\"  ✓ Training groups / 训练组: {len(sample_info['train_groups'])} ({train_ratio*100:.0f}%)\")\n",
    "print(f\"  ✓ Test groups / 测试组: {len(sample_info['test_groups'])} ({(1-train_ratio)*100:.0f}%)\")\n",
    "print(f\"  ✓ Training groups / 训练组: {sample_info['train_groups']}\")\n",
    "print(f\"  ✓ Test groups / 测试组: {sample_info['test_groups']}\")\n",
    "print(\"\\n  Advantages / 优势:\")\n",
    "print(\"  • Training and test data are completely disjoint / 训练和测试数据完全不重叠\")\n",
    "print(\"  • Maintains temporal continuity within groups / 保持组内时间连续性\")\n",
    "print(\"  • Random selection reduces bias / 随机选择减少偏差\")\n",
    "\n",
    "# 6. Key insights / 关键见解\n",
    "print(\"\\n### 6. Key Insights / 关键见解:\")\n",
    "print(\"\\n1. Prediction Horizon Impact / 预测时间范围的影响:\")\n",
    "print(\"   - Hour predictions typically have higher R² / 小时预测通常具有更高的R²\")\n",
    "print(\"   - Longer horizons (week) are more challenging / 更长的时间范围（周）更具挑战性\")\n",
    "\n",
    "print(\"\\n2. Dataset Characteristics / 数据集特征:\")\n",
    "print(f\"   - Dataset1 temperature range: {df_train1['OT'].min():.2f}°C - {df_train1['OT'].max():.2f}°C\")\n",
    "print(f\"   - Dataset2 temperature range: {df_train2['OT'].min():.2f}°C - {df_train2['OT'].max():.2f}°C\")\n",
    "print(\"   - Dataset2 has significantly higher temperatures / Dataset2温度显著更高\")\n",
    "\n",
    "print(\"\\n3. Model Architecture / 模型架构:\")\n",
    "print(f\"   - Input features: {input_size} (HUFL, HULL, MUFL, MULL, LUFL, LULL)\")\n",
    "print(f\"   - RNN hidden units: {hidden_sizes}\")\n",
    "print(f\"   - Total parameters: {total_params:,}\")\n",
    "print(f\"   - Same architecture for all 6 models / 所有6个模型使用相同架构\")\n",
    "\n",
    "# 7. Next steps / 下一步\n",
    "print(\"\\n### 7. Next Steps / 下一步:\")\n",
    "print(\"\\n1. Model Improvements / 模型改进:\")\n",
    "print(\"   • Try LSTM/GRU for better long-term dependencies / 尝试LSTM/GRU以更好地捕获长期依赖\")\n",
    "print(\"   • Experiment with different sequence lengths / 实验不同的序列长度\")\n",
    "print(\"   • Add attention mechanisms / 添加注意力机制\")\n",
    "\n",
    "print(\"\\n2. Feature Engineering / 特征工程:\")\n",
    "print(\"   • Explore feature importance and selection / 探索特征重要性和选择\")\n",
    "print(\"   • Add time-based features (hour, day, season) / 添加基于时间的特征\")\n",
    "print(\"   • Consider feature interactions / 考虑特征交互\")\n",
    "\n",
    "print(\"\\n3. Model Comparison / 模型比较:\")\n",
    "print(\"   • Compare with traditional ML (Linear Regression, Random Forest, MLP)\")\n",
    "print(\"   与传统机器学习比较（线性回归、随机森林、MLP）\")\n",
    "print(\"   • Compare with SOTA model (Informer)\")\n",
    "print(\"   与SOTA模型比较（Informer）\")\n",
    "\n",
    "print(\"\\n4. Hyperparameter Tuning / 超参数调优:\")\n",
    "print(\"   • Grid search for optimal seq_length / 网格搜索最优序列长度\")\n",
    "print(\"   • Learning rate scheduling / 学习率调度\")\n",
    "print(\"   • Batch size optimization / 批次大小优化\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ANALYSIS COMPLETE! / 分析完成！\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Save comparison results to CSV / 将对比结果保存为CSV\n",
    "# overview_df.to_csv('rnn_models_comparison.csv', index=False)\n",
    "# print(\"\\nResults saved to 'rnn_models_comparison.csv'\")\n",
    "# print(\"结果已保存到 'rnn_models_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "### 可选：保存模型 / Optional: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model / 取消注释以保存模型\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "torch.save(model.state_dict(), '../models/rnn_model.pth')\n",
    "print(\"Model saved to ../models/rnn_model.pth\")\n",
    "print(\"模型已保存到 ../models/rnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ba67a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
