{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Oil Temperature Prediction using RNN-ResNet Hybrid Model\n",
    "\n",
    "This notebook implements a hybrid deep learning architecture combining **Recurrent Neural Networks (RNN)** and **Residual Networks (ResNet)** for predicting transformer oil temperature.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "**RNN → ResNet Pipeline**:\n",
    "1. **RNN Layer** (LSTM/GRU): Captures temporal dependencies in the time series\n",
    "2. **ResNet Blocks**: Deep feature learning with skip connections\n",
    "3. **Output Layer**: Temperature prediction\n",
    "\n",
    "**Two Variants**:\n",
    "- **LSTM-ResNet**: Better for long-term dependencies\n",
    "- **GRU-ResNet**: Faster training, fewer parameters\n",
    "\n",
    "## Prediction Horizons\n",
    "- **1 hour ahead**: Short-term prediction\n",
    "- **1 day ahead**: Medium-term prediction\n",
    "- **1 week ahead**: Long-term prediction\n",
    "\n",
    "## Comparison with Baselines\n",
    "- Random Forest (R² = 0.60 for 1h)\n",
    "- Pure ResNet\n",
    "- **New**: LSTM-ResNet\n",
    "- **New**: GRU-ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "# Sklearn for metrics and preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Device Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically select GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"\\nGPU Information:\")\n",
    "    print(f\"  Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Memory Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"  Max Memory Allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"\\nRunning on CPU (GPU not available or not configured)\")\n",
    "    print(\"Training will be slower. Consider using CUDA if available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration\n",
    "\n",
    "Load data from the `dataset/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "data_dir = Path('../dataset')\n",
    "trans1_path = data_dir / 'trans_1.csv'\n",
    "trans2_path = data_dir / 'trans_2.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not trans1_path.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {trans1_path}\")\n",
    "if not trans2_path.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {trans2_path}\")\n",
    "\n",
    "print(\"Loading transformer data...\")\n",
    "df1 = pd.read_csv(trans1_path)\n",
    "df2 = pd.read_csv(trans2_path)\n",
    "\n",
    "print(f\"\\nTransformer 1 data shape: {df1.shape}\")\n",
    "print(f\"Transformer 2 data shape: {df2.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nTransformer 1 - First 5 rows:\")\n",
    "display(df1.head())\n",
    "\n",
    "print(\"\\nData columns:\")\n",
    "print(df1.columns.tolist())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in Transformer 1:\")\n",
    "print(df1.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in Transformer 2:\")\n",
    "print(df2.isnull().sum())\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\")\n",
    "print(df1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Data Preprocessing\n",
    "\n",
    "Create sliding window sequences for RNN input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, target_col='OT', feature_cols=None, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction with RNN\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with time series data\n",
    "        seq_length: Length of input sequence (number of time steps to look back)\n",
    "        target_col: Name of target column (oil temperature)\n",
    "        feature_cols: List of feature column names. If None, use all except target\n",
    "        forecast_horizon: Steps ahead to predict (1 for 1h, 96 for 1d, 672 for 1w)\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences (samples, seq_length, features)\n",
    "        y: Target values (samples,)\n",
    "    \"\"\"\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [col for col in data.columns if col != target_col]\n",
    "    \n",
    "    # Extract features and target\n",
    "    features = data[feature_cols].values\n",
    "    target = data[target_col].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data) - seq_length - forecast_horizon + 1):\n",
    "        # Input sequence: seq_length time steps\n",
    "        X.append(features[i:i+seq_length])\n",
    "        # Target: temperature at forecast_horizon steps ahead\n",
    "        y.append(target[i+seq_length+forecast_horizon-1])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def prepare_data_for_config(df, config='1h', seq_length=10, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for a specific forecasting configuration\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        config: '1h', '1d', or '1w'\n",
    "        seq_length: Length of input sequences\n",
    "        test_size: Fraction for test set\n",
    "        val_size: Fraction of training data for validation\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, scaler\n",
    "    \"\"\"\n",
    "    # Define forecast horizons (based on 15-min intervals)\n",
    "    # 1h = 4 steps, 1d = 96 steps, 1w = 672 steps\n",
    "    horizons = {\n",
    "        '1h': 4,\n",
    "        '1d': 96,\n",
    "        '1w': 672\n",
    "    }\n",
    "    \n",
    "    horizon = horizons[config]\n",
    "    \n",
    "    # Feature columns (exclude target)\n",
    "    feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']\n",
    "    \n",
    "    # Create sequences\n",
    "    print(f\"\\nCreating sequences for {config} prediction...\")\n",
    "    print(f\"  Sequence length: {seq_length}\")\n",
    "    print(f\"  Forecast horizon: {horizon} steps ({config})\")\n",
    "    \n",
    "    X, y = create_sequences(\n",
    "        df, \n",
    "        seq_length=seq_length,\n",
    "        target_col='OT',\n",
    "        feature_cols=feature_cols,\n",
    "        forecast_horizon=horizon\n",
    "    )\n",
    "    \n",
    "    print(f\"  Total sequences created: {len(X)}\")\n",
    "    print(f\"  X shape: {X.shape}  # (samples, seq_length, features)\")\n",
    "    print(f\"  y shape: {y.shape}  # (samples,)\")\n",
    "    \n",
    "    # Split into train and test\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "    X_train_full, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train_full, y_test = y[:split_idx], y[split_idx:]\n",
    "    \n",
    "    # Further split train into train and validation\n",
    "    val_idx = int(len(X_train_full) * (1 - val_size))\n",
    "    X_train, X_val = X_train_full[:val_idx], X_train_full[val_idx:]\n",
    "    y_train, y_val = y_train_full[:val_idx], y_train_full[val_idx:]\n",
    "    \n",
    "    # Normalize features (fit on training data only)\n",
    "    # Reshape for scaling: (samples * seq_length, features)\n",
    "    n_samples_train, seq_len, n_features = X_train.shape\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_2d = X_train.reshape(-1, n_features)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_2d).reshape(n_samples_train, seq_len, n_features)\n",
    "    \n",
    "    # Transform validation and test\n",
    "    n_samples_val = X_val.shape[0]\n",
    "    n_samples_test = X_test.shape[0]\n",
    "    \n",
    "    X_val_2d = X_val.reshape(-1, n_features)\n",
    "    X_val_scaled = scaler.transform(X_val_2d).reshape(n_samples_val, seq_len, n_features)\n",
    "    \n",
    "    X_test_2d = X_test.reshape(-1, n_features)\n",
    "    X_test_scaled = scaler.transform(X_test_2d).reshape(n_samples_test, seq_len, n_features)\n",
    "    \n",
    "    print(f\"\\nData splits:\")\n",
    "    print(f\"  Train: {X_train_scaled.shape[0]} samples\")\n",
    "    print(f\"  Validation: {X_val_scaled.shape[0]} samples\")\n",
    "    print(f\"  Test: {X_test_scaled.shape[0]} samples\")\n",
    "    \n",
    "    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test, scaler\n",
    "\n",
    "\n",
    "# Test the preprocessing function\n",
    "print(\"Testing data preprocessing...\")\n",
    "SEQ_LENGTH = 10  # Look back 10 time steps (2.5 hours with 15-min intervals)\n",
    "\n",
    "# Prepare data for all three configurations\n",
    "data_1h = prepare_data_for_config(df1, config='1h', seq_length=SEQ_LENGTH)\n",
    "data_1d = prepare_data_for_config(df1, config='1d', seq_length=SEQ_LENGTH)\n",
    "data_1w = prepare_data_for_config(df1, config='1w', seq_length=SEQ_LENGTH)\n",
    "\n",
    "print(\"\\n✓ Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM-ResNet Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connection\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTM_ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-ResNet hybrid model for time series prediction\n",
    "    \n",
    "    Architecture:\n",
    "    Input (batch, seq_len, features)\n",
    "        ↓\n",
    "    Bidirectional LSTM\n",
    "        ↓\n",
    "    Flatten/Concatenate\n",
    "        ↓\n",
    "    Dense Layer (projection)\n",
    "        ↓\n",
    "    ResNet Blocks (with skip connections)\n",
    "        ↓\n",
    "    Output Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=6, \n",
    "                 seq_length=10,\n",
    "                 rnn_hidden_dim=64, \n",
    "                 rnn_num_layers=2,\n",
    "                 bidirectional=True,\n",
    "                 resnet_hidden_dim=128,\n",
    "                 resnet_num_blocks=2,\n",
    "                 dropout=0.3):\n",
    "        super(LSTM_ResNet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.rnn_hidden_dim = rnn_hidden_dim\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.resnet_hidden_dim = resnet_hidden_dim\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=rnn_hidden_dim,\n",
    "            num_layers=rnn_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate LSTM output dimension\n",
    "        lstm_output_dim = rnn_hidden_dim * 2 if bidirectional else rnn_hidden_dim\n",
    "        \n",
    "        # Projection layer (from LSTM output to ResNet input)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(lstm_output_dim, resnet_hidden_dim),\n",
    "            nn.BatchNorm1d(resnet_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # ResNet blocks\n",
    "        self.resnet_blocks = nn.ModuleList([\n",
    "            ResidualBlock(resnet_hidden_dim, resnet_hidden_dim * 2, dropout)\n",
    "            for _ in range(resnet_num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(resnet_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_length, features)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # lstm_out shape: (batch, seq_length, hidden_dim * num_directions)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        # last_output shape: (batch, hidden_dim * num_directions)\n",
    "        \n",
    "        # Project to ResNet dimension\n",
    "        x = self.projection(last_output)\n",
    "        # x shape: (batch, resnet_hidden_dim)\n",
    "        \n",
    "        # Pass through ResNet blocks\n",
    "        for resnet_block in self.resnet_blocks:\n",
    "            x = resnet_block(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test LSTM-ResNet model\n",
    "print(\"Testing LSTM-ResNet model...\")\n",
    "test_model = LSTM_ResNet(\n",
    "    input_dim=6,\n",
    "    seq_length=10,\n",
    "    rnn_hidden_dim=64,\n",
    "    rnn_num_layers=2,\n",
    "    bidirectional=True,\n",
    "    resnet_hidden_dim=128,\n",
    "    resnet_num_blocks=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(test_model)\n",
    "print(f\"\\nTotal trainable parameters: {test_model.count_parameters():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(32, 10, 6)  # (batch, seq_length, features)\n",
    "test_output = test_model(test_input)\n",
    "print(f\"\\nTest input shape: {test_input.shape}\")\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "print(\"✓ LSTM-ResNet model test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GRU-ResNet Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU-ResNet hybrid model for time series prediction\n",
    "    \n",
    "    Similar to LSTM-ResNet but uses GRU (faster, fewer parameters)\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_dim=6, \n",
    "                 seq_length=10,\n",
    "                 rnn_hidden_dim=64, \n",
    "                 rnn_num_layers=2,\n",
    "                 bidirectional=True,\n",
    "                 resnet_hidden_dim=128,\n",
    "                 resnet_num_blocks=2,\n",
    "                 dropout=0.3):\n",
    "        super(GRU_ResNet, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.rnn_hidden_dim = rnn_hidden_dim\n",
    "        self.rnn_num_layers = rnn_num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.resnet_hidden_dim = resnet_hidden_dim\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=rnn_hidden_dim,\n",
    "            num_layers=rnn_num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Calculate GRU output dimension\n",
    "        gru_output_dim = rnn_hidden_dim * 2 if bidirectional else rnn_hidden_dim\n",
    "        \n",
    "        # Projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(gru_output_dim, resnet_hidden_dim),\n",
    "            nn.BatchNorm1d(resnet_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # ResNet blocks\n",
    "        self.resnet_blocks = nn.ModuleList([\n",
    "            ResidualBlock(resnet_hidden_dim, resnet_hidden_dim * 2, dropout)\n",
    "            for _ in range(resnet_num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(resnet_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_length, features)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # GRU processing\n",
    "        gru_out, h_n = self.gru(x)\n",
    "        # gru_out shape: (batch, seq_length, hidden_dim * num_directions)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        last_output = gru_out[:, -1, :]\n",
    "        \n",
    "        # Project to ResNet dimension\n",
    "        x = self.projection(last_output)\n",
    "        \n",
    "        # Pass through ResNet blocks\n",
    "        for resnet_block in self.resnet_blocks:\n",
    "            x = resnet_block(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test GRU-ResNet model\n",
    "print(\"Testing GRU-ResNet model...\")\n",
    "test_model_gru = GRU_ResNet(\n",
    "    input_dim=6,\n",
    "    seq_length=10,\n",
    "    rnn_hidden_dim=64,\n",
    "    rnn_num_layers=2,\n",
    "    bidirectional=True,\n",
    "    resnet_hidden_dim=128,\n",
    "    resnet_num_blocks=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(test_model_gru)\n",
    "print(f\"\\nTotal trainable parameters: {test_model_gru.count_parameters():,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_output_gru = test_model_gru(test_input)\n",
    "print(f\"\\nTest output shape: {test_output_gru.shape}\")\n",
    "print(\"✓ GRU-ResNet model test passed!\")\n",
    "\n",
    "# Compare parameter counts\n",
    "print(f\"\\n📊 Parameter Comparison:\")\n",
    "print(f\"  LSTM-ResNet: {test_model.count_parameters():,} parameters\")\n",
    "print(f\"  GRU-ResNet: {test_model_gru.count_parameters():,} parameters\")\n",
    "print(f\"  Difference: {test_model.count_parameters() - test_model_gru.count_parameters():,} parameters\")\n",
    "print(f\"  GRU is {(1 - test_model_gru.count_parameters()/test_model.count_parameters())*100:.1f}% smaller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Utilities and Functions\n",
    "\n",
    "Complete training pipeline with early stopping and progress tracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
