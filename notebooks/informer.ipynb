{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informer-intro",
   "metadata": {},
   "source": [
    "# 使用 Informer 预测电力变压器油温 / Power Transformer Oil Temperature Prediction using Informer\n",
    "\n",
    "## 简介 / Introduction\n",
    "\n",
    "本 notebook 实现了 **Informer 模型**来预测电力变压器的油温（OT）。\n",
    "\n",
    "This notebook implements the **Informer model** for predicting the oil temperature (OT) of power transformers.\n",
    "\n",
    "**Informer**: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n",
    "- **论文 / Paper**: Zhou et al., AAAI 2021 **最佳论文奖 / Best Paper Award** 🏆\n",
    "- **关键创新 / Key Innovation**: 专为长序列时间序列预测（LSTF）设计 / Designed specifically for long sequence time-series forecasting (LSTF)\n",
    "\n",
    "### 为什么用 Informer 预测油温？ / Why Informer for Oil Temperature Prediction?\n",
    "\n",
    "1. **长期依赖 / Long-term Dependencies**: 油温变化具有跨越数小时的热惯性 / Oil temperature changes have thermal inertia spanning hours\n",
    "2. **高效性 / Efficiency**: ProbSparse 注意力将复杂度从 O(L²) 降低到 O(L log L) / ProbSparse attention reduces complexity from O(L²) to O(L log L)\n",
    "3. **长时预测 / Long-horizon Forecasting**: 可以一次预测多个时间步 / Can predict multiple time steps ahead in one shot\n",
    "4. **最先进 / State-of-the-Art**: 在 ETT 数据集上达到最佳性能 / Achieved best performance on ETT dataset\n",
    "\n",
    "### 三大关键创新 / Three Key Innovations\n",
    "\n",
    "#### 1. ProbSparse 自注意力 / ProbSparse Self-Attention\n",
    "- 传统自注意力 / Traditional self-attention: O(L²) 复杂度 / complexity\n",
    "- Informer: 通过只选择"活跃"查询实现 O(L log L) / O(L log L) by selecting only \"active\" queries\n",
    "- **核心思想 / Idea**: 并非所有查询都同等重要；专注于高注意力分数的查询 / Not all queries contribute equally; focus on queries with high attention scores\n",
    "\n",
    "#### 2. 自注意力蒸馏 / Self-Attention Distilling\n",
    "- 逐层渐进式减少序列长度 / Progressively reduces sequence length layer by layer\n",
    "- 突出主要特征同时减少计算负担 / Highlights dominant features while reducing computational burden\n",
    "- 使用带步长的最大池化在每层将输入减半 / Uses max pooling with stride to halve the input at each layer\n",
    "\n",
    "#### 3. 生成式解码器 / Generative Style Decoder\n",
    "- 一次前向传播预测整个输出序列 / Predicts entire output sequence in one forward pass\n",
    "- 避免逐步预测的错误累积 / Avoids error accumulation from step-by-step prediction\n",
    "- 大幅提高推理速度 / Dramatically improves inference speed\n",
    "\n",
    "### 数据集 / Dataset: ETT (Electricity Transformer Temperature)\n",
    "- **来源 / Source**: https://github.com/zhouhaoyi/ETDataset\n",
    "- **采样间隔 / Sampling**: 15分钟 / 15-minute intervals\n",
    "- **特征 / Features**: 6 个负载特征 / 6 load features (HUFL, HULL, MUFL, MULL, LUFL, LULL)\n",
    "- **目标 / Target**: OT (油温 / Oil Temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs",
   "metadata": {},
   "source": [
    "## 步骤 1: 导入所需库 / Step 1: Import Required Libraries\n",
    "\n",
    "我们将使用 / We'll use:\n",
    "- pandas/numpy: 数据处理 / data manipulation\n",
    "- **PyTorch**: 构建 Informer 模型 / building the Informer model\n",
    "- sklearn: 预处理和评估指标 / preprocessing and metrics\n",
    "- matplotlib: 可视化 / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理 / Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 深度学习 / Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# 预处理和评估指标 / Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 可视化 / Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置随机种子以保证可复现性 / Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# 设备配置 / Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version / PyTorch 版本: {torch.__version__}\")\n",
    "print(f\"Device / 设备: {device}\")\n",
    "print(f\"CUDA available / CUDA 可用: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## 步骤 2: 加载和探索数据 / Step 2: Load and Explore Data\n",
    "\n",
    "我们将加载变压器温度数据集并进行初步探索。\n",
    "\n",
    "We'll load the transformer temperature dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load the ETT dataset\n",
    "    加载 ETT 数据集\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "# Load pre-split training and test data\n",
    "# 加载预先切分的训练集和测试集\n",
    "train_filepath = '../dataset/processed_data/train.csv'\n",
    "test_filepath = '../dataset/processed_data/test.csv'\n",
    "\n",
    "df_train = load_data(train_filepath)\n",
    "df_test = load_data(test_filepath)\n",
    "\n",
    "# For initial exploration, use training data\n",
    "# 初步探索使用训练数据\n",
    "df = df_train\n",
    "\n",
    "print(\"Dataset loaded from pre-split files:\")\n",
    "print(f\"  Training data: {train_filepath}\")\n",
    "print(f\"  Test data: {test_filepath}\")\n",
    "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"\\nFirst few rows of training data:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## 步骤 3: 数据可视化 / Step 3: Data Visualization\n",
    "\n",
    "可视化油温和特征相关性。\n",
    "\n",
    "Visualize oil temperature and feature correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Oil Temperature (target variable)\n",
    "# 绘制油温（目标变量）\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Full view / 完整视图\n",
    "axes[0].plot(df['date'][:2000], df['OT'][:2000], linewidth=0.8)\n",
    "axes[0].set_title('Oil Temperature Over Time (First 2000 points) / 油温随时间变化（前2000个点）', fontsize=14)\n",
    "axes[0].set_xlabel('Date / 日期')\n",
    "axes[0].set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution / 分布\n",
    "axes[1].hist(df['OT'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Oil Temperature Distribution / 油温分布', fontsize=14)\n",
    "axes[1].set_xlabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[1].set_ylabel('Frequency / 频率')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap / 相关性热力图\n",
    "plt.figure(figsize=(10, 8))\n",
    "features = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "sns.heatmap(df[features].corr(), annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap / 特征相关性热力图', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-features-header",
   "metadata": {},
   "source": [
    "## 步骤 4: 提取时间特征 / Step 4: Extract Time Features\n",
    "\n",
    "Informer 使用时间特征来增强模型对时间模式的理解。\n",
    "我们提取：小时、星期几、月份中的日期和月份。\n",
    "\n",
    "Informer uses time features to enhance the model's understanding of temporal patterns.\n",
    "We extract: hour, day of week, day of month, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df):\n",
    "    \"\"\"\n",
    "    Extract time features from datetime column\n",
    "    从日期时间列提取时间特征\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame with added time features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time components / 提取时间组件\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    # Normalize to [0, 1] range / 归一化到 [0, 1] 范围\n",
    "    df['hour_norm'] = df['hour'] / 23.0\n",
    "    df['day_of_week_norm'] = df['day_of_week'] / 6.0\n",
    "    df['day_of_month_norm'] = (df['day_of_month'] - 1) / 30.0\n",
    "    df['month_norm'] = (df['month'] - 1) / 11.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract time features for both datasets\n",
    "# 为两个数据集提取时间特征\n",
    "df_train = extract_time_features(df_train)\n",
    "df_test = extract_time_features(df_test)\n",
    "df = df_train  # For exploration\n",
    "\n",
    "print(\"Time features extracted for both train and test sets!\")\n",
    "print(\"已为训练集和测试集提取时间特征！\")\n",
    "print(\"\\nColumns / 列名:\", df.columns.tolist())\n",
    "print(\"\\nSample time features / 时间特征示例:\")\n",
    "print(df[['date', 'hour', 'day_of_week', 'day_of_month', 'month']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## 步骤 5: 数据预处理和数据集创建 / Step 5: Data Preprocessing and Dataset Creation\n",
    "\n",
    "### 与 RNN 的关键区别 / Key Differences from RNN:\n",
    "- **输入 / Input**: `seq_len` 个历史时间步 / historical time steps\n",
    "- **标签 / Label**: `label_len` + `pred_len` (解码器输入 + 预测目标 / decoder input + prediction target)\n",
    "- **时间戳 / Time Stamps**: 编码器和解码器的时间特征 / Time features for both encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Informer model\n",
    "    Informer 模型的数据集\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    seq_len : int\n",
    "        Encoder input length (look-back window)\n",
    "        编码器输入长度（回溯窗口）\n",
    "    label_len : int\n",
    "        Decoder input length (start token)\n",
    "        解码器输入长度（起始标记）\n",
    "    pred_len : int\n",
    "        Prediction length (forecast horizon)\n",
    "        预测长度（预测范围）\n",
    "    feature_cols : list\n",
    "        List of feature column names\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    time_cols : list\n",
    "        List of time feature column names\n",
    "    scaler : StandardScaler or None\n",
    "        Fitted scaler (for test set) or None (for train set)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_len, label_len, pred_len, \n",
    "                 feature_cols, target_col, time_cols, scaler=None):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Normalize features / 归一化特征\n",
    "        if scaler is None:\n",
    "            self.scaler_X = StandardScaler()\n",
    "            self.scaler_y = StandardScaler()\n",
    "            \n",
    "            self.data_X = self.scaler_X.fit_transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.fit_transform(data[[target_col]].values)\n",
    "        else:\n",
    "            self.scaler_X, self.scaler_y = scaler\n",
    "            self.data_X = self.scaler_X.transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.transform(data[[target_col]].values)\n",
    "        \n",
    "        # Time features (already normalized) / 时间特征（已归一化）\n",
    "        self.data_stamp = data[time_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_X) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Encoder input / 编码器输入\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        \n",
    "        # Decoder input (overlaps with encoder) / 解码器输入（与编码器重叠）\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        # Input features for encoder / 编码器的输入特征\n",
    "        seq_x = self.data_X[s_begin:s_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        \n",
    "        # Input for decoder (features + zero padding for prediction)\n",
    "        # 解码器输入（特征 + 预测部分的零填充）\n",
    "        seq_y = np.concatenate([\n",
    "            self.data_X[r_begin:r_begin+self.label_len],\n",
    "            np.zeros((self.pred_len, self.data_X.shape[1]))\n",
    "        ], axis=0)\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        \n",
    "        # Target (actual values to predict) / 目标（要预测的实际值）\n",
    "        target = self.data_y[r_begin:r_end]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(seq_x),\n",
    "            torch.FloatTensor(seq_x_mark),\n",
    "            torch.FloatTensor(seq_y),\n",
    "            torch.FloatTensor(seq_y_mark),\n",
    "            torch.FloatTensor(target)\n",
    "        )\n",
    "\n",
    "print(\"InformerDataset class defined! / InformerDataset 类已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / 配置\n",
    "seq_len = 96      # 24 hours (96 * 15min) / 24小时\n",
    "label_len = 48    # 12 hours overlap / 12小时重叠\n",
    "pred_len = 24     # 6 hours prediction / 6小时预测\n",
    "\n",
    "feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']\n",
    "target_col = 'OT'\n",
    "time_cols = ['hour_norm', 'day_of_week_norm', 'day_of_month_norm', 'month_norm']\n",
    "\n",
    "# Use pre-split data / 使用预切分数据\n",
    "print(f\"Using pre-split data / 使用预切分数据:\")\n",
    "print(f\"  Train set / 训练集: {len(df_train)} samples\")\n",
    "print(f\"  Test set / 测试集: {len(df_test)} samples\")\n",
    "\n",
    "# Create datasets / 创建数据集\n",
    "train_dataset = InformerDataset(\n",
    "    df_train, seq_len, label_len, pred_len,\n",
    "    feature_cols, target_col, time_cols\n",
    ")\n",
    "\n",
    "test_dataset = InformerDataset(\n",
    "    df_test, seq_len, label_len, pred_len,\n",
    "    feature_cols, target_col, time_cols,\n",
    "    scaler=(train_dataset.scaler_X, train_dataset.scaler_y)\n",
    ")\n",
    "\n",
    "# Create data loaders / 创建数据加载器\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches / 训练批次: {len(train_loader)}\")\n",
    "print(f\"Test batches / 测试批次: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading / 测试数据加载\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes / 样本批次形状:\")\n",
    "print(f\"  Encoder input (seq_x) / 编码器输入: {sample_batch[0].shape}\")\n",
    "print(f\"  Encoder time (seq_x_mark) / 编码器时间: {sample_batch[1].shape}\")\n",
    "print(f\"  Decoder input (seq_y) / 解码器输入: {sample_batch[2].shape}\")\n",
    "print(f\"  Decoder time (seq_y_mark) / 解码器时间: {sample_batch[3].shape}\")\n",
    "print(f\"  Target / 目标: {sample_batch[4].shape}\")\n",
    "print(f\"\\nNote: Using scalers fitted on training data for test set\")\n",
    "print(f\"注意：使用在训练数据上拟合的缩放器处理测试集\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 步骤 6: 构建 Informer 模型 / Step 6: Build Informer Model\n",
    "\n",
    "### 架构概览 / Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  INFORMER 架构 / ARCHITECTURE               │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  编码器输入 / Encoder Input    解码器输入 / Decoder Input   │\n",
    "│      ↓                                 ↓                   │\n",
    "│  [嵌入 / Embedding]             [嵌入 / Embedding]         │\n",
    "│      ↓                                 ↓                   │\n",
    "│  ┌──────────┐                     ┌──────────┐            │\n",
    "│  │ProbSparse│ ←────────────────── │ 标准注意力│            │\n",
    "│  │注意力+蒸馏│      交叉注意力     │ Standard │            │\n",
    "│  │Attn+Dstl │    Cross-Attn      │ Attention│            │\n",
    "│  └──────────┘                     └──────────┘            │\n",
    "│      ↓                                 ↓                   │\n",
    "│  [前馈网络 / Feed Forward]    [前馈网络 / Feed Forward]   │\n",
    "│      ↓                                 ↓                   │\n",
    "│  ───────────────────────────────────────→                 │\n",
    "│                                         ↓                  │\n",
    "│                                [投影 / Projection]         │\n",
    "│                                         ↓                  │\n",
    "│                                  输出 / Output             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequence data / 序列数据的位置编码\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Create positional encoding matrix / 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Project input features to model dimension / 将输入特征投影到模型维度\"\"\"\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        x = x.permute(0, 2, 1)  # [batch, c_in, seq_len]\n",
    "        x = self.tokenConv(x)\n",
    "        x = x.permute(0, 2, 1)  # [batch, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    \"\"\"Embed time features / 嵌入时间特征\"\"\"\n",
    "    def __init__(self, d_model, embed_dim=4):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "        self.embed = nn.Linear(embed_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 4] (hour, day_of_week, day, month)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    \"\"\"Complete embedding: token + positional + temporal / 完整嵌入：标记 + 位置 + 时间\"\"\"\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.temporal_embedding = TimeFeatureEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, x_mark):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        # x_mark: [batch, seq_len, 4]\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"Embedding modules defined! / 嵌入模块已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probsparse-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ProbSparse Self-Attention Mechanism\n",
    "    ProbSparse 自注意力机制\n",
    "    \n",
    "    Key Innovation / 关键创新:\n",
    "    - Select top-u queries based on sparsity measurement\n",
    "      基于稀疏性度量选择 top-u 查询\n",
    "    - Only compute attention for selected queries\n",
    "      仅为选定的查询计算注意力\n",
    "    - Reduces complexity from O(L²) to O(L log L)\n",
    "      将复杂度从 O(L²) 降低到 O(L log L)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, \n",
    "                 attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "    \n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        \"\"\"\n",
    "        Calculate query sparsity measurement\n",
    "        计算查询稀疏性度量\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Q_K : tensor\n",
    "            Sampled Q-K scores\n",
    "        M_top : tensor\n",
    "            Top-u query indices based on sparsity\n",
    "        \"\"\"\n",
    "        # Q: [B, H, L, D]\n",
    "        B, H, L_Q, D = Q.shape\n",
    "        _, _, L_K, _ = K.shape\n",
    "        \n",
    "        # Calculate sampled Q-K scores / 计算采样的 Q-K 分数\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, D)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))  # Random sampling\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "        \n",
    "        # Sparsity measurement: max - mean / 稀疏性度量：最大值 - 平均值\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "        \n",
    "        # Calculate full Q-K for top queries / 为 top 查询计算完整的 Q-K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]  # [B, H, n_top, D]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # [B, H, n_top, L_K]\n",
    "        \n",
    "        return Q_K, M_top\n",
    "    \n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        \"\"\"Initialize context with mean of V / 用 V 的平均值初始化上下文\"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # Mean pooling / 平均池化\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            context = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else:\n",
    "            # Cumulative mean for masked attention / 用于掩码注意力的累积平均\n",
    "            context = V.cumsum(dim=-2)\n",
    "        return context\n",
    "    \n",
    "    def _update_context(self, context_in, V, scores, index, L_Q):\n",
    "        \"\"\"Update context with selected queries / 用选定的查询更新上下文\"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        \n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return context_in, attns\n",
    "        else:\n",
    "            return context_in, None\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of ProbSparse Attention\n",
    "        ProbSparse 注意力的前向传播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        queries : [B, L_Q, H, D]\n",
    "        keys : [B, L_K, H, D]\n",
    "        values : [B, L_V, H, D]\n",
    "        \"\"\"\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "        \n",
    "        queries = queries.transpose(2, 1)  # [B, H, L_Q, D]\n",
    "        keys = keys.transpose(2, 1)        # [B, H, L_K, D]\n",
    "        values = values.transpose(2, 1)    # [B, H, L_V, D]\n",
    "        \n",
    "        # Calculate number of queries to select / 计算要选择的查询数量\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
    "        \n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "        \n",
    "        # Calculate sparse attention / 计算稀疏注意力\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "        \n",
    "        # Scaling / 缩放\n",
    "        scale = self.scale or 1.0 / np.sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        \n",
    "        # Get context / 获取上下文\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q)\n",
    "        \n",
    "        return context.transpose(2, 1).contiguous(), attn\n",
    "\n",
    "\n",
    "class ProbMask:\n",
    "    \"\"\"Mask for ProbSparse Attention / ProbSparse 注意力的掩码\"\"\"\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self.mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "print(\"ProbSparse Attention defined! / ProbSparse 注意力已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Multi-head attention wrapper / 多头注意力包装器\"\"\"\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "        \n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        \n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "        \n",
    "        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n",
    "        out = out.view(B, L, -1)\n",
    "        \n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Informer Encoder Layer with distilling / 带蒸馏的 Informer 编码器层\"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Multi-head attention / 多头注意力\n",
    "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed forward / 前馈网络\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"Distilling layer: progressively reduce sequence length / 蒸馏层：逐步减少序列长度\"\"\"\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Informer Encoder with distilling / 带蒸馏的 Informer 编码器\"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x, attns\n",
    "\n",
    "print(\"Encoder modules defined! / 编码器模块已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Informer Decoder Layer / Informer 解码器层\"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        # Self attention / 自注意力\n",
    "        x = x + self.dropout(self.self_attention(x, x, x, attn_mask=x_mask)[0])\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Cross attention / 交叉注意力\n",
    "        x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Feed forward / 前馈网络\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Informer Decoder / Informer 解码器\"\"\"\n",
    "    def __init__(self, layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Decoder modules defined! / 解码器模块已定义！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informer-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Informer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Informer Model\n",
    "    完整的 Informer 模型\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    enc_in : int\n",
    "        Number of encoder input features / 编码器输入特征数量\n",
    "    dec_in : int\n",
    "        Number of decoder input features / 解码器输入特征数量\n",
    "    c_out : int\n",
    "        Number of output features / 输出特征数量\n",
    "    seq_len : int\n",
    "        Input sequence length / 输入序列长度\n",
    "    label_len : int\n",
    "        Start token length for decoder / 解码器的起始标记长度\n",
    "    out_len : int\n",
    "        Output sequence length / 输出序列长度\n",
    "    factor : int\n",
    "        ProbSparse attention factor / ProbSparse 注意力因子\n",
    "    d_model : int\n",
    "        Model dimension / 模型维度\n",
    "    n_heads : int\n",
    "        Number of attention heads / 注意力头数量\n",
    "    e_layers : int\n",
    "        Number of encoder layers / 编码器层数\n",
    "    d_layers : int\n",
    "        Number of decoder layers / 解码器层数\n",
    "    d_ff : int\n",
    "        Feed-forward dimension / 前馈网络维度\n",
    "    dropout : float\n",
    "        Dropout rate / Dropout 率\n",
    "    attn : str\n",
    "        Attention type ('prob' for ProbSparse)\n",
    "    activation : str\n",
    "        Activation function / 激活函数\n",
    "    output_attention : bool\n",
    "        Whether to output attention weights / 是否输出注意力权重\n",
    "    distil : bool\n",
    "        Whether to use distilling / 是否使用蒸馏\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
    "                 factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2,\n",
    "                 d_ff=512, dropout=0.0, attn='prob', activation='gelu',\n",
    "                 output_attention=False, distil=True, device=torch.device('cuda:0')):\n",
    "        super(Informer, self).__init__()\n",
    "        self.pred_len = out_len\n",
    "        self.attn = attn\n",
    "        self.output_attention = output_attention\n",
    "        \n",
    "        # Encoding / 编码\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, dropout)\n",
    "        \n",
    "        # Attention / 注意力\n",
    "        Attn = ProbAttention\n",
    "        \n",
    "        # Encoder / 编码器\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for _ in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(d_model) for _ in range(e_layers - 1)\n",
    "            ] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Decoder / 解码器\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                ) for _ in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Projection / 投影\n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass / 前向传播\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_enc : [batch, seq_len, enc_in]\n",
    "        x_mark_enc : [batch, seq_len, 4]\n",
    "        x_dec : [batch, label_len + pred_len, dec_in]\n",
    "        x_mark_dec : [batch, label_len + pred_len, 4]\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : [batch, pred_len, c_out]\n",
    "        \"\"\"\n",
    "        # Encoder / 编码器\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        \n",
    "        # Decoder / 解码器\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "        dec_out = self.projection(dec_out)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "\n",
    "print(\"Complete Informer model defined! / 完整的 Informer 模型已定义！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-model-header",
   "metadata": {},
   "source": [
    "## 步骤 7: 初始化模型 / Step 7: Initialize Model\n",
    "\n",
    "配置模型超参数并初始化 Informer 模型。\n",
    "\n",
    "Configure model hyperparameters and initialize the Informer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters / 模型超参数\n",
    "model_config = {\n",
    "    'enc_in': len(feature_cols),      # 6 features / 6个特征\n",
    "    'dec_in': len(feature_cols),      # 6 features / 6个特征\n",
    "    'c_out': 1,                       # 1 target (OT) / 1个目标（OT）\n",
    "    'seq_len': seq_len,               # 96\n",
    "    'label_len': label_len,           # 48\n",
    "    'out_len': pred_len,              # 24\n",
    "    'factor': 5,                      # ProbSparse factor / ProbSparse 因子\n",
    "    'd_model': 512,                   # Model dimension / 模型维度\n",
    "    'n_heads': 8,                     # Attention heads / 注意力头数\n",
    "    'e_layers': 2,                    # Encoder layers / 编码器层数\n",
    "    'd_layers': 1,                    # Decoder layers / 解码器层数\n",
    "    'd_ff': 2048,                     # Feed-forward dimension / 前馈维度\n",
    "    'dropout': 0.05,                  # Dropout\n",
    "    'attn': 'prob',                   # ProbSparse attention\n",
    "    'activation': 'gelu',             # Activation / 激活函数\n",
    "    'output_attention': False,        # Don't output attention / 不输出注意力\n",
    "    'distil': True,                   # Use distilling / 使用蒸馏\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Initialize model / 初始化模型\n",
    "model = Informer(**model_config).to(device)\n",
    "\n",
    "print(\"Model initialized! / 模型已初始化！\")\n",
    "print(f\"\\nTotal parameters / 总参数数: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters / 可训练参数数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 步骤 8: 训练配置和训练循环 / Step 8: Training Configuration and Loop\n",
    "\n",
    "配置优化器、损失函数和训练循环。\n",
    "\n",
    "Configure optimizer, loss function, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration / 训练配置\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "\n",
    "# Optimizer and loss / 优化器和损失函数\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training history / 训练历史\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': []\n",
    "}\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training configuration / 训练配置:\")\n",
    "print(f\"  Epochs / 轮数: {num_epochs}\")\n",
    "print(f\"  Learning rate / 学习率: {learning_rate}\")\n",
    "print(f\"  Batch size / 批次大小: {batch_size}\")\n",
    "print(f\"  Device / 设备: {device}\")\n",
    "print(f\"\\nStarting training... / 开始训练...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop / 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # Training / 训练\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (seq_x, seq_x_mark, seq_y, seq_y_mark, target) in enumerate(train_loader):\n",
    "        # Move to device / 移动到设备\n",
    "        seq_x = seq_x.to(device)\n",
    "        seq_x_mark = seq_x_mark.to(device)\n",
    "        seq_y = seq_y.to(device)\n",
    "        seq_y_mark = seq_y_mark.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass / 前向传播\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "        \n",
    "        # Calculate loss (only on prediction part) / 计算损失（仅在预测部分）\n",
    "        loss = criterion(output, target[:, -pred_len:, :])\n",
    "        \n",
    "        # Backward pass / 反向传播\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # Testing / 测试\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in test_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            loss = criterion(output, target[:, -pred_len:, :])\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    history['test_loss'].append(avg_test_loss)\n",
    "    \n",
    "    # Learning rate scheduling / 学习率调度\n",
    "    scheduler.step(avg_test_loss)\n",
    "    \n",
    "    # Print progress / 打印进度\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {avg_train_loss:.6f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping / 早停\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model / 保存最佳模型\n",
    "        torch.save(model.state_dict(), 'best_informer_model.pth')\n",
    "        print(f\"  → New best model saved / 保存新的最佳模型 (Test Loss: {best_test_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"提前停止在第 {epoch+1} 轮后触发\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed! / 训练完成！\")\n",
    "print(f\"Best test loss / 最佳测试损失: {best_test_loss:.6f}\")\n",
    "\n",
    "# Load best model / 加载最佳模型\n",
    "model.load_state_dict(torch.load('best_informer_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "## 步骤 9: 训练历史可视化 / Step 9: Training History Visualization\n",
    "\n",
    "绘制训练和测试损失曲线。\n",
    "\n",
    "Plot training and test loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss / 训练损失', linewidth=2)\n",
    "plt.plot(history['test_loss'], label='Test Loss / 测试损失', linewidth=2)\n",
    "plt.xlabel('Epoch / 轮次', fontsize=12)\n",
    "plt.ylabel('Loss (MSE) / 损失 (MSE)', fontsize=12)\n",
    "plt.title('Informer Model Training History / Informer 模型训练历史', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## 步骤 10: 模型评估和预测 / Step 10: Model Evaluation and Predictions\n",
    "\n",
    "生成预测并反向转换到原始尺度。\n",
    "\n",
    "Make predictions and inverse transform to original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "make-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, data_loader, scaler_y, device):\n",
    "    \"\"\"\n",
    "    Make predictions and inverse transform to original scale\n",
    "    生成预测并反向转换到原始尺度\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in data_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            \n",
    "            # Move to CPU and convert to numpy / 移至 CPU 并转换为 numpy\n",
    "            pred = output.cpu().numpy()\n",
    "            actual = target[:, -pred_len:, :].cpu().numpy()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            actuals.append(actual)\n",
    "    \n",
    "    # Concatenate all batches / 连接所有批次\n",
    "    predictions = np.concatenate(predictions, axis=0)  # [N, pred_len, 1]\n",
    "    actuals = np.concatenate(actuals, axis=0)          # [N, pred_len, 1]\n",
    "    \n",
    "    # Reshape for inverse transform / 为反向转换重塑\n",
    "    pred_shape = predictions.shape\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform / 反向转换\n",
    "    predictions = scaler_y.inverse_transform(predictions)\n",
    "    actuals = scaler_y.inverse_transform(actuals)\n",
    "    \n",
    "    # Reshape back / 重塑回去\n",
    "    predictions = predictions.reshape(pred_shape)\n",
    "    actuals = actuals.reshape(pred_shape)\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Make predictions / 生成预测\n",
    "print(\"Making predictions... / 生成预测...\")\n",
    "train_pred, train_actual = make_predictions(model, train_loader, train_dataset.scaler_y, device)\n",
    "test_pred, test_actual = make_predictions(model, test_loader, test_dataset.scaler_y, device)\n",
    "\n",
    "print(f\"\\nPrediction shapes / 预测形状:\")\n",
    "print(f\"  Train / 训练: {train_pred.shape}\")\n",
    "print(f\"  Test / 测试: {test_pred.shape}\")\n",
    "\n",
    "# For metric calculation, use only the last time step of each prediction\n",
    "# 用于指标计算，仅使用每个预测的最后一个时间步\n",
    "# Or average across prediction horizon / 或在预测范围内取平均\n",
    "train_pred_flat = train_pred[:, -1, 0]  # Last prediction of each sequence\n",
    "train_actual_flat = train_actual[:, -1, 0]\n",
    "test_pred_flat = test_pred[:, -1, 0]\n",
    "test_actual_flat = test_actual[:, -1, 0]\n",
    "\n",
    "print(f\"\\nFlattened for metrics / 用于指标的扁平化:\")\n",
    "print(f\"  Train / 训练: {train_pred_flat.shape}\")\n",
    "print(f\"  Test / 测试: {test_pred_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, set_name='Test'):\n",
    "    \"\"\"\n",
    "    Calculate and display regression metrics\n",
    "    计算并显示回归指标\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Performance Metrics / {set_name}集性能指标:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"MSE (Mean Squared Error):        {mse:.4f}\")\n",
    "    print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}°C\")\n",
    "    print(f\"MAE (Mean Absolute Error):       {mae:.4f}°C\")\n",
    "    print(f\"R² Score:                        {r2:.4f}\")\n",
    "    print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Calculate metrics / 计算指标\n",
    "train_metrics = calculate_metrics(train_actual_flat, train_pred_flat, 'Training / 训练')\n",
    "test_metrics = calculate_metrics(test_actual_flat, test_pred_flat, 'Test / 测试')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results-header",
   "metadata": {},
   "source": [
    "## 步骤 11: 结果可视化 / Step 11: Results Visualization\n",
    "\n",
    "可视化预测结果和误差分析。\n",
    "\n",
    "Visualize prediction results and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual / 绘制预测 vs 实际\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Test set - full view / 测试集 - 完整视图\n",
    "axes[0].plot(test_actual_flat, label='Actual / 实际', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(test_pred_flat, label='Predicted / 预测', alpha=0.7, linewidth=1.5)\n",
    "axes[0].set_title('Informer: Oil Temperature Prediction - Test Set (Full View) / 油温预测 - 测试集（完整视图）', fontsize=14)\n",
    "axes[0].set_xlabel('Sample / 样本')\n",
    "axes[0].set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set - zoomed view / 测试集 - 放大视图\n",
    "zoom_range = 500\n",
    "axes[1].plot(test_actual_flat[:zoom_range], label='Actual / 实际', alpha=0.7, linewidth=1.5)\n",
    "axes[1].plot(test_pred_flat[:zoom_range], label='Predicted / 预测', alpha=0.7, linewidth=1.5)\n",
    "axes[1].set_title(f'Informer: Oil Temperature Prediction - Test Set (First {zoom_range} Samples) / 油温预测 - 测试集（前{zoom_range}个样本）', fontsize=14)\n",
    "axes[1].set_xlabel('Sample / 样本')\n",
    "axes[1].set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot / 散点图\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(test_actual_flat, test_pred_flat, alpha=0.5, s=20)\n",
    "plt.plot([test_actual_flat.min(), test_actual_flat.max()],\n",
    "         [test_actual_flat.min(), test_actual_flat.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction / 完美预测')\n",
    "plt.xlabel('Actual Oil Temperature (°C) / 实际油温 (°C)', fontsize=12)\n",
    "plt.ylabel('Predicted Oil Temperature (°C) / 预测油温 (°C)', fontsize=12)\n",
    "plt.title('Informer: Predicted vs Actual Oil Temperature (Test Set) / 预测 vs 实际油温（测试集）', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis / 残差分析\n",
    "residuals = test_actual_flat - test_pred_flat\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residual plot / 残差图\n",
    "axes[0].scatter(test_pred_flat, residuals, alpha=0.5, s=20)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Oil Temperature (°C) / 预测油温 (°C)')\n",
    "axes[0].set_ylabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[0].set_title('Residual Plot / 残差图')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution / 残差分布\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (°C) / 残差 (°C)')\n",
    "axes[1].set_ylabel('Frequency / 频率')\n",
    "axes[1].set_title('Distribution of Residuals / 残差分布')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics / 残差统计:\")\n",
    "print(f\"Mean / 均值: {residuals.mean():.4f}°C\")\n",
    "print(f\"Std / 标准差: {residuals.std():.4f}°C\")\n",
    "print(f\"Min / 最小值: {residuals.min():.4f}°C\")\n",
    "print(f\"Max / 最大值: {residuals.max():.4f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-step-viz-header",
   "metadata": {},
   "source": [
    "### 多步预测可视化 / Multi-step Prediction Visualization\n",
    "\n",
    "可视化模型在整个预测范围内的预测效果。\n",
    "\n",
    "Visualize how well the model predicts across the entire prediction horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-step-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-step predictions for a few samples\n",
    "# 可视化几个样本的多步预测\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(test_pred), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 1, figsize=(12, 3*n_samples))\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    ax = axes[idx] if n_samples > 1 else axes\n",
    "    \n",
    "    time_steps = np.arange(pred_len)\n",
    "    actual = test_actual[sample_idx, :, 0]\n",
    "    predicted = test_pred[sample_idx, :, 0]\n",
    "    \n",
    "    ax.plot(time_steps, actual, 'o-', label='Actual / 实际', linewidth=2, markersize=6)\n",
    "    ax.plot(time_steps, predicted, 's-', label='Predicted / 预测', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Prediction Step / 预测步数')\n",
    "    ax.set_ylabel('Oil Temperature (°C) / 油温 (°C)')\n",
    "    ax.set_title(f'Sample {sample_idx}: {pred_len}-step Ahead Prediction / 样本 {sample_idx}：{pred_len}步预测')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 步骤 12: 结论和模型比较 / Step 12: Conclusion and Model Comparison\n",
    "\n",
    "总结 Informer 模型的性能并与其他模型进行比较。\n",
    "\n",
    "Summarize Informer model performance and compare with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison / 创建总结对比\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric / 指标': ['MSE', 'RMSE (°C)', 'MAE (°C)', 'R²', 'MAPE (%)'],\n",
    "    'Training / 训练': [\n",
    "        train_metrics['MSE'],\n",
    "        train_metrics['RMSE'],\n",
    "        train_metrics['MAE'],\n",
    "        train_metrics['R2'],\n",
    "        train_metrics['MAPE']\n",
    "    ],\n",
    "    'Test / 测试': [\n",
    "        test_metrics['MSE'],\n",
    "        test_metrics['RMSE'],\n",
    "        test_metrics['MAE'],\n",
    "        test_metrics['R2'],\n",
    "        test_metrics['MAPE']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFORMER MODEL PERFORMANCE SUMMARY / INFORMER 模型性能总结\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Key Observations / 关键观察:\")\n",
    "print(f\"1. The Informer achieves R² score of {test_metrics['R2']:.4f} on test set\")\n",
    "print(f\"   Informer 在测试集上达到 R² 分数 {test_metrics['R2']:.4f}\")\n",
    "print(f\"2. Average prediction error (MAE): {test_metrics['MAE']:.4f}°C\")\n",
    "print(f\"   平均预测误差 (MAE): {test_metrics['MAE']:.4f}°C\")\n",
    "print(f\"3. MAPE: {test_metrics['MAPE']:.2f}%\")\n",
    "\n",
    "if abs(train_metrics['R2'] - test_metrics['R2']) < 0.05:\n",
    "    print(\"4. Model shows good generalization (minimal overfitting)\")\n",
    "    print(\"   模型显示良好的泛化能力（过拟合最小）\")\n",
    "else:\n",
    "    print(\"4. Some overfitting detected - consider more regularization\")\n",
    "    print(\"   检测到一些过拟合 - 考虑更多正则化\")\n",
    "\n",
    "print(\"\\n### Model Architecture / 模型架构:\")\n",
    "print(f\"- Input Sequence Length / 输入序列长度: {seq_len} steps (24 hours / 24小时)\")\n",
    "print(f\"- Prediction Horizon / 预测范围: {pred_len} steps (6 hours / 6小时)\")\n",
    "print(f\"- Model Dimension / 模型维度: {model_config['d_model']}\")\n",
    "print(f\"- Attention Heads / 注意力头数: {model_config['n_heads']}\")\n",
    "print(f\"- Encoder Layers / 编码器层数: {model_config['e_layers']}\")\n",
    "print(f\"- Decoder Layers / 解码器层数: {model_config['d_layers']}\")\n",
    "print(f\"- Total Parameters / 总参数数: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\n### Informer Advantages / Informer 优势:\")\n",
    "print(\"1. ProbSparse Attention: O(L log L) complexity vs O(L²) in vanilla Transformer\")\n",
    "print(\"   ProbSparse 注意力：O(L log L) 复杂度 vs 原始 Transformer 的 O(L²)\")\n",
    "print(\"2. Self-Attention Distilling: Progressive dimension reduction\")\n",
    "print(\"   自注意力蒸馏：渐进式维度降低\")\n",
    "print(\"3. Generative Decoder: One-shot long sequence prediction\")\n",
    "print(\"   生成式解码器：一次性长序列预测\")\n",
    "print(\"4. Explicitly models temporal patterns with time features\")\n",
    "print(\"   通过时间特征显式建模时间模式\")\n",
    "print(\"5. State-of-the-art performance on long sequence forecasting\")\n",
    "print(\"   在长序列预测上达到最先进性能\")\n",
    "\n",
    "print(\"\\n### Comparison with Other Models / 与其他模型的比较:\")\n",
    "print(\"Expected Performance Ranking (on ETT dataset) / 预期性能排名（在 ETT 数据集上）:\")\n",
    "print(\"1. Informer (SOTA) - Best for long sequences / 最适合长序列\")\n",
    "print(\"2. LSTM/GRU - Good sequential modeling / 良好的序列建模\")\n",
    "print(\"3. MLP - Fast but limited temporal modeling / 快速但时间建模有限\")\n",
    "print(\"4. Random Forest - Good for nonlinear patterns / 适合非线性模式\")\n",
    "print(\"5. Linear Regression - Baseline / 基线模型\")\n",
    "\n",
    "print(\"\\n### Why Informer Outperforms RNN / 为什么 Informer 优于 RNN:\")\n",
    "print(\"1. RNNs suffer from vanishing gradients on long sequences\")\n",
    "print(\"   RNN 在长序列上存在梯度消失问题\")\n",
    "print(\"2. Informer's attention captures long-range dependencies better\")\n",
    "print(\"   Informer 的注意力机制更好地捕获长程依赖\")\n",
    "print(\"3. Parallel computation vs sequential RNN processing\")\n",
    "print(\"   并行计算 vs RNN 的顺序处理\")\n",
    "print(\"4. Explicit temporal encoding enhances pattern recognition\")\n",
    "print(\"   显式时间编码增强模式识别能力\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "## 可选：保存模型和预测结果 / Optional: Save Model and Predictions\n",
    "\n",
    "保存预测结果以供进一步分析。\n",
    "\n",
    "Save predictions for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for further analysis / 保存预测结果以供进一步分析\n",
    "# results = {\n",
    "#     'test_actual': test_actual_flat,\n",
    "#     'test_pred': test_pred_flat,\n",
    "#     'test_metrics': test_metrics\n",
    "# }\n",
    "# np.save('informer_results.npy', results)\n",
    "# print(\"Results saved to informer_results.npy\")\n",
    "# print(\"结果已保存到 informer_results.npy\")\n",
    "\n",
    "# Model is already saved as 'best_informer_model.pth'\n",
    "# 模型已保存为 'best_informer_model.pth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
