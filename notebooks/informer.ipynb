{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informer-intro",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Informer é¢„æµ‹ç”µåŠ›å˜å‹å™¨æ²¹æ¸© / Power Transformer Oil Temperature Prediction using Informer\n",
    "\n",
    "## ç®€ä»‹ / Introduction\n",
    "\n",
    "æœ¬ notebook å®ç°äº† **Informer æ¨¡å‹**æ¥é¢„æµ‹ç”µåŠ›å˜å‹å™¨çš„æ²¹æ¸©ï¼ˆOTï¼‰ã€‚\n",
    "\n",
    "This notebook implements the **Informer model** for predicting the oil temperature (OT) of power transformers.\n",
    "\n",
    "**Informer**: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n",
    "- **è®ºæ–‡ / Paper**: Zhou et al., AAAI 2021 **æœ€ä½³è®ºæ–‡å¥– / Best Paper Award** ğŸ†\n",
    "- **å…³é”®åˆ›æ–° / Key Innovation**: ä¸“ä¸ºé•¿åºåˆ—æ—¶é—´åºåˆ—é¢„æµ‹ï¼ˆLSTFï¼‰è®¾è®¡ / Designed specifically for long sequence time-series forecasting (LSTF)\n",
    "\n",
    "### ä¸ºä»€ä¹ˆç”¨ Informer é¢„æµ‹æ²¹æ¸©ï¼Ÿ / Why Informer for Oil Temperature Prediction?\n",
    "\n",
    "1. **é•¿æœŸä¾èµ– / Long-term Dependencies**: æ²¹æ¸©å˜åŒ–å…·æœ‰è·¨è¶Šæ•°å°æ—¶çš„çƒ­æƒ¯æ€§ / Oil temperature changes have thermal inertia spanning hours\n",
    "2. **é«˜æ•ˆæ€§ / Efficiency**: ProbSparse æ³¨æ„åŠ›å°†å¤æ‚åº¦ä» O(LÂ²) é™ä½åˆ° O(L log L) / ProbSparse attention reduces complexity from O(LÂ²) to O(L log L)\n",
    "3. **é•¿æ—¶é¢„æµ‹ / Long-horizon Forecasting**: å¯ä»¥ä¸€æ¬¡é¢„æµ‹å¤šä¸ªæ—¶é—´æ­¥ / Can predict multiple time steps ahead in one shot\n",
    "4. **æœ€å…ˆè¿› / State-of-the-Art**: åœ¨ ETT æ•°æ®é›†ä¸Šè¾¾åˆ°æœ€ä½³æ€§èƒ½ / Achieved best performance on ETT dataset\n",
    "\n",
    "### ä¸‰å¤§å…³é”®åˆ›æ–° / Three Key Innovations\n",
    "\n",
    "#### 1. ProbSparse è‡ªæ³¨æ„åŠ› / ProbSparse Self-Attention\n",
    "- ä¼ ç»Ÿè‡ªæ³¨æ„åŠ› / Traditional self-attention: O(LÂ²) å¤æ‚åº¦ / complexity\n",
    "- Informer: é€šè¿‡åªé€‰æ‹©"æ´»è·ƒ"æŸ¥è¯¢å®ç° O(L log L) / O(L log L) by selecting only \"active\" queries\n",
    "- **æ ¸å¿ƒæ€æƒ³ / Idea**: å¹¶éæ‰€æœ‰æŸ¥è¯¢éƒ½åŒç­‰é‡è¦ï¼›ä¸“æ³¨äºé«˜æ³¨æ„åŠ›åˆ†æ•°çš„æŸ¥è¯¢ / Not all queries contribute equally; focus on queries with high attention scores\n",
    "\n",
    "#### 2. è‡ªæ³¨æ„åŠ›è’¸é¦ / Self-Attention Distilling\n",
    "- é€å±‚æ¸è¿›å¼å‡å°‘åºåˆ—é•¿åº¦ / Progressively reduces sequence length layer by layer\n",
    "- çªå‡ºä¸»è¦ç‰¹å¾åŒæ—¶å‡å°‘è®¡ç®—è´Ÿæ‹… / Highlights dominant features while reducing computational burden\n",
    "- ä½¿ç”¨å¸¦æ­¥é•¿çš„æœ€å¤§æ± åŒ–åœ¨æ¯å±‚å°†è¾“å…¥å‡åŠ / Uses max pooling with stride to halve the input at each layer\n",
    "\n",
    "#### 3. ç”Ÿæˆå¼è§£ç å™¨ / Generative Style Decoder\n",
    "- ä¸€æ¬¡å‰å‘ä¼ æ’­é¢„æµ‹æ•´ä¸ªè¾“å‡ºåºåˆ— / Predicts entire output sequence in one forward pass\n",
    "- é¿å…é€æ­¥é¢„æµ‹çš„é”™è¯¯ç´¯ç§¯ / Avoids error accumulation from step-by-step prediction\n",
    "- å¤§å¹…æé«˜æ¨ç†é€Ÿåº¦ / Dramatically improves inference speed\n",
    "\n",
    "### æ•°æ®é›† / Dataset: ETT (Electricity Transformer Temperature)\n",
    "- **æ¥æº / Source**: https://github.com/zhouhaoyi/ETDataset\n",
    "- **é‡‡æ ·é—´éš” / Sampling**: 15åˆ†é’Ÿ / 15-minute intervals\n",
    "- **ç‰¹å¾ / Features**: 6 ä¸ªè´Ÿè½½ç‰¹å¾ / 6 load features (HUFL, HULL, MUFL, MULL, LUFL, LULL)\n",
    "- **ç›®æ ‡ / Target**: OT (æ²¹æ¸© / Oil Temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 1: å¯¼å…¥æ‰€éœ€åº“ / Step 1: Import Required Libraries\n",
    "\n",
    "æˆ‘ä»¬å°†ä½¿ç”¨ / We'll use:\n",
    "- pandas/numpy: æ•°æ®å¤„ç† / data manipulation\n",
    "- **PyTorch**: æ„å»º Informer æ¨¡å‹ / building the Informer model\n",
    "- sklearn: é¢„å¤„ç†å’Œè¯„ä¼°æŒ‡æ ‡ / preprocessing and metrics\n",
    "- matplotlib: å¯è§†åŒ– / visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®å¤„ç† / Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# æ·±åº¦å­¦ä¹  / Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# é¢„å¤„ç†å’Œè¯„ä¼°æŒ‡æ ‡ / Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# å¯è§†åŒ– / Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§ / Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# è®¾å¤‡é…ç½® / Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version / PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"Device / è®¾å¤‡: {device}\")\n",
    "print(f\"CUDA available / CUDA å¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 2: åŠ è½½å’Œæ¢ç´¢æ•°æ® / Step 2: Load and Explore Data\n",
    "\n",
    "æˆ‘ä»¬å°†åŠ è½½å˜å‹å™¨æ¸©åº¦æ•°æ®é›†å¹¶è¿›è¡Œåˆæ­¥æ¢ç´¢ã€‚\n",
    "\n",
    "We'll load the transformer temperature dataset and perform initial exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load the ETT dataset\n",
    "    åŠ è½½ ETT æ•°æ®é›†\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "# Load pre-split training and test data\n",
    "# åŠ è½½é¢„å…ˆåˆ‡åˆ†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "train_filepath = '../dataset/processed_data/train.csv'\n",
    "test_filepath = '../dataset/processed_data/test.csv'\n",
    "\n",
    "df_train = load_data(train_filepath)\n",
    "df_test = load_data(test_filepath)\n",
    "\n",
    "# For initial exploration, use training data\n",
    "# åˆæ­¥æ¢ç´¢ä½¿ç”¨è®­ç»ƒæ•°æ®\n",
    "df = df_train\n",
    "\n",
    "print(\"Dataset loaded from pre-split files:\")\n",
    "print(f\"  Training data: {train_filepath}\")\n",
    "print(f\"  Test data: {test_filepath}\")\n",
    "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"\\nFirst few rows of training data:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 3: æ•°æ®å¯è§†åŒ– / Step 3: Data Visualization\n",
    "\n",
    "å¯è§†åŒ–æ²¹æ¸©å’Œç‰¹å¾ç›¸å…³æ€§ã€‚\n",
    "\n",
    "Visualize oil temperature and feature correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Oil Temperature (target variable)\n",
    "# ç»˜åˆ¶æ²¹æ¸©ï¼ˆç›®æ ‡å˜é‡ï¼‰\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Full view / å®Œæ•´è§†å›¾\n",
    "axes[0].plot(df['date'][:2000], df['OT'][:2000], linewidth=0.8)\n",
    "axes[0].set_title('Oil Temperature Over Time (First 2000 points) / æ²¹æ¸©éšæ—¶é—´å˜åŒ–ï¼ˆå‰2000ä¸ªç‚¹ï¼‰', fontsize=14)\n",
    "axes[0].set_xlabel('Date / æ—¥æœŸ')\n",
    "axes[0].set_ylabel('Oil Temperature (Â°C) / æ²¹æ¸© (Â°C)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution / åˆ†å¸ƒ\n",
    "axes[1].hist(df['OT'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Oil Temperature Distribution / æ²¹æ¸©åˆ†å¸ƒ', fontsize=14)\n",
    "axes[1].set_xlabel('Oil Temperature (Â°C) / æ²¹æ¸© (Â°C)')\n",
    "axes[1].set_ylabel('Frequency / é¢‘ç‡')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap / ç›¸å…³æ€§çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(10, 8))\n",
    "features = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "sns.heatmap(df[features].corr(), annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap / ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-features-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 4: æå–æ—¶é—´ç‰¹å¾ / Step 4: Extract Time Features\n",
    "\n",
    "Informer ä½¿ç”¨æ—¶é—´ç‰¹å¾æ¥å¢å¼ºæ¨¡å‹å¯¹æ—¶é—´æ¨¡å¼çš„ç†è§£ã€‚\n",
    "æˆ‘ä»¬æå–ï¼šå°æ—¶ã€æ˜ŸæœŸå‡ ã€æœˆä»½ä¸­çš„æ—¥æœŸå’Œæœˆä»½ã€‚\n",
    "\n",
    "Informer uses time features to enhance the model's understanding of temporal patterns.\n",
    "We extract: hour, day of week, day of month, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df):\n",
    "    \"\"\"\n",
    "    Extract time features from datetime column\n",
    "    ä»æ—¥æœŸæ—¶é—´åˆ—æå–æ—¶é—´ç‰¹å¾\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pd.DataFrame with added time features\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract time components / æå–æ—¶é—´ç»„ä»¶\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    # Normalize to [0, 1] range / å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´\n",
    "    df['hour_norm'] = df['hour'] / 23.0\n",
    "    df['day_of_week_norm'] = df['day_of_week'] / 6.0\n",
    "    df['day_of_month_norm'] = (df['day_of_month'] - 1) / 30.0\n",
    "    df['month_norm'] = (df['month'] - 1) / 11.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract time features for both datasets\n",
    "# ä¸ºä¸¤ä¸ªæ•°æ®é›†æå–æ—¶é—´ç‰¹å¾\n",
    "df_train = extract_time_features(df_train)\n",
    "df_test = extract_time_features(df_test)\n",
    "df = df_train  # For exploration\n",
    "\n",
    "print(\"Time features extracted for both train and test sets!\")\n",
    "print(\"å·²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†æå–æ—¶é—´ç‰¹å¾ï¼\")\n",
    "print(\"\\nColumns / åˆ—å:\", df.columns.tolist())\n",
    "print(\"\\nSample time features / æ—¶é—´ç‰¹å¾ç¤ºä¾‹:\")\n",
    "print(df[['date', 'hour', 'day_of_week', 'day_of_month', 'month']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 5: æ•°æ®é¢„å¤„ç†å’Œæ•°æ®é›†åˆ›å»º / Step 5: Data Preprocessing and Dataset Creation\n",
    "\n",
    "### ä¸ RNN çš„å…³é”®åŒºåˆ« / Key Differences from RNN:\n",
    "- **è¾“å…¥ / Input**: `seq_len` ä¸ªå†å²æ—¶é—´æ­¥ / historical time steps\n",
    "- **æ ‡ç­¾ / Label**: `label_len` + `pred_len` (è§£ç å™¨è¾“å…¥ + é¢„æµ‹ç›®æ ‡ / decoder input + prediction target)\n",
    "- **æ—¶é—´æˆ³ / Time Stamps**: ç¼–ç å™¨å’Œè§£ç å™¨çš„æ—¶é—´ç‰¹å¾ / Time features for both encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Informer model\n",
    "    Informer æ¨¡å‹çš„æ•°æ®é›†\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    seq_len : int\n",
    "        Encoder input length (look-back window)\n",
    "        ç¼–ç å™¨è¾“å…¥é•¿åº¦ï¼ˆå›æº¯çª—å£ï¼‰\n",
    "    label_len : int\n",
    "        Decoder input length (start token)\n",
    "        è§£ç å™¨è¾“å…¥é•¿åº¦ï¼ˆèµ·å§‹æ ‡è®°ï¼‰\n",
    "    pred_len : int\n",
    "        Prediction length (forecast horizon)\n",
    "        é¢„æµ‹é•¿åº¦ï¼ˆé¢„æµ‹èŒƒå›´ï¼‰\n",
    "    feature_cols : list\n",
    "        List of feature column names\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    time_cols : list\n",
    "        List of time feature column names\n",
    "    scaler : StandardScaler or None\n",
    "        Fitted scaler (for test set) or None (for train set)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_len, label_len, pred_len, \n",
    "                 feature_cols, target_col, time_cols, scaler=None):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Normalize features / å½’ä¸€åŒ–ç‰¹å¾\n",
    "        if scaler is None:\n",
    "            self.scaler_X = StandardScaler()\n",
    "            self.scaler_y = StandardScaler()\n",
    "            \n",
    "            self.data_X = self.scaler_X.fit_transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.fit_transform(data[[target_col]].values)\n",
    "        else:\n",
    "            self.scaler_X, self.scaler_y = scaler\n",
    "            self.data_X = self.scaler_X.transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.transform(data[[target_col]].values)\n",
    "        \n",
    "        # Time features (already normalized) / æ—¶é—´ç‰¹å¾ï¼ˆå·²å½’ä¸€åŒ–ï¼‰\n",
    "        self.data_stamp = data[time_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_X) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Encoder input / ç¼–ç å™¨è¾“å…¥\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        \n",
    "        # Decoder input (overlaps with encoder) / è§£ç å™¨è¾“å…¥ï¼ˆä¸ç¼–ç å™¨é‡å ï¼‰\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        # Input features for encoder / ç¼–ç å™¨çš„è¾“å…¥ç‰¹å¾\n",
    "        seq_x = self.data_X[s_begin:s_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        \n",
    "        # Input for decoder (features + zero padding for prediction)\n",
    "        # è§£ç å™¨è¾“å…¥ï¼ˆç‰¹å¾ + é¢„æµ‹éƒ¨åˆ†çš„é›¶å¡«å……ï¼‰\n",
    "        seq_y = np.concatenate([\n",
    "            self.data_X[r_begin:r_begin+self.label_len],\n",
    "            np.zeros((self.pred_len, self.data_X.shape[1]))\n",
    "        ], axis=0)\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        \n",
    "        # Target (actual values to predict) / ç›®æ ‡ï¼ˆè¦é¢„æµ‹çš„å®é™…å€¼ï¼‰\n",
    "        target = self.data_y[r_begin:r_end]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(seq_x),\n",
    "            torch.FloatTensor(seq_x_mark),\n",
    "            torch.FloatTensor(seq_y),\n",
    "            torch.FloatTensor(seq_y_mark),\n",
    "            torch.FloatTensor(target)\n",
    "        )\n",
    "\n",
    "print(\"InformerDataset class defined! / InformerDataset ç±»å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration / é…ç½®\n",
    "seq_len = 96      # 24 hours (96 * 15min) / 24å°æ—¶\n",
    "label_len = 48    # 12 hours overlap / 12å°æ—¶é‡å \n",
    "pred_len = 24     # 6 hours prediction / 6å°æ—¶é¢„æµ‹\n",
    "\n",
    "feature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']\n",
    "target_col = 'OT'\n",
    "time_cols = ['hour_norm', 'day_of_week_norm', 'day_of_month_norm', 'month_norm']\n",
    "\n",
    "# Use pre-split data / ä½¿ç”¨é¢„åˆ‡åˆ†æ•°æ®\n",
    "print(f\"Using pre-split data / ä½¿ç”¨é¢„åˆ‡åˆ†æ•°æ®:\")\n",
    "print(f\"  Train set / è®­ç»ƒé›†: {len(df_train)} samples\")\n",
    "print(f\"  Test set / æµ‹è¯•é›†: {len(df_test)} samples\")\n",
    "\n",
    "# Create datasets / åˆ›å»ºæ•°æ®é›†\n",
    "train_dataset = InformerDataset(\n",
    "    df_train, seq_len, label_len, pred_len,\n",
    "    feature_cols, target_col, time_cols\n",
    ")\n",
    "\n",
    "test_dataset = InformerDataset(\n",
    "    df_test, seq_len, label_len, pred_len,\n",
    "    feature_cols, target_col, time_cols,\n",
    "    scaler=(train_dataset.scaler_X, train_dataset.scaler_y)\n",
    ")\n",
    "\n",
    "# Create data loaders / åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain batches / è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)}\")\n",
    "print(f\"Test batches / æµ‹è¯•æ‰¹æ¬¡: {len(test_loader)}\")\n",
    "\n",
    "# Test data loading / æµ‹è¯•æ•°æ®åŠ è½½\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shapes / æ ·æœ¬æ‰¹æ¬¡å½¢çŠ¶:\")\n",
    "print(f\"  Encoder input (seq_x) / ç¼–ç å™¨è¾“å…¥: {sample_batch[0].shape}\")\n",
    "print(f\"  Encoder time (seq_x_mark) / ç¼–ç å™¨æ—¶é—´: {sample_batch[1].shape}\")\n",
    "print(f\"  Decoder input (seq_y) / è§£ç å™¨è¾“å…¥: {sample_batch[2].shape}\")\n",
    "print(f\"  Decoder time (seq_y_mark) / è§£ç å™¨æ—¶é—´: {sample_batch[3].shape}\")\n",
    "print(f\"  Target / ç›®æ ‡: {sample_batch[4].shape}\")\n",
    "print(f\"\\nNote: Using scalers fitted on training data for test set\")\n",
    "print(f\"æ³¨æ„ï¼šä½¿ç”¨åœ¨è®­ç»ƒæ•°æ®ä¸Šæ‹Ÿåˆçš„ç¼©æ”¾å™¨å¤„ç†æµ‹è¯•é›†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 6: æ„å»º Informer æ¨¡å‹ / Step 6: Build Informer Model\n",
    "\n",
    "### æ¶æ„æ¦‚è§ˆ / Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  INFORMER æ¶æ„ / ARCHITECTURE               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  ç¼–ç å™¨è¾“å…¥ / Encoder Input    è§£ç å™¨è¾“å…¥ / Decoder Input   â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  [åµŒå…¥ / Embedding]             [åµŒå…¥ / Embedding]         â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚ProbSparseâ”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ æ ‡å‡†æ³¨æ„åŠ›â”‚            â”‚\n",
    "â”‚  â”‚æ³¨æ„åŠ›+è’¸é¦â”‚      äº¤å‰æ³¨æ„åŠ›     â”‚ Standard â”‚            â”‚\n",
    "â”‚  â”‚Attn+Dstl â”‚    Cross-Attn      â”‚ Attentionâ”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  [å‰é¦ˆç½‘ç»œ / Feed Forward]    [å‰é¦ˆç½‘ç»œ / Feed Forward]   â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                 â”‚\n",
    "â”‚                                         â†“                  â”‚\n",
    "â”‚                                [æŠ•å½± / Projection]         â”‚\n",
    "â”‚                                         â†“                  â”‚\n",
    "â”‚                                  è¾“å‡º / Output             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequence data / åºåˆ—æ•°æ®çš„ä½ç½®ç¼–ç \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Create positional encoding matrix / åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Project input features to model dimension / å°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦\"\"\"\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        x = x.permute(0, 2, 1)  # [batch, c_in, seq_len]\n",
    "        x = self.tokenConv(x)\n",
    "        x = x.permute(0, 2, 1)  # [batch, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    \"\"\"Embed time features / åµŒå…¥æ—¶é—´ç‰¹å¾\"\"\"\n",
    "    def __init__(self, d_model, embed_dim=4):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "        self.embed = nn.Linear(embed_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 4] (hour, day_of_week, day, month)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    \"\"\"Complete embedding: token + positional + temporal / å®Œæ•´åµŒå…¥ï¼šæ ‡è®° + ä½ç½® + æ—¶é—´\"\"\"\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.temporal_embedding = TimeFeatureEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, x_mark):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        # x_mark: [batch, seq_len, 4]\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"Embedding modules defined! / åµŒå…¥æ¨¡å—å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probsparse-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ProbSparse Self-Attention Mechanism\n",
    "    ProbSparse è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "    \n",
    "    Key Innovation / å…³é”®åˆ›æ–°:\n",
    "    - Select top-u queries based on sparsity measurement\n",
    "      åŸºäºç¨€ç–æ€§åº¦é‡é€‰æ‹© top-u æŸ¥è¯¢\n",
    "    - Only compute attention for selected queries\n",
    "      ä»…ä¸ºé€‰å®šçš„æŸ¥è¯¢è®¡ç®—æ³¨æ„åŠ›\n",
    "    - Reduces complexity from O(LÂ²) to O(L log L)\n",
    "      å°†å¤æ‚åº¦ä» O(LÂ²) é™ä½åˆ° O(L log L)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, \n",
    "                 attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "    \n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        \"\"\"\n",
    "        Calculate query sparsity measurement\n",
    "        è®¡ç®—æŸ¥è¯¢ç¨€ç–æ€§åº¦é‡\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Q_K : tensor\n",
    "            Sampled Q-K scores\n",
    "        M_top : tensor\n",
    "            Top-u query indices based on sparsity\n",
    "        \"\"\"\n",
    "        # Q: [B, H, L, D]\n",
    "        B, H, L_Q, D = Q.shape\n",
    "        _, _, L_K, _ = K.shape\n",
    "        \n",
    "        # Calculate sampled Q-K scores / è®¡ç®—é‡‡æ ·çš„ Q-K åˆ†æ•°\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, D)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))  # Random sampling\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "        \n",
    "        # Sparsity measurement: max - mean / ç¨€ç–æ€§åº¦é‡ï¼šæœ€å¤§å€¼ - å¹³å‡å€¼\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "        \n",
    "        # Calculate full Q-K for top queries / ä¸º top æŸ¥è¯¢è®¡ç®—å®Œæ•´çš„ Q-K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]  # [B, H, n_top, D]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # [B, H, n_top, L_K]\n",
    "        \n",
    "        return Q_K, M_top\n",
    "    \n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        \"\"\"Initialize context with mean of V / ç”¨ V çš„å¹³å‡å€¼åˆå§‹åŒ–ä¸Šä¸‹æ–‡\"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # Mean pooling / å¹³å‡æ± åŒ–\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            context = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else:\n",
    "            # Cumulative mean for masked attention / ç”¨äºæ©ç æ³¨æ„åŠ›çš„ç´¯ç§¯å¹³å‡\n",
    "            context = V.cumsum(dim=-2)\n",
    "        return context\n",
    "    \n",
    "    def _update_context(self, context_in, V, scores, index, L_Q):\n",
    "        \"\"\"Update context with selected queries / ç”¨é€‰å®šçš„æŸ¥è¯¢æ›´æ–°ä¸Šä¸‹æ–‡\"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        \n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return context_in, attns\n",
    "        else:\n",
    "            return context_in, None\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of ProbSparse Attention\n",
    "        ProbSparse æ³¨æ„åŠ›çš„å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        queries : [B, L_Q, H, D]\n",
    "        keys : [B, L_K, H, D]\n",
    "        values : [B, L_V, H, D]\n",
    "        \"\"\"\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "        \n",
    "        queries = queries.transpose(2, 1)  # [B, H, L_Q, D]\n",
    "        keys = keys.transpose(2, 1)        # [B, H, L_K, D]\n",
    "        values = values.transpose(2, 1)    # [B, H, L_V, D]\n",
    "        \n",
    "        # Calculate number of queries to select / è®¡ç®—è¦é€‰æ‹©çš„æŸ¥è¯¢æ•°é‡\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
    "        \n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "        \n",
    "        # Calculate sparse attention / è®¡ç®—ç¨€ç–æ³¨æ„åŠ›\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "        \n",
    "        # Scaling / ç¼©æ”¾\n",
    "        scale = self.scale or 1.0 / np.sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        \n",
    "        # Get context / è·å–ä¸Šä¸‹æ–‡\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q)\n",
    "        \n",
    "        return context.transpose(2, 1).contiguous(), attn\n",
    "\n",
    "\n",
    "class ProbMask:\n",
    "    \"\"\"Mask for ProbSparse Attention / ProbSparse æ³¨æ„åŠ›çš„æ©ç \"\"\"\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self.mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "print(\"ProbSparse Attention defined! / ProbSparse æ³¨æ„åŠ›å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Multi-head attention wrapper / å¤šå¤´æ³¨æ„åŠ›åŒ…è£…å™¨\"\"\"\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "        \n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        \n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "        \n",
    "        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n",
    "        out = out.view(B, L, -1)\n",
    "        \n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Informer Encoder Layer with distilling / å¸¦è’¸é¦çš„ Informer ç¼–ç å™¨å±‚\"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Multi-head attention / å¤šå¤´æ³¨æ„åŠ›\n",
    "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed forward / å‰é¦ˆç½‘ç»œ\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"Distilling layer: progressively reduce sequence length / è’¸é¦å±‚ï¼šé€æ­¥å‡å°‘åºåˆ—é•¿åº¦\"\"\"\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Informer Encoder with distilling / å¸¦è’¸é¦çš„ Informer ç¼–ç å™¨\"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x, attns\n",
    "\n",
    "print(\"Encoder modules defined! / ç¼–ç å™¨æ¨¡å—å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Informer Decoder Layer / Informer è§£ç å™¨å±‚\"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        # Self attention / è‡ªæ³¨æ„åŠ›\n",
    "        x = x + self.dropout(self.self_attention(x, x, x, attn_mask=x_mask)[0])\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Cross attention / äº¤å‰æ³¨æ„åŠ›\n",
    "        x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Feed forward / å‰é¦ˆç½‘ç»œ\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Informer Decoder / Informer è§£ç å™¨\"\"\"\n",
    "    def __init__(self, layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Decoder modules defined! / è§£ç å™¨æ¨¡å—å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informer-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Informer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Informer Model\n",
    "    å®Œæ•´çš„ Informer æ¨¡å‹\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    enc_in : int\n",
    "        Number of encoder input features / ç¼–ç å™¨è¾“å…¥ç‰¹å¾æ•°é‡\n",
    "    dec_in : int\n",
    "        Number of decoder input features / è§£ç å™¨è¾“å…¥ç‰¹å¾æ•°é‡\n",
    "    c_out : int\n",
    "        Number of output features / è¾“å‡ºç‰¹å¾æ•°é‡\n",
    "    seq_len : int\n",
    "        Input sequence length / è¾“å…¥åºåˆ—é•¿åº¦\n",
    "    label_len : int\n",
    "        Start token length for decoder / è§£ç å™¨çš„èµ·å§‹æ ‡è®°é•¿åº¦\n",
    "    out_len : int\n",
    "        Output sequence length / è¾“å‡ºåºåˆ—é•¿åº¦\n",
    "    factor : int\n",
    "        ProbSparse attention factor / ProbSparse æ³¨æ„åŠ›å› å­\n",
    "    d_model : int\n",
    "        Model dimension / æ¨¡å‹ç»´åº¦\n",
    "    n_heads : int\n",
    "        Number of attention heads / æ³¨æ„åŠ›å¤´æ•°é‡\n",
    "    e_layers : int\n",
    "        Number of encoder layers / ç¼–ç å™¨å±‚æ•°\n",
    "    d_layers : int\n",
    "        Number of decoder layers / è§£ç å™¨å±‚æ•°\n",
    "    d_ff : int\n",
    "        Feed-forward dimension / å‰é¦ˆç½‘ç»œç»´åº¦\n",
    "    dropout : float\n",
    "        Dropout rate / Dropout ç‡\n",
    "    attn : str\n",
    "        Attention type ('prob' for ProbSparse)\n",
    "    activation : str\n",
    "        Activation function / æ¿€æ´»å‡½æ•°\n",
    "    output_attention : bool\n",
    "        Whether to output attention weights / æ˜¯å¦è¾“å‡ºæ³¨æ„åŠ›æƒé‡\n",
    "    distil : bool\n",
    "        Whether to use distilling / æ˜¯å¦ä½¿ç”¨è’¸é¦\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
    "                 factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2,\n",
    "                 d_ff=512, dropout=0.0, attn='prob', activation='gelu',\n",
    "                 output_attention=False, distil=True, device=torch.device('cuda:0')):\n",
    "        super(Informer, self).__init__()\n",
    "        self.pred_len = out_len\n",
    "        self.attn = attn\n",
    "        self.output_attention = output_attention\n",
    "        \n",
    "        # Encoding / ç¼–ç \n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, dropout)\n",
    "        \n",
    "        # Attention / æ³¨æ„åŠ›\n",
    "        Attn = ProbAttention\n",
    "        \n",
    "        # Encoder / ç¼–ç å™¨\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for _ in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(d_model) for _ in range(e_layers - 1)\n",
    "            ] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Decoder / è§£ç å™¨\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                ) for _ in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Projection / æŠ•å½±\n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass / å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_enc : [batch, seq_len, enc_in]\n",
    "        x_mark_enc : [batch, seq_len, 4]\n",
    "        x_dec : [batch, label_len + pred_len, dec_in]\n",
    "        x_mark_dec : [batch, label_len + pred_len, 4]\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : [batch, pred_len, c_out]\n",
    "        \"\"\"\n",
    "        # Encoder / ç¼–ç å™¨\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        \n",
    "        # Decoder / è§£ç å™¨\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "        dec_out = self.projection(dec_out)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "\n",
    "print(\"Complete Informer model defined! / å®Œæ•´çš„ Informer æ¨¡å‹å·²å®šä¹‰ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-model-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 7: åˆå§‹åŒ–æ¨¡å‹ / Step 7: Initialize Model\n",
    "\n",
    "é…ç½®æ¨¡å‹è¶…å‚æ•°å¹¶åˆå§‹åŒ– Informer æ¨¡å‹ã€‚\n",
    "\n",
    "Configure model hyperparameters and initialize the Informer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters / æ¨¡å‹è¶…å‚æ•°\n",
    "model_config = {\n",
    "    'enc_in': len(feature_cols),      # 6 features / 6ä¸ªç‰¹å¾\n",
    "    'dec_in': len(feature_cols),      # 6 features / 6ä¸ªç‰¹å¾\n",
    "    'c_out': 1,                       # 1 target (OT) / 1ä¸ªç›®æ ‡ï¼ˆOTï¼‰\n",
    "    'seq_len': seq_len,               # 96\n",
    "    'label_len': label_len,           # 48\n",
    "    'out_len': pred_len,              # 24\n",
    "    'factor': 5,                      # ProbSparse factor / ProbSparse å› å­\n",
    "    'd_model': 512,                   # Model dimension / æ¨¡å‹ç»´åº¦\n",
    "    'n_heads': 8,                     # Attention heads / æ³¨æ„åŠ›å¤´æ•°\n",
    "    'e_layers': 2,                    # Encoder layers / ç¼–ç å™¨å±‚æ•°\n",
    "    'd_layers': 1,                    # Decoder layers / è§£ç å™¨å±‚æ•°\n",
    "    'd_ff': 2048,                     # Feed-forward dimension / å‰é¦ˆç»´åº¦\n",
    "    'dropout': 0.05,                  # Dropout\n",
    "    'attn': 'prob',                   # ProbSparse attention\n",
    "    'activation': 'gelu',             # Activation / æ¿€æ´»å‡½æ•°\n",
    "    'output_attention': False,        # Don't output attention / ä¸è¾“å‡ºæ³¨æ„åŠ›\n",
    "    'distil': True,                   # Use distilling / ä½¿ç”¨è’¸é¦\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Initialize model / åˆå§‹åŒ–æ¨¡å‹\n",
    "model = Informer(**model_config).to(device)\n",
    "\n",
    "print(\"Model initialized! / æ¨¡å‹å·²åˆå§‹åŒ–ï¼\")\n",
    "print(f\"\\nTotal parameters / æ€»å‚æ•°æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters / å¯è®­ç»ƒå‚æ•°æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 8: è®­ç»ƒé…ç½®å’Œè®­ç»ƒå¾ªç¯ / Step 8: Training Configuration and Loop\n",
    "\n",
    "é…ç½®ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œè®­ç»ƒå¾ªç¯ã€‚\n",
    "\n",
    "Configure optimizer, loss function, and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration / è®­ç»ƒé…ç½®\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "\n",
    "# Optimizer and loss / ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training history / è®­ç»ƒå†å²\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': []\n",
    "}\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training configuration / è®­ç»ƒé…ç½®:\")\n",
    "print(f\"  Epochs / è½®æ•°: {num_epochs}\")\n",
    "print(f\"  Learning rate / å­¦ä¹ ç‡: {learning_rate}\")\n",
    "print(f\"  Batch size / æ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "print(f\"  Device / è®¾å¤‡: {device}\")\n",
    "print(f\"\\nStarting training... / å¼€å§‹è®­ç»ƒ...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop / è®­ç»ƒå¾ªç¯\n",
    "for epoch in range(num_epochs):\n",
    "    # Training / è®­ç»ƒ\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (seq_x, seq_x_mark, seq_y, seq_y_mark, target) in enumerate(train_loader):\n",
    "        # Move to device / ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "        seq_x = seq_x.to(device)\n",
    "        seq_x_mark = seq_x_mark.to(device)\n",
    "        seq_y = seq_y.to(device)\n",
    "        seq_y_mark = seq_y_mark.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass / å‰å‘ä¼ æ’­\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "        \n",
    "        # Calculate loss (only on prediction part) / è®¡ç®—æŸå¤±ï¼ˆä»…åœ¨é¢„æµ‹éƒ¨åˆ†ï¼‰\n",
    "        loss = criterion(output, target[:, -pred_len:, :])\n",
    "        \n",
    "        # Backward pass / åå‘ä¼ æ’­\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # Testing / æµ‹è¯•\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in test_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            loss = criterion(output, target[:, -pred_len:, :])\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    history['test_loss'].append(avg_test_loss)\n",
    "    \n",
    "    # Learning rate scheduling / å­¦ä¹ ç‡è°ƒåº¦\n",
    "    scheduler.step(avg_test_loss)\n",
    "    \n",
    "    # Print progress / æ‰“å°è¿›åº¦\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {avg_train_loss:.6f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping / æ—©åœ\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model / ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        torch.save(model.state_dict(), 'best_informer_model.pth')\n",
    "        print(f\"  â†’ New best model saved / ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹ (Test Loss: {best_test_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            print(f\"æå‰åœæ­¢åœ¨ç¬¬ {epoch+1} è½®åè§¦å‘\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed! / è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"Best test loss / æœ€ä½³æµ‹è¯•æŸå¤±: {best_test_loss:.6f}\")\n",
    "\n",
    "# Load best model / åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load('best_informer_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 9: è®­ç»ƒå†å²å¯è§†åŒ– / Step 9: Training History Visualization\n",
    "\n",
    "ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿ã€‚\n",
    "\n",
    "Plot training and test loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss / è®­ç»ƒæŸå¤±', linewidth=2)\n",
    "plt.plot(history['test_loss'], label='Test Loss / æµ‹è¯•æŸå¤±', linewidth=2)\n",
    "plt.xlabel('Epoch / è½®æ¬¡', fontsize=12)\n",
    "plt.ylabel('Loss (MSE) / æŸå¤± (MSE)', fontsize=12)\n",
    "plt.title('Informer Model Training History / Informer æ¨¡å‹è®­ç»ƒå†å²', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 10: æ¨¡å‹è¯„ä¼°å’Œé¢„æµ‹ / Step 10: Model Evaluation and Predictions\n",
    "\n",
    "ç”Ÿæˆé¢„æµ‹å¹¶åå‘è½¬æ¢åˆ°åŸå§‹å°ºåº¦ã€‚\n",
    "\n",
    "Make predictions and inverse transform to original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "make-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, data_loader, scaler_y, device):\n",
    "    \"\"\"\n",
    "    Make predictions and inverse transform to original scale\n",
    "    ç”Ÿæˆé¢„æµ‹å¹¶åå‘è½¬æ¢åˆ°åŸå§‹å°ºåº¦\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in data_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            \n",
    "            # Move to CPU and convert to numpy / ç§»è‡³ CPU å¹¶è½¬æ¢ä¸º numpy\n",
    "            pred = output.cpu().numpy()\n",
    "            actual = target[:, -pred_len:, :].cpu().numpy()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            actuals.append(actual)\n",
    "    \n",
    "    # Concatenate all batches / è¿æ¥æ‰€æœ‰æ‰¹æ¬¡\n",
    "    predictions = np.concatenate(predictions, axis=0)  # [N, pred_len, 1]\n",
    "    actuals = np.concatenate(actuals, axis=0)          # [N, pred_len, 1]\n",
    "    \n",
    "    # Reshape for inverse transform / ä¸ºåå‘è½¬æ¢é‡å¡‘\n",
    "    pred_shape = predictions.shape\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform / åå‘è½¬æ¢\n",
    "    predictions = scaler_y.inverse_transform(predictions)\n",
    "    actuals = scaler_y.inverse_transform(actuals)\n",
    "    \n",
    "    # Reshape back / é‡å¡‘å›å»\n",
    "    predictions = predictions.reshape(pred_shape)\n",
    "    actuals = actuals.reshape(pred_shape)\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Make predictions / ç”Ÿæˆé¢„æµ‹\n",
    "print(\"Making predictions... / ç”Ÿæˆé¢„æµ‹...\")\n",
    "train_pred, train_actual = make_predictions(model, train_loader, train_dataset.scaler_y, device)\n",
    "test_pred, test_actual = make_predictions(model, test_loader, test_dataset.scaler_y, device)\n",
    "\n",
    "print(f\"\\nPrediction shapes / é¢„æµ‹å½¢çŠ¶:\")\n",
    "print(f\"  Train / è®­ç»ƒ: {train_pred.shape}\")\n",
    "print(f\"  Test / æµ‹è¯•: {test_pred.shape}\")\n",
    "\n",
    "# For metric calculation, use only the last time step of each prediction\n",
    "# ç”¨äºæŒ‡æ ‡è®¡ç®—ï¼Œä»…ä½¿ç”¨æ¯ä¸ªé¢„æµ‹çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥\n",
    "# Or average across prediction horizon / æˆ–åœ¨é¢„æµ‹èŒƒå›´å†…å–å¹³å‡\n",
    "train_pred_flat = train_pred[:, -1, 0]  # Last prediction of each sequence\n",
    "train_actual_flat = train_actual[:, -1, 0]\n",
    "test_pred_flat = test_pred[:, -1, 0]\n",
    "test_actual_flat = test_actual[:, -1, 0]\n",
    "\n",
    "print(f\"\\nFlattened for metrics / ç”¨äºæŒ‡æ ‡çš„æ‰å¹³åŒ–:\")\n",
    "print(f\"  Train / è®­ç»ƒ: {train_pred_flat.shape}\")\n",
    "print(f\"  Test / æµ‹è¯•: {test_pred_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, set_name='Test'):\n",
    "    \"\"\"\n",
    "    Calculate and display regression metrics\n",
    "    è®¡ç®—å¹¶æ˜¾ç¤ºå›å½’æŒ‡æ ‡\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Performance Metrics / {set_name}é›†æ€§èƒ½æŒ‡æ ‡:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"MSE (Mean Squared Error):        {mse:.4f}\")\n",
    "    print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}Â°C\")\n",
    "    print(f\"MAE (Mean Absolute Error):       {mae:.4f}Â°C\")\n",
    "    print(f\"RÂ² Score:                        {r2:.4f}\")\n",
    "    print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Calculate metrics / è®¡ç®—æŒ‡æ ‡\n",
    "train_metrics = calculate_metrics(train_actual_flat, train_pred_flat, 'Training / è®­ç»ƒ')\n",
    "test_metrics = calculate_metrics(test_actual_flat, test_pred_flat, 'Test / æµ‹è¯•')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 11: ç»“æœå¯è§†åŒ– / Step 11: Results Visualization\n",
    "\n",
    "å¯è§†åŒ–é¢„æµ‹ç»“æœå’Œè¯¯å·®åˆ†æã€‚\n",
    "\n",
    "Visualize prediction results and error analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual / ç»˜åˆ¶é¢„æµ‹ vs å®é™…\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Test set - full view / æµ‹è¯•é›† - å®Œæ•´è§†å›¾\n",
    "axes[0].plot(test_actual_flat, label='Actual / å®é™…', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(test_pred_flat, label='Predicted / é¢„æµ‹', alpha=0.7, linewidth=1.5)\n",
    "axes[0].set_title('Informer: Oil Temperature Prediction - Test Set (Full View) / æ²¹æ¸©é¢„æµ‹ - æµ‹è¯•é›†ï¼ˆå®Œæ•´è§†å›¾ï¼‰', fontsize=14)\n",
    "axes[0].set_xlabel('Sample / æ ·æœ¬')\n",
    "axes[0].set_ylabel('Oil Temperature (Â°C) / æ²¹æ¸© (Â°C)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set - zoomed view / æµ‹è¯•é›† - æ”¾å¤§è§†å›¾\n",
    "zoom_range = 500\n",
    "axes[1].plot(test_actual_flat[:zoom_range], label='Actual / å®é™…', alpha=0.7, linewidth=1.5)\n",
    "axes[1].plot(test_pred_flat[:zoom_range], label='Predicted / é¢„æµ‹', alpha=0.7, linewidth=1.5)\n",
    "axes[1].set_title(f'Informer: Oil Temperature Prediction - Test Set (First {zoom_range} Samples) / æ²¹æ¸©é¢„æµ‹ - æµ‹è¯•é›†ï¼ˆå‰{zoom_range}ä¸ªæ ·æœ¬ï¼‰', fontsize=14)\n",
    "axes[1].set_xlabel('Sample / æ ·æœ¬')\n",
    "axes[1].set_ylabel('Oil Temperature (Â°C) / æ²¹æ¸© (Â°C)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot / æ•£ç‚¹å›¾\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(test_actual_flat, test_pred_flat, alpha=0.5, s=20)\n",
    "plt.plot([test_actual_flat.min(), test_actual_flat.max()],\n",
    "         [test_actual_flat.min(), test_actual_flat.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction / å®Œç¾é¢„æµ‹')\n",
    "plt.xlabel('Actual Oil Temperature (Â°C) / å®é™…æ²¹æ¸© (Â°C)', fontsize=12)\n",
    "plt.ylabel('Predicted Oil Temperature (Â°C) / é¢„æµ‹æ²¹æ¸© (Â°C)', fontsize=12)\n",
    "plt.title('Informer: Predicted vs Actual Oil Temperature (Test Set) / é¢„æµ‹ vs å®é™…æ²¹æ¸©ï¼ˆæµ‹è¯•é›†ï¼‰', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis / æ®‹å·®åˆ†æ\n",
    "residuals = test_actual_flat - test_pred_flat\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residual plot / æ®‹å·®å›¾\n",
    "axes[0].scatter(test_pred_flat, residuals, alpha=0.5, s=20)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Oil Temperature (Â°C) / é¢„æµ‹æ²¹æ¸© (Â°C)')\n",
    "axes[0].set_ylabel('Residuals (Â°C) / æ®‹å·® (Â°C)')\n",
    "axes[0].set_title('Residual Plot / æ®‹å·®å›¾')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution / æ®‹å·®åˆ†å¸ƒ\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (Â°C) / æ®‹å·® (Â°C)')\n",
    "axes[1].set_ylabel('Frequency / é¢‘ç‡')\n",
    "axes[1].set_title('Distribution of Residuals / æ®‹å·®åˆ†å¸ƒ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics / æ®‹å·®ç»Ÿè®¡:\")\n",
    "print(f\"Mean / å‡å€¼: {residuals.mean():.4f}Â°C\")\n",
    "print(f\"Std / æ ‡å‡†å·®: {residuals.std():.4f}Â°C\")\n",
    "print(f\"Min / æœ€å°å€¼: {residuals.min():.4f}Â°C\")\n",
    "print(f\"Max / æœ€å¤§å€¼: {residuals.max():.4f}Â°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-step-viz-header",
   "metadata": {},
   "source": [
    "### å¤šæ­¥é¢„æµ‹å¯è§†åŒ– / Multi-step Prediction Visualization\n",
    "\n",
    "å¯è§†åŒ–æ¨¡å‹åœ¨æ•´ä¸ªé¢„æµ‹èŒƒå›´å†…çš„é¢„æµ‹æ•ˆæœã€‚\n",
    "\n",
    "Visualize how well the model predicts across the entire prediction horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-step-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-step predictions for a few samples\n",
    "# å¯è§†åŒ–å‡ ä¸ªæ ·æœ¬çš„å¤šæ­¥é¢„æµ‹\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(test_pred), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 1, figsize=(12, 3*n_samples))\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    ax = axes[idx] if n_samples > 1 else axes\n",
    "    \n",
    "    time_steps = np.arange(pred_len)\n",
    "    actual = test_actual[sample_idx, :, 0]\n",
    "    predicted = test_pred[sample_idx, :, 0]\n",
    "    \n",
    "    ax.plot(time_steps, actual, 'o-', label='Actual / å®é™…', linewidth=2, markersize=6)\n",
    "    ax.plot(time_steps, predicted, 's-', label='Predicted / é¢„æµ‹', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Prediction Step / é¢„æµ‹æ­¥æ•°')\n",
    "    ax.set_ylabel('Oil Temperature (Â°C) / æ²¹æ¸© (Â°C)')\n",
    "    ax.set_title(f'Sample {sample_idx}: {pred_len}-step Ahead Prediction / æ ·æœ¬ {sample_idx}ï¼š{pred_len}æ­¥é¢„æµ‹')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## æ­¥éª¤ 12: ç»“è®ºå’Œæ¨¡å‹æ¯”è¾ƒ / Step 12: Conclusion and Model Comparison\n",
    "\n",
    "æ€»ç»“ Informer æ¨¡å‹çš„æ€§èƒ½å¹¶ä¸å…¶ä»–æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚\n",
    "\n",
    "Summarize Informer model performance and compare with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison / åˆ›å»ºæ€»ç»“å¯¹æ¯”\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric / æŒ‡æ ‡': ['MSE', 'RMSE (Â°C)', 'MAE (Â°C)', 'RÂ²', 'MAPE (%)'],\n",
    "    'Training / è®­ç»ƒ': [\n",
    "        train_metrics['MSE'],\n",
    "        train_metrics['RMSE'],\n",
    "        train_metrics['MAE'],\n",
    "        train_metrics['R2'],\n",
    "        train_metrics['MAPE']\n",
    "    ],\n",
    "    'Test / æµ‹è¯•': [\n",
    "        test_metrics['MSE'],\n",
    "        test_metrics['RMSE'],\n",
    "        test_metrics['MAE'],\n",
    "        test_metrics['R2'],\n",
    "        test_metrics['MAPE']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFORMER MODEL PERFORMANCE SUMMARY / INFORMER æ¨¡å‹æ€§èƒ½æ€»ç»“\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Key Observations / å…³é”®è§‚å¯Ÿ:\")\n",
    "print(f\"1. The Informer achieves RÂ² score of {test_metrics['R2']:.4f} on test set\")\n",
    "print(f\"   Informer åœ¨æµ‹è¯•é›†ä¸Šè¾¾åˆ° RÂ² åˆ†æ•° {test_metrics['R2']:.4f}\")\n",
    "print(f\"2. Average prediction error (MAE): {test_metrics['MAE']:.4f}Â°C\")\n",
    "print(f\"   å¹³å‡é¢„æµ‹è¯¯å·® (MAE): {test_metrics['MAE']:.4f}Â°C\")\n",
    "print(f\"3. MAPE: {test_metrics['MAPE']:.2f}%\")\n",
    "\n",
    "if abs(train_metrics['R2'] - test_metrics['R2']) < 0.05:\n",
    "    print(\"4. Model shows good generalization (minimal overfitting)\")\n",
    "    print(\"   æ¨¡å‹æ˜¾ç¤ºè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼ˆè¿‡æ‹Ÿåˆæœ€å°ï¼‰\")\n",
    "else:\n",
    "    print(\"4. Some overfitting detected - consider more regularization\")\n",
    "    print(\"   æ£€æµ‹åˆ°ä¸€äº›è¿‡æ‹Ÿåˆ - è€ƒè™‘æ›´å¤šæ­£åˆ™åŒ–\")\n",
    "\n",
    "print(\"\\n### Model Architecture / æ¨¡å‹æ¶æ„:\")\n",
    "print(f\"- Input Sequence Length / è¾“å…¥åºåˆ—é•¿åº¦: {seq_len} steps (24 hours / 24å°æ—¶)\")\n",
    "print(f\"- Prediction Horizon / é¢„æµ‹èŒƒå›´: {pred_len} steps (6 hours / 6å°æ—¶)\")\n",
    "print(f\"- Model Dimension / æ¨¡å‹ç»´åº¦: {model_config['d_model']}\")\n",
    "print(f\"- Attention Heads / æ³¨æ„åŠ›å¤´æ•°: {model_config['n_heads']}\")\n",
    "print(f\"- Encoder Layers / ç¼–ç å™¨å±‚æ•°: {model_config['e_layers']}\")\n",
    "print(f\"- Decoder Layers / è§£ç å™¨å±‚æ•°: {model_config['d_layers']}\")\n",
    "print(f\"- Total Parameters / æ€»å‚æ•°æ•°: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\n### Informer Advantages / Informer ä¼˜åŠ¿:\")\n",
    "print(\"1. ProbSparse Attention: O(L log L) complexity vs O(LÂ²) in vanilla Transformer\")\n",
    "print(\"   ProbSparse æ³¨æ„åŠ›ï¼šO(L log L) å¤æ‚åº¦ vs åŸå§‹ Transformer çš„ O(LÂ²)\")\n",
    "print(\"2. Self-Attention Distilling: Progressive dimension reduction\")\n",
    "print(\"   è‡ªæ³¨æ„åŠ›è’¸é¦ï¼šæ¸è¿›å¼ç»´åº¦é™ä½\")\n",
    "print(\"3. Generative Decoder: One-shot long sequence prediction\")\n",
    "print(\"   ç”Ÿæˆå¼è§£ç å™¨ï¼šä¸€æ¬¡æ€§é•¿åºåˆ—é¢„æµ‹\")\n",
    "print(\"4. Explicitly models temporal patterns with time features\")\n",
    "print(\"   é€šè¿‡æ—¶é—´ç‰¹å¾æ˜¾å¼å»ºæ¨¡æ—¶é—´æ¨¡å¼\")\n",
    "print(\"5. State-of-the-art performance on long sequence forecasting\")\n",
    "print(\"   åœ¨é•¿åºåˆ—é¢„æµ‹ä¸Šè¾¾åˆ°æœ€å…ˆè¿›æ€§èƒ½\")\n",
    "\n",
    "print(\"\\n### Comparison with Other Models / ä¸å…¶ä»–æ¨¡å‹çš„æ¯”è¾ƒ:\")\n",
    "print(\"Expected Performance Ranking (on ETT dataset) / é¢„æœŸæ€§èƒ½æ’åï¼ˆåœ¨ ETT æ•°æ®é›†ä¸Šï¼‰:\")\n",
    "print(\"1. Informer (SOTA) - Best for long sequences / æœ€é€‚åˆé•¿åºåˆ—\")\n",
    "print(\"2. LSTM/GRU - Good sequential modeling / è‰¯å¥½çš„åºåˆ—å»ºæ¨¡\")\n",
    "print(\"3. MLP - Fast but limited temporal modeling / å¿«é€Ÿä½†æ—¶é—´å»ºæ¨¡æœ‰é™\")\n",
    "print(\"4. Random Forest - Good for nonlinear patterns / é€‚åˆéçº¿æ€§æ¨¡å¼\")\n",
    "print(\"5. Linear Regression - Baseline / åŸºçº¿æ¨¡å‹\")\n",
    "\n",
    "print(\"\\n### Why Informer Outperforms RNN / ä¸ºä»€ä¹ˆ Informer ä¼˜äº RNN:\")\n",
    "print(\"1. RNNs suffer from vanishing gradients on long sequences\")\n",
    "print(\"   RNN åœ¨é•¿åºåˆ—ä¸Šå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜\")\n",
    "print(\"2. Informer's attention captures long-range dependencies better\")\n",
    "print(\"   Informer çš„æ³¨æ„åŠ›æœºåˆ¶æ›´å¥½åœ°æ•è·é•¿ç¨‹ä¾èµ–\")\n",
    "print(\"3. Parallel computation vs sequential RNN processing\")\n",
    "print(\"   å¹¶è¡Œè®¡ç®— vs RNN çš„é¡ºåºå¤„ç†\")\n",
    "print(\"4. Explicit temporal encoding enhances pattern recognition\")\n",
    "print(\"   æ˜¾å¼æ—¶é—´ç¼–ç å¢å¼ºæ¨¡å¼è¯†åˆ«èƒ½åŠ›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "## å¯é€‰ï¼šä¿å­˜æ¨¡å‹å’Œé¢„æµ‹ç»“æœ / Optional: Save Model and Predictions\n",
    "\n",
    "ä¿å­˜é¢„æµ‹ç»“æœä»¥ä¾›è¿›ä¸€æ­¥åˆ†æã€‚\n",
    "\n",
    "Save predictions for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for further analysis / ä¿å­˜é¢„æµ‹ç»“æœä»¥ä¾›è¿›ä¸€æ­¥åˆ†æ\n",
    "# results = {\n",
    "#     'test_actual': test_actual_flat,\n",
    "#     'test_pred': test_pred_flat,\n",
    "#     'test_metrics': test_metrics\n",
    "# }\n",
    "# np.save('informer_results.npy', results)\n",
    "# print(\"Results saved to informer_results.npy\")\n",
    "# print(\"ç»“æœå·²ä¿å­˜åˆ° informer_results.npy\")\n",
    "\n",
    "# Model is already saved as 'best_informer_model.pth'\n",
    "# æ¨¡å‹å·²ä¿å­˜ä¸º 'best_informer_model.pth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
