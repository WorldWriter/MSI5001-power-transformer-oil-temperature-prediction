{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informer-intro",
   "metadata": {},
   "source": [
    "# Power Transformer Oil Temperature Prediction using Informer\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements the **Informer model** for predicting the oil temperature (OT) of power transformers.\n",
    "\n",
    "**Informer**: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\n",
    "- **Paper**: Zhou et al., AAAI 2021 **Best Paper Award** ğŸ†\n",
    "- **Key Innovation**: Designed specifically for long sequence time-series forecasting (LSTF)\n",
    "\n",
    "### Why Informer for Oil Temperature Prediction?\n",
    "\n",
    "1. **Long-term Dependencies**: Oil temperature changes have thermal inertia spanning hours\n",
    "2. **Efficiency**: ProbSparse attention reduces complexity from O(LÂ²) to O(L log L)\n",
    "3. **Long-horizon Forecasting**: Can predict multiple time steps ahead in one shot\n",
    "4. **State-of-the-Art**: Achieved best performance on ETT dataset\n",
    "\n",
    "### Three Key Innovations\n",
    "\n",
    "#### 1. ProbSparse Self-Attention\n",
    "- Traditional self-attention: O(LÂ²) complexity\n",
    "- Informer: O(L log L) by selecting only \"active\" queries\n",
    "- **Idea**: Not all queries contribute equally; focus on queries with high attention scores\n",
    "\n",
    "#### 2. Self-Attention Distilling\n",
    "- Progressively reduces sequence length layer by layer\n",
    "- Highlights dominant features while reducing computational burden\n",
    "- Uses max pooling with stride to halve the input at each layer\n",
    "\n",
    "#### 3. Generative Style Decoder\n",
    "- Predicts entire output sequence in one forward pass\n",
    "- Avoids error accumulation from step-by-step prediction\n",
    "- Dramatically improves inference speed\n",
    "\n",
    "### Dataset: ETT (Electricity Transformer Temperature)\n",
    "- **Source**: https://github.com/zhouhaoyi/ETDataset\n",
    "- **Sampling**: 15-minute intervals\n",
    "- **Features**: 6 load features (HUFL, HULL, MUFL, MULL, LUFL, LULL)\n",
    "- **Target**: OT (Oil Temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-libs",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Preprocessing and Metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data-header",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": "def load_data(filepath):\n    \"\"\"\n    Load the ETT dataset\n    \"\"\"\n    df = pd.read_csv(filepath)\n    df['date'] = pd.to_datetime(df['date'])\n    return df\n\n# Load pre-split training and test data\ntrain_filepath = '../dataset/processed_data/train.csv'\ntest_filepath = '../dataset/processed_data/test.csv'\n\ndf_train = load_data(train_filepath)\ndf_test = load_data(test_filepath)\n\n# For initial exploration, use training data\ndf = df_train\n\nprint(\"Dataset loaded from pre-split files:\")\nprint(f\"  Training data: {train_filepath}\")\nprint(f\"  Test data: {test_filepath}\")\nprint(f\"\\nTraining set shape: {df_train.shape}\")\nprint(f\"Test set shape: {df_test.shape}\")\nprint(f\"\\nFirst few rows of training data:\")\nprint(df.head())\nprint(\"\\nDataset Info:\")\nprint(df.info())\nprint(\"\\nBasic Statistics:\")\nprint(df.describe())"
  },
  {
   "cell_type": "markdown",
   "id": "visualize-header",
   "metadata": {},
   "source": [
    "## Step 3: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Oil Temperature (target variable)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Full view\n",
    "axes[0].plot(df['date'][:2000], df['OT'][:2000], linewidth=0.8)\n",
    "axes[0].set_title('Oil Temperature Over Time (First 2000 points)', fontsize=14)\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Oil Temperature (Â°C)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[1].hist(df['OT'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Oil Temperature Distribution', fontsize=14)\n",
    "axes[1].set_xlabel('Oil Temperature (Â°C)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "features = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']\n",
    "sns.heatmap(df[features].corr(), annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "time-features-header",
   "metadata": {},
   "source": [
    "## Step 4: Extract Time Features\n",
    "\n",
    "Informer uses time features to enhance the model's understanding of temporal patterns.\n",
    "We extract: hour, day of week, day of month, and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "time-features",
   "metadata": {},
   "outputs": [],
   "source": "def extract_time_features(df):\n    \"\"\"\n    Extract time features from datetime column\n    \n    Returns:\n    --------\n    df : pd.DataFrame with added time features\n    \"\"\"\n    df = df.copy()\n    \n    # Extract time components\n    df['hour'] = df['date'].dt.hour\n    df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n    df['day_of_month'] = df['date'].dt.day\n    df['month'] = df['date'].dt.month\n    \n    # Normalize to [0, 1] range\n    df['hour_norm'] = df['hour'] / 23.0\n    df['day_of_week_norm'] = df['day_of_week'] / 6.0\n    df['day_of_month_norm'] = (df['day_of_month'] - 1) / 30.0\n    df['month_norm'] = (df['month'] - 1) / 11.0\n    \n    return df\n\n# Extract time features for both datasets\ndf_train = extract_time_features(df_train)\ndf_test = extract_time_features(df_test)\ndf = df_train  # For exploration\n\nprint(\"Time features extracted for both train and test sets!\")\nprint(\"\\nColumns:\", df.columns.tolist())\nprint(\"\\nSample time features:\")\nprint(df[['date', 'hour', 'day_of_week', 'day_of_month', 'month']].head(10))"
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## Step 5: Data Preprocessing and Dataset Creation\n",
    "\n",
    "### Key Differences from RNN:\n",
    "- **Input**: `seq_len` historical time steps\n",
    "- **Label**: `label_len` + `pred_len` (decoder input + prediction target)\n",
    "- **Time Stamps**: Time features for both encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InformerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Informer model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Input dataframe with features and target\n",
    "    seq_len : int\n",
    "        Encoder input length (look-back window)\n",
    "    label_len : int\n",
    "        Decoder input length (start token)\n",
    "    pred_len : int\n",
    "        Prediction length (forecast horizon)\n",
    "    feature_cols : list\n",
    "        List of feature column names\n",
    "    target_col : str\n",
    "        Target column name\n",
    "    time_cols : list\n",
    "        List of time feature column names\n",
    "    scaler : StandardScaler or None\n",
    "        Fitted scaler (for test set) or None (for train set)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, seq_len, label_len, pred_len, \n",
    "                 feature_cols, target_col, time_cols, scaler=None):\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        \n",
    "        # Normalize features\n",
    "        if scaler is None:\n",
    "            self.scaler_X = StandardScaler()\n",
    "            self.scaler_y = StandardScaler()\n",
    "            \n",
    "            self.data_X = self.scaler_X.fit_transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.fit_transform(data[[target_col]].values)\n",
    "        else:\n",
    "            self.scaler_X, self.scaler_y = scaler\n",
    "            self.data_X = self.scaler_X.transform(data[feature_cols].values)\n",
    "            self.data_y = self.scaler_y.transform(data[[target_col]].values)\n",
    "        \n",
    "        # Time features (already normalized)\n",
    "        self.data_stamp = data[time_cols].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_X) - self.seq_len - self.pred_len + 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Encoder input\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        \n",
    "        # Decoder input (overlaps with encoder)\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "        \n",
    "        # Input features for encoder\n",
    "        seq_x = self.data_X[s_begin:s_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        \n",
    "        # Input for decoder (features + zero padding for prediction)\n",
    "        seq_y = np.concatenate([\n",
    "            self.data_X[r_begin:r_begin+self.label_len],\n",
    "            np.zeros((self.pred_len, self.data_X.shape[1]))\n",
    "        ], axis=0)\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "        \n",
    "        # Target (actual values to predict)\n",
    "        target = self.data_y[r_begin:r_end]\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(seq_x),\n",
    "            torch.FloatTensor(seq_x_mark),\n",
    "            torch.FloatTensor(seq_y),\n",
    "            torch.FloatTensor(seq_y_mark),\n",
    "            torch.FloatTensor(target)\n",
    "        )\n",
    "\n",
    "print(\"InformerDataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-data",
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nseq_len = 96      # 24 hours (96 * 15min)\nlabel_len = 48    # 12 hours overlap\npred_len = 24     # 6 hours prediction\n\nfeature_cols = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL']\ntarget_col = 'OT'\ntime_cols = ['hour_norm', 'day_of_week_norm', 'day_of_month_norm', 'month_norm']\n\n# Use pre-split data\nprint(f\"Using pre-split data:\")\nprint(f\"  Train set: {len(df_train)} samples\")\nprint(f\"  Test set: {len(df_test)} samples\")\n\n# Create datasets\ntrain_dataset = InformerDataset(\n    df_train, seq_len, label_len, pred_len,\n    feature_cols, target_col, time_cols\n)\n\ntest_dataset = InformerDataset(\n    df_test, seq_len, label_len, pred_len,\n    feature_cols, target_col, time_cols,\n    scaler=(train_dataset.scaler_X, train_dataset.scaler_y)\n)\n\n# Create data loaders\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"\\nTrain batches: {len(train_loader)}\")\nprint(f\"Test batches: {len(test_loader)}\")\n\n# Test data loading\nsample_batch = next(iter(train_loader))\nprint(f\"\\nSample batch shapes:\")\nprint(f\"  Encoder input (seq_x): {sample_batch[0].shape}\")\nprint(f\"  Encoder time (seq_x_mark): {sample_batch[1].shape}\")\nprint(f\"  Decoder input (seq_y): {sample_batch[2].shape}\")\nprint(f\"  Decoder time (seq_y_mark): {sample_batch[3].shape}\")\nprint(f\"  Target: {sample_batch[4].shape}\")\nprint(f\"\\nNote: Using scalers fitted on training data for test set\")"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Step 6: Build Informer Model\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     INFORMER ARCHITECTURE                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Encoder Input                    Decoder Input            â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  [Embedding]                      [Embedding]              â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚ProbSparseâ”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ Standard â”‚            â”‚\n",
    "â”‚  â”‚Attention â”‚      Cross-Attn     â”‚ Attentionâ”‚            â”‚\n",
    "â”‚  â”‚+ Distill â”‚                     â”‚          â”‚            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  [Feed Forward]                   [Feed Forward]          â”‚\n",
    "â”‚      â†“                                 â†“                   â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                 â”‚\n",
    "â”‚                                         â†“                  â”‚\n",
    "â”‚                                    [Projection]            â”‚\n",
    "â”‚                                         â†“                  â”‚\n",
    "â”‚                                      Output                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positional-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Positional encoding for sequence data\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Project input features to model dimension\"\"\"\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        x = x.permute(0, 2, 1)  # [batch, c_in, seq_len]\n",
    "        x = self.tokenConv(x)\n",
    "        x = x.permute(0, 2, 1)  # [batch, seq_len, d_model]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    \"\"\"Embed time features\"\"\"\n",
    "    def __init__(self, d_model, embed_dim=4):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "        self.embed = nn.Linear(embed_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, 4] (hour, day_of_week, day, month)\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    \"\"\"Complete embedding: token + positional + temporal\"\"\"\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.temporal_embedding = TimeFeatureEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, x_mark):\n",
    "        # x: [batch, seq_len, c_in]\n",
    "        # x_mark: [batch, seq_len, 4]\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) + self.temporal_embedding(x_mark)\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"Embedding modules defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probsparse-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ProbSparse Self-Attention Mechanism\n",
    "    \n",
    "    Key Innovation:\n",
    "    - Select top-u queries based on sparsity measurement\n",
    "    - Only compute attention for selected queries\n",
    "    - Reduces complexity from O(LÂ²) to O(L log L)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, \n",
    "                 attention_dropout=0.1, output_attention=False):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "    \n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        \"\"\"\n",
    "        Calculate query sparsity measurement\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Q_K : tensor\n",
    "            Sampled Q-K scores\n",
    "        M_top : tensor\n",
    "            Top-u query indices based on sparsity\n",
    "        \"\"\"\n",
    "        # Q: [B, H, L, D]\n",
    "        B, H, L_Q, D = Q.shape\n",
    "        _, _, L_K, _ = K.shape\n",
    "        \n",
    "        # Calculate sampled Q-K scores\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, D)\n",
    "        index_sample = torch.randint(L_K, (L_Q, sample_k))  # Random sampling\n",
    "        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze(-2)\n",
    "        \n",
    "        # Sparsity measurement: max - mean\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "        \n",
    "        # Calculate full Q-K for top queries\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]  # [B, H, n_top, D]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # [B, H, n_top, L_K]\n",
    "        \n",
    "        return Q_K, M_top\n",
    "    \n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        \"\"\"\n",
    "        Initialize context with mean of V\n",
    "        \"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            # Mean pooling\n",
    "            V_sum = V.mean(dim=-2)\n",
    "            context = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else:\n",
    "            # Cumulative mean for masked attention\n",
    "            context = V.cumsum(dim=-2)\n",
    "        return context\n",
    "    \n",
    "    def _update_context(self, context_in, V, scores, index, L_Q):\n",
    "        \"\"\"\n",
    "        Update context with selected queries\n",
    "        \"\"\"\n",
    "        B, H, L_V, D = V.shape\n",
    "        \n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V).type_as(context_in)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            attns = (torch.ones([B, H, L_V, L_V]) / L_V).type_as(attn).to(attn.device)\n",
    "            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n",
    "            return context_in, attns\n",
    "        else:\n",
    "            return context_in, None\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of ProbSparse Attention\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        queries : [B, L_Q, H, D]\n",
    "        keys : [B, L_K, H, D]\n",
    "        values : [B, L_V, H, D]\n",
    "        \"\"\"\n",
    "        B, L_Q, H, D = queries.shape\n",
    "        _, L_K, _, _ = keys.shape\n",
    "        \n",
    "        queries = queries.transpose(2, 1)  # [B, H, L_Q, D]\n",
    "        keys = keys.transpose(2, 1)        # [B, H, L_K, D]\n",
    "        values = values.transpose(2, 1)    # [B, H, L_V, D]\n",
    "        \n",
    "        # Calculate number of queries to select\n",
    "        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()\n",
    "        \n",
    "        U_part = U_part if U_part < L_K else L_K\n",
    "        u = u if u < L_Q else L_Q\n",
    "        \n",
    "        # Calculate sparse attention\n",
    "        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n",
    "        \n",
    "        # Scaling\n",
    "        scale = self.scale or 1.0 / np.sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        \n",
    "        # Get context\n",
    "        context = self._get_initial_context(values, L_Q)\n",
    "        context, attn = self._update_context(context, values, scores_top, index, L_Q)\n",
    "        \n",
    "        return context.transpose(2, 1).contiguous(), attn\n",
    "\n",
    "\n",
    "class ProbMask:\n",
    "    \"\"\"Mask for ProbSparse Attention\"\"\"\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self.mask = indicator.view(scores.shape).to(device)\n",
    "\n",
    "print(\"ProbSparse Attention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attention-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"Multi-head attention wrapper\"\"\"\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None, d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "        \n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        \n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "        \n",
    "        out, attn = self.inner_attention(queries, keys, values, attn_mask)\n",
    "        out = out.view(B, L, -1)\n",
    "        \n",
    "        return self.out_projection(out), attn\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Informer Encoder Layer with distilling\"\"\"\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Multi-head attention\n",
    "        new_x, attn = self.attention(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(new_x)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed forward\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    \"\"\"Distilling layer: progressively reduce sequence length\"\"\"\n",
    "    def __init__(self, c_in):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=c_in,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='circular'\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x.permute(0, 2, 1))\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Informer Encoder with distilling\"\"\"\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x, attns\n",
    "\n",
    "print(\"Encoder modules defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decoder-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Informer Decoder Layer\"\"\"\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        # Self attention\n",
    "        x = x + self.dropout(self.self_attention(x, x, x, attn_mask=x_mask)[0])\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Cross attention\n",
    "        x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Feed forward\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        \n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Informer Decoder\"\"\"\n",
    "    def __init__(self, layers, norm_layer=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "    \n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"Decoder modules defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informer-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Informer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Informer Model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    enc_in : int\n",
    "        Number of encoder input features\n",
    "    dec_in : int\n",
    "        Number of decoder input features\n",
    "    c_out : int\n",
    "        Number of output features\n",
    "    seq_len : int\n",
    "        Input sequence length\n",
    "    label_len : int\n",
    "        Start token length for decoder\n",
    "    out_len : int\n",
    "        Output sequence length\n",
    "    factor : int\n",
    "        ProbSparse attention factor\n",
    "    d_model : int\n",
    "        Model dimension\n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "    e_layers : int\n",
    "        Number of encoder layers\n",
    "    d_layers : int\n",
    "        Number of decoder layers\n",
    "    d_ff : int\n",
    "        Feed-forward dimension\n",
    "    dropout : float\n",
    "        Dropout rate\n",
    "    attn : str\n",
    "        Attention type ('prob' for ProbSparse)\n",
    "    activation : str\n",
    "        Activation function\n",
    "    output_attention : bool\n",
    "        Whether to output attention weights\n",
    "    distil : bool\n",
    "        Whether to use distilling\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,\n",
    "                 factor=5, d_model=512, n_heads=8, e_layers=3, d_layers=2,\n",
    "                 d_ff=512, dropout=0.0, attn='prob', activation='gelu',\n",
    "                 output_attention=False, distil=True, device=torch.device('cuda:0')):\n",
    "        super(Informer, self).__init__()\n",
    "        self.pred_len = out_len\n",
    "        self.attn = attn\n",
    "        self.output_attention = output_attention\n",
    "        \n",
    "        # Encoding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, dropout)\n",
    "        \n",
    "        # Attention\n",
    "        Attn = ProbAttention\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for _ in range(e_layers)\n",
    "            ],\n",
    "            [\n",
    "                ConvLayer(d_model) for _ in range(e_layers - 1)\n",
    "            ] if distil else None,\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        Attn(True, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        Attn(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                ) for _ in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        # Projection\n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
    "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x_enc : [batch, seq_len, enc_in]\n",
    "        x_mark_enc : [batch, seq_len, 4]\n",
    "        x_dec : [batch, label_len + pred_len, dec_in]\n",
    "        x_mark_dec : [batch, label_len + pred_len, 4]\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : [batch, pred_len, c_out]\n",
    "        \"\"\"\n",
    "        # Encoder\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
    "        \n",
    "        # Decoder\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)\n",
    "        dec_out = self.projection(dec_out)\n",
    "        \n",
    "        if self.output_attention:\n",
    "            return dec_out[:, -self.pred_len:, :], attns\n",
    "        else:\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "\n",
    "print(\"Complete Informer model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build-model-header",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    'enc_in': len(feature_cols),      # 6 features\n",
    "    'dec_in': len(feature_cols),      # 6 features\n",
    "    'c_out': 1,                       # 1 target (OT)\n",
    "    'seq_len': seq_len,               # 96\n",
    "    'label_len': label_len,           # 48\n",
    "    'out_len': pred_len,              # 24\n",
    "    'factor': 5,                      # ProbSparse factor\n",
    "    'd_model': 512,                   # Model dimension\n",
    "    'n_heads': 8,                     # Attention heads\n",
    "    'e_layers': 2,                    # Encoder layers\n",
    "    'd_layers': 1,                    # Decoder layers\n",
    "    'd_ff': 2048,                     # Feed-forward dimension\n",
    "    'dropout': 0.05,                  # Dropout\n",
    "    'attn': 'prob',                   # ProbSparse attention\n",
    "    'activation': 'gelu',             # Activation\n",
    "    'output_attention': False,        # Don't output attention\n",
    "    'distil': True,                   # Use distilling\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = Informer(**model_config).to(device)\n",
    "\n",
    "print(\"Model initialized!\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## Step 8: Training Configuration and Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "patience = 5\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'test_loss': []\n",
    "}\n",
    "\n",
    "best_test_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"\\nStarting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (seq_x, seq_x_mark, seq_y, seq_y_mark, target) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        seq_x = seq_x.to(device)\n",
    "        seq_x_mark = seq_x_mark.to(device)\n",
    "        seq_y = seq_y.to(device)\n",
    "        seq_y_mark = seq_y_mark.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "        \n",
    "        # Calculate loss (only on prediction part)\n",
    "        loss = criterion(output, target[:, -pred_len:, :])\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in test_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            loss = criterion(output, target[:, -pred_len:, :])\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    history['test_loss'].append(avg_test_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_test_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {avg_train_loss:.6f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if avg_test_loss < best_test_loss:\n",
    "        best_test_loss = avg_test_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_informer_model.pth')\n",
    "        print(f\"  â†’ New best model saved (Test Loss: {best_test_loss:.6f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best test loss: {best_test_loss:.6f}\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_informer_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "history-header",
   "metadata": {},
   "source": [
    "## Step 9: Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history['test_loss'], label='Test Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Informer Model Training History', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate-header",
   "metadata": {},
   "source": [
    "## Step 10: Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "make-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(model, data_loader, scaler_y, device):\n",
    "    \"\"\"\n",
    "    Make predictions and inverse transform to original scale\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for seq_x, seq_x_mark, seq_y, seq_y_mark, target in data_loader:\n",
    "            seq_x = seq_x.to(device)\n",
    "            seq_x_mark = seq_x_mark.to(device)\n",
    "            seq_y = seq_y.to(device)\n",
    "            seq_y_mark = seq_y_mark.to(device)\n",
    "            \n",
    "            output = model(seq_x, seq_x_mark, seq_y, seq_y_mark)\n",
    "            \n",
    "            # Move to CPU and convert to numpy\n",
    "            pred = output.cpu().numpy()\n",
    "            actual = target[:, -pred_len:, :].cpu().numpy()\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            actuals.append(actual)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions = np.concatenate(predictions, axis=0)  # [N, pred_len, 1]\n",
    "    actuals = np.concatenate(actuals, axis=0)          # [N, pred_len, 1]\n",
    "    \n",
    "    # Reshape for inverse transform\n",
    "    pred_shape = predictions.shape\n",
    "    predictions = predictions.reshape(-1, 1)\n",
    "    actuals = actuals.reshape(-1, 1)\n",
    "    \n",
    "    # Inverse transform\n",
    "    predictions = scaler_y.inverse_transform(predictions)\n",
    "    actuals = scaler_y.inverse_transform(actuals)\n",
    "    \n",
    "    # Reshape back\n",
    "    predictions = predictions.reshape(pred_shape)\n",
    "    actuals = actuals.reshape(pred_shape)\n",
    "    \n",
    "    return predictions, actuals\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions...\")\n",
    "train_pred, train_actual = make_predictions(model, train_loader, train_dataset.scaler_y, device)\n",
    "test_pred, test_actual = make_predictions(model, test_loader, test_dataset.scaler_y, device)\n",
    "\n",
    "print(f\"\\nPrediction shapes:\")\n",
    "print(f\"  Train: {train_pred.shape}\")\n",
    "print(f\"  Test: {test_pred.shape}\")\n",
    "\n",
    "# For metric calculation, use only the last time step of each prediction\n",
    "# Or average across prediction horizon\n",
    "train_pred_flat = train_pred[:, -1, 0]  # Last prediction of each sequence\n",
    "train_actual_flat = train_actual[:, -1, 0]\n",
    "test_pred_flat = test_pred[:, -1, 0]\n",
    "test_actual_flat = test_actual[:, -1, 0]\n",
    "\n",
    "print(f\"\\nFlattened for metrics:\")\n",
    "print(f\"  Train: {train_pred_flat.shape}\")\n",
    "print(f\"  Test: {test_pred_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, set_name='Test'):\n",
    "    \"\"\"\n",
    "    Calculate and display regression metrics\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\n{set_name} Set Performance Metrics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"MSE (Mean Squared Error):        {mse:.4f}\")\n",
    "    print(f\"RMSE (Root Mean Squared Error):  {rmse:.4f}Â°C\")\n",
    "    print(f\"MAE (Mean Absolute Error):       {mae:.4f}Â°C\")\n",
    "    print(f\"RÂ² Score:                        {r2:.4f}\")\n",
    "    print(f\"MAPE (Mean Absolute % Error):    {mape:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# Calculate metrics\n",
    "train_metrics = calculate_metrics(train_actual_flat, train_pred_flat, 'Training')\n",
    "test_metrics = calculate_metrics(test_actual_flat, test_pred_flat, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-results-header",
   "metadata": {},
   "source": [
    "## Step 11: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Test set - full view\n",
    "axes[0].plot(test_actual_flat, label='Actual', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(test_pred_flat, label='Predicted', alpha=0.7, linewidth=1.5)\n",
    "axes[0].set_title('Informer: Oil Temperature Prediction - Test Set (Full View)', fontsize=14)\n",
    "axes[0].set_xlabel('Sample')\n",
    "axes[0].set_ylabel('Oil Temperature (Â°C)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Test set - zoomed view\n",
    "zoom_range = 500\n",
    "axes[1].plot(test_actual_flat[:zoom_range], label='Actual', alpha=0.7, linewidth=1.5)\n",
    "axes[1].plot(test_pred_flat[:zoom_range], label='Predicted', alpha=0.7, linewidth=1.5)\n",
    "axes[1].set_title(f'Informer: Oil Temperature Prediction - Test Set (First {zoom_range} Samples)', fontsize=14)\n",
    "axes[1].set_xlabel('Sample')\n",
    "axes[1].set_ylabel('Oil Temperature (Â°C)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scatter-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(test_actual_flat, test_pred_flat, alpha=0.5, s=20)\n",
    "plt.plot([test_actual_flat.min(), test_actual_flat.max()],\n",
    "         [test_actual_flat.min(), test_actual_flat.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Oil Temperature (Â°C)', fontsize=12)\n",
    "plt.ylabel('Predicted Oil Temperature (Â°C)', fontsize=12)\n",
    "plt.title('Informer: Predicted vs Actual Oil Temperature (Test Set)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residuals",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals = test_actual_flat - test_pred_flat\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(test_pred_flat, residuals, alpha=0.5, s=20)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Predicted Oil Temperature (Â°C)')\n",
    "axes[0].set_ylabel('Residuals (Â°C)')\n",
    "axes[0].set_title('Residual Plot')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual distribution\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Residuals (Â°C)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Statistics:\")\n",
    "print(f\"Mean: {residuals.mean():.4f}Â°C\")\n",
    "print(f\"Std: {residuals.std():.4f}Â°C\")\n",
    "print(f\"Min: {residuals.min():.4f}Â°C\")\n",
    "print(f\"Max: {residuals.max():.4f}Â°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-step-viz-header",
   "metadata": {},
   "source": [
    "### Multi-step Prediction Visualization\n",
    "\n",
    "Visualize how well the model predicts across the entire prediction horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-step-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multi-step predictions for a few samples\n",
    "n_samples = 5\n",
    "sample_indices = np.random.choice(len(test_pred), n_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(n_samples, 1, figsize=(12, 3*n_samples))\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    ax = axes[idx] if n_samples > 1 else axes\n",
    "    \n",
    "    time_steps = np.arange(pred_len)\n",
    "    actual = test_actual[sample_idx, :, 0]\n",
    "    predicted = test_pred[sample_idx, :, 0]\n",
    "    \n",
    "    ax.plot(time_steps, actual, 'o-', label='Actual', linewidth=2, markersize=6)\n",
    "    ax.plot(time_steps, predicted, 's-', label='Predicted', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Prediction Step')\n",
    "    ax.set_ylabel('Oil Temperature (Â°C)')\n",
    "    ax.set_title(f'Sample {sample_idx}: {pred_len}-step Ahead Prediction')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## Step 12: Conclusion and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary comparison\n",
    "summary_df = pd.DataFrame({\n",
    "    'Metric': ['MSE', 'RMSE (Â°C)', 'MAE (Â°C)', 'RÂ²', 'MAPE (%)'],\n",
    "    'Training': [\n",
    "        train_metrics['MSE'],\n",
    "        train_metrics['RMSE'],\n",
    "        train_metrics['MAE'],\n",
    "        train_metrics['R2'],\n",
    "        train_metrics['MAPE']\n",
    "    ],\n",
    "    'Test': [\n",
    "        test_metrics['MSE'],\n",
    "        test_metrics['RMSE'],\n",
    "        test_metrics['MAE'],\n",
    "        test_metrics['R2'],\n",
    "        test_metrics['MAPE']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INFORMER MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n### Key Observations:\")\n",
    "print(f\"1. The Informer achieves RÂ² score of {test_metrics['R2']:.4f} on test set\")\n",
    "print(f\"2. Average prediction error (MAE): {test_metrics['MAE']:.4f}Â°C\")\n",
    "print(f\"3. MAPE: {test_metrics['MAPE']:.2f}%\")\n",
    "\n",
    "if abs(train_metrics['R2'] - test_metrics['R2']) < 0.05:\n",
    "    print(\"4. Model shows good generalization (minimal overfitting)\")\n",
    "else:\n",
    "    print(\"4. Some overfitting detected - consider more regularization\")\n",
    "\n",
    "print(\"\\n### Model Architecture:\")\n",
    "print(f\"- Input Sequence Length: {seq_len} steps (24 hours)\")\n",
    "print(f\"- Prediction Horizon: {pred_len} steps (6 hours)\")\n",
    "print(f\"- Model Dimension: {model_config['d_model']}\")\n",
    "print(f\"- Attention Heads: {model_config['n_heads']}\")\n",
    "print(f\"- Encoder Layers: {model_config['e_layers']}\")\n",
    "print(f\"- Decoder Layers: {model_config['d_layers']}\")\n",
    "print(f\"- Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\n### Informer Advantages:\")\n",
    "print(\"1. ProbSparse Attention: O(L log L) complexity vs O(LÂ²) in vanilla Transformer\")\n",
    "print(\"2. Self-Attention Distilling: Progressive dimension reduction\")\n",
    "print(\"3. Generative Decoder: One-shot long sequence prediction\")\n",
    "print(\"4. Explicitly models temporal patterns with time features\")\n",
    "print(\"5. State-of-the-art performance on long sequence forecasting\")\n",
    "\n",
    "print(\"\\n### Comparison with Other Models:\")\n",
    "print(\"Expected Performance Ranking (on ETT dataset):\")\n",
    "print(\"1. Informer (SOTA) - Best for long sequences\")\n",
    "print(\"2. LSTM/GRU - Good sequential modeling\")\n",
    "print(\"3. MLP - Fast but limited temporal modeling\")\n",
    "print(\"4. Random Forest - Good for nonlinear patterns\")\n",
    "print(\"5. Linear Regression - Baseline\")\n",
    "\n",
    "print(\"\\n### Why Informer Outperforms RNN:\")\n",
    "print(\"1. RNNs suffer from vanishing gradients on long sequences\")\n",
    "print(\"2. Informer's attention captures long-range dependencies better\")\n",
    "print(\"3. Parallel computation vs sequential RNN processing\")\n",
    "print(\"4. Explicit temporal encoding enhances pattern recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-model-header",
   "metadata": {},
   "source": [
    "## Optional: Save Model and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions for further analysis\n",
    "# results = {\n",
    "#     'test_actual': test_actual_flat,\n",
    "#     'test_pred': test_pred_flat,\n",
    "#     'test_metrics': test_metrics\n",
    "# }\n",
    "# np.save('informer_results.npy', results)\n",
    "# print(\"Results saved to informer_results.npy\")\n",
    "\n",
    "# Model is already saved as 'best_informer_model.pth'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}